{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xZFybY_IsdAn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "from cnn import Net\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mFNJpcvabRSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3edf4335-26cb-4434-e955-c5a4c1304807"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFQTocussdAp",
        "outputId": "0b5e9aa4-e776-42f6-dda8-2c5c37aec4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 10939)\n"
          ]
        }
      ],
      "source": [
        "CLEANED_DIR = '/content/drive/MyDrive/'\n",
        "# DATASET_PICKLE = [traces, images, tokens, expressions]\n",
        "path = os.path.join(CLEANED_DIR, 'dataset_cleaned_125.pkl')\n",
        "\n",
        "with open(path, 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "          \n",
        "print((len(dataset), len(dataset[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYQJ4bbfsdAq",
        "outputId": "f68a4da2-18b4-49ea-cf22-24c139fe8087"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "125"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "v = []\n",
        "tokens = dataset[2]\n",
        "for token in tokens:\n",
        "    for item in token.split():\n",
        "        v.append(item)\n",
        "        \n",
        "np.unique(v)\n",
        "len(np.unique(v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d9IVH2YsdAq",
        "outputId": "8162a15f-d531-436a-d83a-a123e2a4b782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pickled dataset at  /content/drive/MyDrive/dataset_cleaned_125.pkl\n"
          ]
        }
      ],
      "source": [
        "CLEANED_DIR = '/content/drive/MyDrive/'\n",
        "path = os.path.join(CLEANED_DIR, 'dataset_cleaned_125.pkl')\n",
        "with open(path, 'wb') as f:\n",
        "    pickle.dump(dataset, f)\n",
        "          \n",
        "print('Saved pickled dataset at ', path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8eGSKpwDsdAr"
      },
      "outputs": [],
      "source": [
        "images, tokens = dataset[1], dataset[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C_OpHOFsdAr"
      },
      "source": [
        "### Hold out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2fga9y0sdAs",
        "outputId": "4dd255d0-e41a-4bcb-aa38-a199b5b6f6ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8204\n",
            "8204\n",
            "2735\n",
            "2735\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, tokens, test_size=1/4)\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(y_train))\n",
        "print(len(X_test))\n",
        "print(len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGutuLy0sdAs"
      },
      "source": [
        "### KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quEGJLC9sdAs",
        "outputId": "5fec2e3a-d291-4ae5-ca02-3dc6c37a78bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126\n"
          ]
        }
      ],
      "source": [
        "# load vocabulary\n",
        "\n",
        "with open('/content/drive/MyDrive/vocab.txt', 'r') as f:\n",
        "    vocab = f.read().split('\\n')\n",
        "    \n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcLxs8eWsdAt"
      },
      "source": [
        "### Visualize target lengths (i.e. how many tokens is one expression?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "aHmmzFmFsdAt",
        "outputId": "2760cce3-3fda-42d1-805d-0fb23d0bb36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "482\n",
            "7271\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'token length')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAV1klEQVR4nO3df/BddX3n8efLgNiKLUHSTApo0E13FxwNbgQU16LdIj+cBWcdC9OpGZoxtgUXu852Q3emuLrMwqLSpetS0kKBjgXZqmsWUEyR1l27QhKKkIBIimFIGkiUX4I7rIH3/nE/X7iG5Jubk+/Nzc19Pmbu3HM+55x7Pp8v33xfnM/n3M9JVSFJUhevGHUFJEnjyxCRJHVmiEiSOjNEJEmdGSKSpM4OGHUFhuGwww6r+fPnj7oakjRW1qxZ84OqmrM7x+yXITJ//nxWr1496mpI0lhJ8vDuHmN3liSpM0NEktSZISJJ6swQkSR1ZohIkjobWogkOTLJ7UnuS7Iuyfmt/BNJNiW5u71O6zvmgiTrkzyQ5L195ae0svVJlg2rzpKk3TPMW3y3AR+vqruSvAZYk2Rl23ZZVX26f+ckRwNnAccAvwj8VZJfaps/B/wqsBFYlWRFVd03xLpLkgYwtBCpqs3A5rb8oyT3A4dPc8gZwA1V9Rzw/STrgePatvVV9RBAkhvavoaIJI3YXhkTSTIfOBa4oxWdl+SeJFcnmd3KDgce6TtsYyvbWfn251iaZHWS1Vu3bp3hFkiSdmTo31hPcjDwReBjVfV0kiuATwHV3j8D/OaenqeqlgPLARYtWjS0J23NX3bztNs3XHz6sE4tSfucoYZIkgPpBcjnq+pLAFX1WN/2PwFuaqubgCP7Dj+ilTFNuSRphIZ5d1aAq4D7q+qzfeXz+nZ7P7C2La8AzkpyUJKjgAXAncAqYEGSo5K8kt7g+4ph1VuSNLhhXomcCPwGcG+Su1vZ7wNnJ1lIrztrA/ARgKpal+RGegPm24Bzq+p5gCTnAbcCs4Crq2rdEOstSRrQMO/O+t9AdrDplmmOuQi4aAflt0x3nCRpNPzGuiSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmdDC5EkRya5Pcl9SdYlOb+VH5pkZZIH2/vsVp4klydZn+SeJG/t+6zFbf8HkyweVp0lSbtnmFci24CPV9XRwAnAuUmOBpYBt1XVAuC2tg5wKrCgvZYCV0AvdIALgeOB44ALp4JHkjRaQwuRqtpcVXe15R8B9wOHA2cA17bdrgXObMtnANdVz7eBQ5LMA94LrKyqx6vqCWAlcMqw6i1JGtxeGRNJMh84FrgDmFtVm9umR4G5bflw4JG+wza2sp2VS5JGbOghkuRg4IvAx6rq6f5tVVVAzdB5liZZnWT11q1bZ+IjJUm7MNQQSXIgvQD5fFV9qRU/1rqpaO9bWvkm4Mi+w49oZTsr/ylVtbyqFlXVojlz5sxsQyRJOzTMu7MCXAXcX1Wf7du0Api6w2ox8JW+8g+1u7ROAJ5q3V63Aicnmd0G1E9uZZKkETtgiJ99IvAbwL1J7m5lvw9cDNyYZAnwMPDBtu0W4DRgPfBj4ByAqno8yaeAVW2/T1bV40OstyRpQOkNS+xfFi1aVKtXrx7KZ89fdnPnYzdcfPoM1kSSZlaSNVW1aHeOGeaVyNjak6CQpEnitCeSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM52O0SSvCLJzw2jMpKk8TJQiCT5iyQ/l+TVwFrgviT/drhVkyTt6w4YcL+jq+rpJL8OfBVYBqwBLh1azfZD85fdPO32DRefvpdqIkkzY9DurAOTHAicCayoqp8ANbxqSZLGwaAhciWwAXg18M0krweeHlalJEnjYaDurKq6HLi8r+jhJO8eTpUkSeNi0IH1uUmuSvLVtn40sHioNZMk7fMG7c66BrgV+MW2/j3gY9MdkOTqJFuSrO0r+0SSTUnubq/T+rZdkGR9kgeSvLev/JRWtj7JskEbJkkavkFD5LCquhF4AaCqtgHP7+KYa4BTdlB+WVUtbK9b4MUrm7OAY9ox/y3JrCSzgM8BpwJHA2e3fSVJ+4BBb/F9NslraXdkJTkBeGq6A6rqm0nmD/j5ZwA3VNVzwPeTrAeOa9vWV9VD7bw3tH3vG/BzJUlDNOiVyL8BVgBvTPIt4Drgox3PeV6Se1p31+xWdjjwSN8+G1vZzspfJsnSJKuTrN66dWvHqkmSdsdAIVJVdwG/DLwD+AhwTFXd0+F8VwBvBBYCm4HPdPiMndVxeVUtqqpFc+bMmamPlSRNY9C7s84FDq6qdVW1Fjg4ye/s7smq6rGqer6qXgD+hJe6rDYBR/btekQr21m5JGkfMGh31oer6smplap6Avjw7p4syby+1ffTm4cLel1lZyU5KMlRwALgTmAVsCDJUUleSW/wfcXunleSNByDDqzPSpKqmhpYnwW8croDklwPnAQclmQjcCFwUpKF9AboN9DrGqOq1iW5kd6A+Tbg3Kp6vn3OefRuL54FXF1V63arhZKkoRk0RL4GfCHJlW39I61sp6rq7B0UXzXN/hcBF+2g/BbglgHrKUnaiwYNkX9HLzh+u62vBP50KDWSJI2NQefOeoHenVVXDLc6kqRxMlCIJDkR+ATw+nZMgKqqNwyvapKkfd2g3VlXAb9L70FUu5ruRJI0IQYNkaeq6qtDrYkkaewMGiK3J7kU+BLw3FRh+ya7JGlCDRoix7f3RX1lBbxnZqsjSRong96d5VMMJUkv0/nJhkmWDLdqkqR93dCebChJ2v8N88mGkqT93KAhsttPNpQk7f8GvTtr+ycbzgE+MLRaSZLGwi5DpE37/svt9Y/pTXnyQFX9ZMh1kyTt43bZndWe63F2VW2berKhASJJgsG7s76V5L8CXwCenSr0G+uSNNkGDZGF7f2TfWV+Y12SJpzfWJckdTbo80T+YEflVfXJHZVLkibDoN1Zz/Ytvwp4H3D/zFdHkjROBu3O+kz/epJP05sGRZI0wQb9xvr2fhY4YiYrIkkaP4OOidxLm/IEmEXvG+uOh0jShBt0TOR9fcvbgMfaJIySpAk2aHfWPODxqnq4qjYBP5Pk+F0dJEnavw0aIlcAz/StP9vKJEkTbNAQSVVNjYlQVS8weFeYJGk/NWiIPJTkXyc5sL3OBx4aZsUkSfu+QUPkt4B3AJuAjcDxwNJhVUqSNB4G/bLhFuCsIddl4s1fdvO02zdcfPpeqokkDWagK5Ek1yY5pG99dpKrh1ctSdI4GLQ7681V9eTUSlU9ARw7nCpJksbFoCHyiiSzp1aSHIp3Z0nSxBs0RD4DfDvJp5L8R+Bvgf883QFJrk6yJcnavrJDk6xM8mB7n93Kk+TyJOuT3JPkrX3HLG77P5hk8e43UZI0LAOFSFVdB5wDbAUeA86pqj/fxWHXAKdsV7YMuK2qFgC3tXWAU4EF7bWU9kXGdsVzIb27wY4DLuy/IpIkjdagA+vnA1cCr6U3+eKVST463TFV9U3g8e2KzwCubcvXAmf2lV9XPd8GDkkyD3gvsLKqHm/jMCt5eTBJkkZk0HGNJcAJVfUsQJJLgP8D/NFunm9uVW1uy48Cc9vy4cAjffttbGU7K3+ZJEtp31153etet5vVkiR1MfC0J8DzfevPt7LO2jQqtcsdB/+85VW1qKoWzZkzZ6Y+VpI0jUGvRP4MuCPJl9v6mcBVHc73WJJ5VbW5dVdtaeWbgCP79juilW0CTtqu/K87nFeSNASDDqx/lt7A+uPtdU5V/WGH860Apu6wWgx8pa/8Q+0urROAp1q3163Aye3LjbOBk/GxvJK0zxj4ux5VdRdw16D7J7me3lXEYUk20rvL6mLgxiRLgIeBD7bdbwFOA9YDP6YXWFTV40k+Baxq+32yqrYfrJckjcjQvjBYVWfvZNOv7GDfAs7dyedcDTjFiiTtgwYdWJck6WUMEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM58xO0Ymb/s5p1u23Dx6XuxJpLU45WIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdjSREkmxIcm+Su5OsbmWHJlmZ5MH2PruVJ8nlSdYnuSfJW0dRZ0nSy43ySuTdVbWwqha19WXAbVW1ALitrQOcCixor6XAFXu9ppKkHdqXurPOAK5ty9cCZ/aVX1c93wYOSTJvFBWUJP20UYVIAV9PsibJ0lY2t6o2t+VHgblt+XDgkb5jN7ayn5JkaZLVSVZv3bp1WPWWJPU5YETnfWdVbUryC8DKJN/t31hVlaR25wOrajmwHGDRokW7dawkqZuRXIlU1ab2vgX4MnAc8NhUN1V739J23wQc2Xf4Ea1MkjRiez1Ekrw6yWumloGTgbXACmBx220x8JW2vAL4ULtL6wTgqb5uL0nSCI2iO2su8OUkU+f/i6r6WpJVwI1JlgAPAx9s+98CnAasB34MnLP3qyxJ2pG9HiJV9RDwlh2U/xD4lR2UF3DuXqjaWJu/7OZpt2+4+PS9VBNJk2RfusVXkjRmDBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktTZqB5Kpb3MCRolDYNXIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmRMwCph+gkYnZ5S0M16JSJI6M0QkSZ0ZIpKkzhwT0S75QCtJO2OIaI85KC9NLruzJEmdGSKSpM7GJkSSnJLkgSTrkywbdX0kSWMyJpJkFvA54FeBjcCqJCuq6r7R1ky7sqtB+V1xTEXat41FiADHAeur6iGAJDcAZwCGyH7OQXtp3zYuIXI48Ejf+kbg+P4dkiwFlrbVZ5I80PFchwE/6HjsuBurtueSGf/IsWr/DLPtk2n7tr9+dz9gXEJkl6pqObB8Tz8nyeqqWjQDVRo7k9x2mOz223bb3tW4DKxvAo7sWz+ilUmSRmhcQmQVsCDJUUleCZwFrBhxnSRp4o1Fd1ZVbUtyHnArMAu4uqrWDel0e9wlNsYmue0w2e237ZNpz4cAqmomKiJJmkDj0p0lSdoHGSKSpM4MkT77+9QqSa5OsiXJ2r6yQ5OsTPJge5/dypPk8vazuCfJW0dX8z2X5Mgktye5L8m6JOe38v2+/UleleTOJN9pbf8PrfyoJHe0Nn6h3bRCkoPa+vq2ff4o6z8TksxK8ndJbmrrk9T2DUnuTXJ3ktWtbMZ+7w2Rpm9qlVOBo4Gzkxw92lrNuGuAU7YrWwbcVlULgNvaOvR+DgvaaylwxV6q47BsAz5eVUcDJwDntv++k9D+54D3VNVbgIXAKUlOAC4BLquqfwQ8ASxp+y8Bnmjll7X9xt35wP1965PUdoB3V9XCvu+EzNzvfVX56t1c8Hbg1r71C4ALRl2vIbRzPrC2b/0BYF5bngc80JavBM7e0X77wwv4Cr252Caq/cDPAnfRm/HhB8ABrfzF3396d0G+vS0f0PbLqOu+B20+ov2hfA9wE5BJaXtrxwbgsO3KZuz33iuRl+xoapXDR1SXvWluVW1uy48Cc9vyfvvzaF0UxwJ3MCHtb905dwNbgJXA3wNPVtW2tkt/+15se9v+FPDavVvjGfWHwO8BL7T11zI5bQco4OtJ1rTpoWAGf+/H4nsi2juqqpLs1/d8JzkY+CLwsap6OsmL2/bn9lfV88DCJIcAXwb+yYirtFckeR+wparWJDlp1PUZkXdW1aYkvwCsTPLd/o17+nvvlchLJnVqlceSzANo71ta+X7380hyIL0A+XxVfakVT0z7AarqSeB2el04hySZ+h/J/va92Pa2/eeBH+7lqs6UE4F/mWQDcAO9Lq3/wmS0HYCq2tTet9D7H4jjmMHfe0PkJZM6tcoKYHFbXkxvrGCq/EPtbo0TgKf6Ln/HTnqXHFcB91fVZ/s27fftTzKnXYGQ5GfojQXdTy9MPtB2277tUz+TDwDfqNZBPm6q6oKqOqKq5tP7N/2Nqvp1JqDtAEleneQ1U8vAycBaZvL3ftSDPvvSCzgN+B69/uJ/P+r6DKF91wObgZ/Q6+tcQq+/9zbgQeCvgEPbvqF3t9rfA/cCi0Zd/z1s+zvp9Q3fA9zdXqdNQvuBNwN/19q+FviDVv4G4E5gPfDfgYNa+ava+vq2/Q2jbsMM/RxOAm6apLa3dn6nvdZN/V2byd97pz2RJHVmd5YkqTNDRJLUmSEiSerMEJEkdWaISJI6M0Q0cZIckuR3BtjvpKlZX2fw3PPTN4vyDH7uSUne0bd+TZIPTHeMNBMMEU2iQ4BdhsiYOQl4x652kmaaIaJJdDHwxvZ8hUvbt3MvTbK2PXfh17Y/IMnb2vMo3pjknyX5mzah3a1900f8dZJL0nt2x/eS/PPpKtEmRbw0yar27IaPtPKT2mf9ZZLvJvl8+8Y9SU5rZWvacx9uahNK/hbwu61NU+d9V5K/TfKQVyUaFidg1CRaBrypqhYCJPlX9J6z8RbgMGBVkm9O7dy6if4IOIPeN/7/HDijqra2wLkI+M22+wFVdVyS04ALgX8xTT2W0JtW4m1JDgK+leTrbduxwDHAPwDfAk5M74FCVwLvqqrvJ7keoKo2JPlj4Jmq+nSr8xJ6U3y/k95kiyuAv+z485J2yhCRen9or6/eTLePJfkb4G3A08A/BZYDJ1fVPyR5E/AmerOhAsyiFyxTpiZ2XEPv2S3TORl4c99Vws/TexjQ/wPurKqNAG0K9/nAM8BDVfX9tv/19B4ctDP/o6peAO5LMnea/aTODBFpepvpzad0LL2rggDrqurtO9n/ufb+PLv+9xXgo1V1608V9qYsf66vaJDPmq4uU+eSZpxjIppEPwJe07f+v4Bfa2MUc4B30Zt8D+BJ4HTgP7U/7g8Ac5K8HXrTyyc5pmM9bgV+u01RT5JfajOt7swDwBvy0nO/+8dutm+TtFcYIpo4VfVDeuMPa5NcSu8ZC/fQm+n0G8DvVdWjffs/BryP3uymx9KbIvySJN+hNxtw17ui/hS4D7ir3fZ7JdNccVTV/6V3V9nXkqyhFxxPtc3/E3j/dgPr0tA5i680RpIcXFXPtLu1Pgc8WFWXjbpemlxeiUjj5cNtoH0dvYH4K0dcH004r0QkSZ15JSJJ6swQkSR1ZohIkjozRCRJnRkikqTO/j/UqQo/SKJmMwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "L = []\n",
        "\n",
        "for tok in tokens:\n",
        "    L.append(len(tok))\n",
        "    \n",
        "print(np.max(L))\n",
        "print(np.argmax(L))\n",
        "plt.hist(L, bins=40)\n",
        "plt.ylabel('occurences')\n",
        "plt.xlabel('token length')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "05kkdOw3sdAt",
        "outputId": "30846e24-89e3-40ad-d749-44e1a0bfc3bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa7ae8bbbd0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(images[7278], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO5hL2A-sdAu"
      },
      "source": [
        "### Cutoff tokens longer than length 150\n",
        "\n",
        "Like mentioned in the paper (A Visual Latex Decompiler), we remove training samples with tokens longer than 150. However, we leave them in for testing.\n",
        "\n",
        "We would thus remove 23 samples from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZjDupp0sdAu",
        "outputId": "911799e4-c7e3-4354-dd5e-b8a0836667a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0021025687905658653"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "L = np.array(L)\n",
        "\n",
        "(L>150).sum()\n",
        "((L>150).sum()) / len(L)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLHUfSfFsdAu",
        "outputId": "836fcda6-1fa2-48be-9983-14bd0065fc79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8157\n",
            "6805\n",
            "6783\n",
            "6012\n",
            "5934\n",
            "5417\n",
            "4858\n",
            "4575\n",
            "4309\n",
            "3959\n",
            "3582\n",
            "2885\n",
            "970\n"
          ]
        }
      ],
      "source": [
        "def cutoff(X, y, token_length):\n",
        "    '''Remove samples from X an y where y has a token length > 150.'''\n",
        "    removed = []\n",
        "    for idx in reversed(range(len(y))):\n",
        "        if len(y[idx]) > token_length:\n",
        "            print(idx)\n",
        "            removed.append((X[idx], y[idx]))\n",
        "    return removed\n",
        "                           \n",
        "rem = cutoff(X_train, y_train, token_length=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKKhkw1UsdAv"
      },
      "source": [
        "## Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc4pfuitsdAv",
        "outputId": "76fc0492-38bd-4860-c45f-37eecf6800eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['!', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3',\n",
              "       '4', '5', '6', '7', '8', '9', '<', '=', '>', 'A', 'B', 'C', 'E',\n",
              "       'F', 'G', 'H', 'I', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V',\n",
              "       'X', 'Y', '[', '\\\\', '\\\\!', '\\\\;', '\\\\Big', '\\\\Bigg', '\\\\Delta',\n",
              "       '\\\\alpha', '\\\\beta', '\\\\cdot', '\\\\cdots', '\\\\cos', '\\\\div',\n",
              "       '\\\\exists', '\\\\forall', '\\\\frac', '\\\\gamma', '\\\\geq', '\\\\gt',\n",
              "       '\\\\hbox', '\\\\in', '\\\\infty', '\\\\int', '\\\\lambda', '\\\\lbrack',\n",
              "       '\\\\ldots', '\\\\left', '\\\\leq', '\\\\lim', '\\\\limits', '\\\\log', '\\\\lt',\n",
              "       '\\\\mathrm', '\\\\mbox', '\\\\mu', '\\\\neq', '\\\\parallel', '\\\\phi',\n",
              "       '\\\\pi', '\\\\pm', '\\\\prime', '\\\\rbrack', '\\\\right', '\\\\rightarrow',\n",
              "       '\\\\sigma', '\\\\sin', '\\\\sqrt', '\\\\sum', '\\\\tan', '\\\\theta',\n",
              "       '\\\\times', '\\\\{', '\\\\}', ']', '^', '_', 'a', 'b', 'c', 'd', 'e',\n",
              "       'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r',\n",
              "       's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}'],\n",
              "      dtype='<U11')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# load encoder\n",
        "DATA = '/content/drive/MyDrive/'\n",
        "\n",
        "vocab = np.load(DATA + 'labelencoding.npy')\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from encode import train_label_encoding, OneHotEncoder\n",
        "\n",
        "label_enc = train_label_encoding(vocab)"
      ],
      "metadata": {
        "id": "vYZMZfA-wWdp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sUdY3K9NsdAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d8f2d7-f57d-4e45-e200-ca234871fdea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1293: UserWarning: The parameter 'resample' is deprecated since 0.12 and will be removed 0.14. Please use 'interpolation' instead.\n",
            "  \"The parameter 'resample' is deprecated since 0.12 and will be removed 0.14. \"\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import PIL\n",
        "\n",
        "input_shape=(250, 490)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(input_shape),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
        "            transforms.ToTensor(), # This also normalizes the image\n",
        "        ])\n",
        "\n",
        "target_transform = OneHotEncoder(label_enc)\n",
        "\n",
        "class ImgTokenDataset(Dataset):\n",
        "    def __init__(self, images, tokens, transform=transform, target_transform=target_transform, vocab=vocab):\n",
        "        self.images = images\n",
        "        self.tokens = tokens\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        target = self.tokens[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            target = self.target_transform(target)\n",
        "        return image, target\n",
        "    \n",
        "    \n",
        "train_data = ImgTokenDataset(X_train, y_train, transform=transform, target_transform=target_transform, vocab=vocab)\n",
        "    \n",
        "test_data = ImgTokenDataset(X_test, y_test, transform=transform, target_transform=target_transform, vocab=vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LrJ1tYH0sdAw"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(train_data, batch_size=28, shuffle=True)\n",
        "testloader = DataLoader(test_data, batch_size=28, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8_RdDBKsdAx"
      },
      "source": [
        "## CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QPmuIM2sdAx",
        "outputId": "61a2155f-8e25-4bc0-fa07-1b57f81b2556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading...\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv1_bn): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=115200, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=32, bias=True)\n",
              "  (fc4): Linear(in_features=32, out_features=126, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from cnn import Net\n",
        "\n",
        "import torch.optim as optim\n",
        "print('loading...')\n",
        "net = Net(input_shape, vocab)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# if we set the hardware to GPU in the Notebook settings, this should print a CUDA device:\n",
        "# device = 'cpu'\n",
        "\n",
        "print(device)\n",
        "\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def report_precision_recall(net, data, device, threshold=0.5):\n",
        "      \n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    P, R = [], []\n",
        "    \n",
        "    for sample in range(len(outputs.cpu())):\n",
        "        pred_pos = set(np.where(outputs[sample].cpu()>=threshold)[0])\n",
        "        pred_neg = set(np.where(outputs[sample].cpu()<threshold)[0])\n",
        "        actual_pos = set(np.where(labels[sample].cpu()==1)[0])\n",
        "        actual_neg = set(np.where(labels[sample].cpu()==0)[0])\n",
        "        if len(actual_pos) < 1:\n",
        "            continue\n",
        "        TP = len(pred_pos & actual_pos)\n",
        "        FP = len(pred_pos & actual_neg)\n",
        "        TN = len(pred_neg & actual_neg)\n",
        "        FN = len(pred_neg & actual_pos)\n",
        "        precision = TP/(TP+FP)\n",
        "        recall = TP/(TP+FN)\n",
        "        P.append(precision)\n",
        "        R.append(recall)\n",
        "\n",
        "    P, R = np.array(P), np.array(R)\n",
        "    return [P.mean(), P.std(), R.mean(), R.std()]"
      ],
      "metadata": {
        "id": "YqJM_NnX8EXE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dZFdzU0sdAx",
        "outputId": "a4f1f3df-bad7-4f36-c118-8c6427809edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 234   Training loss: 0.1542280912399292\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 235   Training loss: 0.16635552048683167\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 236   Training loss: 0.15841761231422424\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 237   Training loss: 0.18384364247322083\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 238   Training loss: 0.17102347314357758\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 239   Training loss: 0.13412904739379883\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 240   Training loss: 0.1713649183511734\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 241   Training loss: 0.16670289635658264\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 242   Training loss: 0.15532030165195465\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 243   Training loss: 0.1572514921426773\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 244   Training loss: 0.16027197241783142\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 245   Training loss: 0.1646592766046524\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 246   Training loss: 0.1696632355451584\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 247   Training loss: 0.1620291769504547\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 248   Training loss: 0.1689194291830063\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 249   Training loss: 0.15808477997779846\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 250   Training loss: 0.17693127691745758\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 251   Training loss: 0.14777982234954834\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 252   Training loss: 0.1876189261674881\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 253   Training loss: 0.17644114792346954\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 254   Training loss: 0.17242299020290375\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 255   Training loss: 0.16093969345092773\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 256   Training loss: 0.16407977044582367\n",
            "Precision: 57.38 +/- 42.2 %\n",
            "Recall: 25.30 +/- 19.2 %\n",
            "         \n",
            "\n",
            "Batch: 257   Training loss: 0.1526765078306198\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 258   Training loss: 0.16047100722789764\n",
            "Precision: 78.39 +/- 29.8 %\n",
            "Recall: 29.94 +/- 17.9 %\n",
            "         \n",
            "\n",
            "Batch: 259   Training loss: 0.16025635600090027\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 260   Training loss: 0.1539970487356186\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 261   Training loss: 0.15623851120471954\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 262   Training loss: 0.17069104313850403\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 263   Training loss: 0.17126516997814178\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 264   Training loss: 0.16985492408275604\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 265   Training loss: 0.1570422649383545\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 266   Training loss: 0.16756699979305267\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 267   Training loss: 0.15796317160129547\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 268   Training loss: 0.16491611301898956\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 269   Training loss: 0.1584664136171341\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 270   Training loss: 0.1765115112066269\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 271   Training loss: 0.17324480414390564\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 272   Training loss: 0.18145976960659027\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 273   Training loss: 0.1816621869802475\n",
            "Precision: 73.01 +/- 34.8 %\n",
            "Recall: 27.56 +/- 16.5 %\n",
            "         \n",
            "\n",
            "Batch: 274   Training loss: 0.1498478204011917\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 275   Training loss: 0.17414617538452148\n",
            "Precision: 46.23 +/- 37.7 %\n",
            "Recall: 23.89 +/- 21.6 %\n",
            "         \n",
            "\n",
            "Batch: 276   Training loss: 0.15867015719413757\n",
            "Precision: 65.46 +/- 39.2 %\n",
            "Recall: 30.47 +/- 23.2 %\n",
            "         \n",
            "\n",
            "Batch: 277   Training loss: 0.1538998931646347\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 278   Training loss: 0.164509579539299\n",
            "Precision: 58.57 +/- 42.8 %\n",
            "Recall: 23.53 +/- 20.6 %\n",
            "         \n",
            "\n",
            "Batch: 279   Training loss: 0.15633974969387054\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 280   Training loss: 0.1747339367866516\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 281   Training loss: 0.15920871496200562\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 282   Training loss: 0.14311684668064117\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 283   Training loss: 0.1673423796892166\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 284   Training loss: 0.15536166727542877\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 285   Training loss: 0.1579999476671219\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 286   Training loss: 0.16889123618602753\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 287   Training loss: 0.16999612748622894\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 288   Training loss: 0.15387970209121704\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 289   Training loss: 0.15668229758739471\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 290   Training loss: 0.15548159182071686\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 291   Training loss: 0.19011124968528748\n",
            "Precision: 62.04 +/- 38.6 %\n",
            "Recall: 25.92 +/- 16.5 %\n",
            "         \n",
            "\n",
            "Batch: 292   Training loss: 0.15320400893688202\n",
            "Positive samples missing, skipping\n",
            "start\n",
            "         \n",
            "\n",
            "Batch: 0   Training loss: 0.1562279909849167\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 1   Training loss: 0.15830786526203156\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 2   Training loss: 0.1629960536956787\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 3   Training loss: 0.15605807304382324\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 4   Training loss: 0.1649324893951416\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 5   Training loss: 0.1633577197790146\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 6   Training loss: 0.15784499049186707\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 7   Training loss: 0.16973786056041718\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 8   Training loss: 0.1547194868326187\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 9   Training loss: 0.1714228391647339\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 10   Training loss: 0.1495068073272705\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 11   Training loss: 0.15010148286819458\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 12   Training loss: 0.16142302751541138\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 13   Training loss: 0.1479414701461792\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 14   Training loss: 0.1675412356853485\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 15   Training loss: 0.1696452498435974\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 16   Training loss: 0.18023701012134552\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 17   Training loss: 0.17861847579479218\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 18   Training loss: 0.15527883172035217\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 19   Training loss: 0.16040292382240295\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 20   Training loss: 0.17098331451416016\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 21   Training loss: 0.16356173157691956\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 22   Training loss: 0.1767703890800476\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 23   Training loss: 0.16380371153354645\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 24   Training loss: 0.17255112528800964\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 25   Training loss: 0.17491264641284943\n",
            "Precision: 62.59 +/- 37.6 %\n",
            "Recall: 25.55 +/- 16.3 %\n",
            "         \n",
            "\n",
            "Batch: 26   Training loss: 0.1708483099937439\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 27   Training loss: 0.16500021517276764\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 28   Training loss: 0.16991211473941803\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 29   Training loss: 0.16590872406959534\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 30   Training loss: 0.15957686305046082\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 31   Training loss: 0.16931991279125214\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 32   Training loss: 0.17260780930519104\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 33   Training loss: 0.16813863813877106\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 34   Training loss: 0.16248609125614166\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 35   Training loss: 0.15716123580932617\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 36   Training loss: 0.1762837916612625\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 37   Training loss: 0.19067126512527466\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 38   Training loss: 0.16247926652431488\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 39   Training loss: 0.15367306768894196\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 40   Training loss: 0.17604976892471313\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 41   Training loss: 0.16249296069145203\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 42   Training loss: 0.15988881886005402\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 43   Training loss: 0.18353669345378876\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 44   Training loss: 0.15434370934963226\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 45   Training loss: 0.1716393679380417\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 46   Training loss: 0.15353721380233765\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 47   Training loss: 0.1647329032421112\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 48   Training loss: 0.16150391101837158\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 49   Training loss: 0.1504412144422531\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 50   Training loss: 0.14593861997127533\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 51   Training loss: 0.16380338370800018\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 52   Training loss: 0.1694847196340561\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 53   Training loss: 0.1578933447599411\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 54   Training loss: 0.17428629100322723\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 55   Training loss: 0.1706496626138687\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 56   Training loss: 0.16789546608924866\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 57   Training loss: 0.15326327085494995\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 58   Training loss: 0.18220975995063782\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 59   Training loss: 0.15359219908714294\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 60   Training loss: 0.16716048121452332\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 61   Training loss: 0.162589430809021\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 62   Training loss: 0.15168693661689758\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 63   Training loss: 0.16810393333435059\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 64   Training loss: 0.1615762561559677\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 65   Training loss: 0.18212901055812836\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 66   Training loss: 0.1814916729927063\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 67   Training loss: 0.15542437136173248\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 68   Training loss: 0.15757285058498383\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 69   Training loss: 0.16526836156845093\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 70   Training loss: 0.1678599715232849\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 71   Training loss: 0.16168996691703796\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 72   Training loss: 0.16818024218082428\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 73   Training loss: 0.15785883367061615\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 74   Training loss: 0.17074210941791534\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 75   Training loss: 0.1561073213815689\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 76   Training loss: 0.15355035662651062\n",
            "Precision: 74.00 +/- 34.4 %\n",
            "Recall: 30.40 +/- 20.6 %\n",
            "         \n",
            "\n",
            "Batch: 77   Training loss: 0.16764020919799805\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 78   Training loss: 0.15398861467838287\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 79   Training loss: 0.17542774975299835\n",
            "Precision: 52.56 +/- 40.2 %\n",
            "Recall: 22.61 +/- 17.3 %\n",
            "         \n",
            "\n",
            "Batch: 80   Training loss: 0.15099675953388214\n",
            "Precision: 70.59 +/- 37.9 %\n",
            "Recall: 30.93 +/- 20.4 %\n",
            "         \n",
            "\n",
            "Batch: 81   Training loss: 0.20103326439857483\n",
            "Precision: 55.90 +/- 40.0 %\n",
            "Recall: 22.78 +/- 16.0 %\n",
            "         \n",
            "\n",
            "Batch: 82   Training loss: 0.1624859720468521\n",
            "Precision: 59.78 +/- 45.5 %\n",
            "Recall: 24.07 +/- 22.7 %\n",
            "         \n",
            "\n",
            "Batch: 83   Training loss: 0.1670389175415039\n",
            "Precision: 53.82 +/- 42.8 %\n",
            "Recall: 21.41 +/- 19.2 %\n",
            "         \n",
            "\n",
            "Batch: 84   Training loss: 0.1470451056957245\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 85   Training loss: 0.15213344991207123\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 86   Training loss: 0.17609809339046478\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 87   Training loss: 0.1573207974433899\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 88   Training loss: 0.1702398955821991\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 89   Training loss: 0.15990276634693146\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 90   Training loss: 0.1746605485677719\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 91   Training loss: 0.1488761007785797\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 92   Training loss: 0.17253419756889343\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 93   Training loss: 0.15634730458259583\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 94   Training loss: 0.16385625302791595\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 95   Training loss: 0.15527531504631042\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 96   Training loss: 0.1657736450433731\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 97   Training loss: 0.16657866537570953\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 98   Training loss: 0.15712927281856537\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 99   Training loss: 0.15401384234428406\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 100   Training loss: 0.17747944593429565\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 101   Training loss: 0.1641194224357605\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 102   Training loss: 0.17030002176761627\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 103   Training loss: 0.1600189208984375\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 104   Training loss: 0.14240850508213043\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 105   Training loss: 0.14697083830833435\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 106   Training loss: 0.15206405520439148\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 107   Training loss: 0.1622934490442276\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 108   Training loss: 0.15434393286705017\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 109   Training loss: 0.16414999961853027\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 110   Training loss: 0.15641169250011444\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 111   Training loss: 0.16561253368854523\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 112   Training loss: 0.14704397320747375\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 113   Training loss: 0.1447802186012268\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 114   Training loss: 0.15788592398166656\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 115   Training loss: 0.16473859548568726\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 116   Training loss: 0.16693109273910522\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 117   Training loss: 0.14199043810367584\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 118   Training loss: 0.16594794392585754\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 119   Training loss: 0.18771477043628693\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 120   Training loss: 0.17055866122245789\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 121   Training loss: 0.16641934216022491\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 122   Training loss: 0.1624463051557541\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 123   Training loss: 0.16867715120315552\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 124   Training loss: 0.14967304468154907\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 125   Training loss: 0.16337579488754272\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 126   Training loss: 0.16637325286865234\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 127   Training loss: 0.17116129398345947\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 128   Training loss: 0.17424604296684265\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 129   Training loss: 0.16481941938400269\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 130   Training loss: 0.1724195033311844\n",
            "Precision: 53.21 +/- 41.8 %\n",
            "Recall: 23.45 +/- 21.3 %\n",
            "         \n",
            "\n",
            "Batch: 131   Training loss: 0.15167775750160217\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 132   Training loss: 0.16197945177555084\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 133   Training loss: 0.1664200723171234\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 134   Training loss: 0.16427156329154968\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 135   Training loss: 0.1529364287853241\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 136   Training loss: 0.16331952810287476\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 137   Training loss: 0.17211392521858215\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 138   Training loss: 0.152523472905159\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 139   Training loss: 0.15598662197589874\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 140   Training loss: 0.15834537148475647\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 141   Training loss: 0.1626179963350296\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 142   Training loss: 0.14892269670963287\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 143   Training loss: 0.16771796345710754\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 144   Training loss: 0.15087053179740906\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 145   Training loss: 0.1506662517786026\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 146   Training loss: 0.1474083960056305\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 147   Training loss: 0.17761510610580444\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 148   Training loss: 0.15647339820861816\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 149   Training loss: 0.17262215912342072\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 150   Training loss: 0.13333187997341156\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 151   Training loss: 0.13585445284843445\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 152   Training loss: 0.16235888004302979\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 153   Training loss: 0.1660473495721817\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 154   Training loss: 0.13802364468574524\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 155   Training loss: 0.16707555949687958\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 156   Training loss: 0.17450927197933197\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 157   Training loss: 0.18120673298835754\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 158   Training loss: 0.1680447906255722\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 159   Training loss: 0.15154717862606049\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 160   Training loss: 0.1655363142490387\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 161   Training loss: 0.15306024253368378\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 162   Training loss: 0.16099213063716888\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 163   Training loss: 0.15740056335926056\n",
            "Precision: 76.33 +/- 33.6 %\n",
            "Recall: 31.30 +/- 22.2 %\n",
            "         \n",
            "\n",
            "Batch: 164   Training loss: 0.17015020549297333\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 165   Training loss: 0.18388281762599945\n",
            "Precision: 61.30 +/- 41.8 %\n",
            "Recall: 26.46 +/- 22.7 %\n",
            "         \n",
            "\n",
            "Batch: 166   Training loss: 0.15166060626506805\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 167   Training loss: 0.15107648074626923\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 168   Training loss: 0.1542145311832428\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 169   Training loss: 0.14373168349266052\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 170   Training loss: 0.1657523512840271\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 171   Training loss: 0.16891878843307495\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 172   Training loss: 0.15410149097442627\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 173   Training loss: 0.15031176805496216\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 174   Training loss: 0.1555645614862442\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 175   Training loss: 0.17893542349338531\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 176   Training loss: 0.15861572325229645\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 177   Training loss: 0.16975028812885284\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 178   Training loss: 0.17525023221969604\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 179   Training loss: 0.18108819425106049\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 180   Training loss: 0.18155363202095032\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 181   Training loss: 0.16191506385803223\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 182   Training loss: 0.15588733553886414\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 183   Training loss: 0.15187160670757294\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 184   Training loss: 0.16083987057209015\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 185   Training loss: 0.15561069548130035\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 186   Training loss: 0.1491483747959137\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 187   Training loss: 0.17298609018325806\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 188   Training loss: 0.16001708805561066\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 189   Training loss: 0.15183234214782715\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 190   Training loss: 0.14994752407073975\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 191   Training loss: 0.157467320561409\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 192   Training loss: 0.15734261274337769\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 193   Training loss: 0.15794195234775543\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 194   Training loss: 0.15822990238666534\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 195   Training loss: 0.1412794291973114\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 196   Training loss: 0.14303994178771973\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 197   Training loss: 0.1659705936908722\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 198   Training loss: 0.1551169604063034\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 199   Training loss: 0.15704336762428284\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 200   Training loss: 0.16042302548885345\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 201   Training loss: 0.16689801216125488\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 202   Training loss: 0.1567516326904297\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 203   Training loss: 0.16405346989631653\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 204   Training loss: 0.15724417567253113\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 205   Training loss: 0.1497364044189453\n",
            "Precision: 62.57 +/- 39.3 %\n",
            "Recall: 32.91 +/- 23.6 %\n",
            "         \n",
            "\n",
            "Batch: 206   Training loss: 0.15943866968154907\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 207   Training loss: 0.18378855288028717\n",
            "Precision: 76.22 +/- 30.5 %\n",
            "Recall: 29.44 +/- 16.4 %\n",
            "         \n",
            "\n",
            "Batch: 208   Training loss: 0.16170457005500793\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 209   Training loss: 0.16240853071212769\n",
            "Precision: 69.07 +/- 35.7 %\n",
            "Recall: 32.78 +/- 20.9 %\n",
            "         \n",
            "\n",
            "Batch: 210   Training loss: 0.16395768523216248\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 211   Training loss: 0.16420885920524597\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 212   Training loss: 0.16402187943458557\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 213   Training loss: 0.1612326204776764\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 214   Training loss: 0.14447736740112305\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 215   Training loss: 0.16178885102272034\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 216   Training loss: 0.17310762405395508\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 217   Training loss: 0.16298097372055054\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 218   Training loss: 0.147790789604187\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 219   Training loss: 0.17080271244049072\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 220   Training loss: 0.15918943285942078\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 221   Training loss: 0.18401986360549927\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 222   Training loss: 0.17965292930603027\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 223   Training loss: 0.17120987176895142\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 224   Training loss: 0.14201006293296814\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 225   Training loss: 0.1684502810239792\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 226   Training loss: 0.14975574612617493\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 227   Training loss: 0.18058469891548157\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 228   Training loss: 0.16540618240833282\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 229   Training loss: 0.1521337926387787\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 230   Training loss: 0.1682673841714859\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 231   Training loss: 0.1563129723072052\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 232   Training loss: 0.13618047535419464\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 233   Training loss: 0.16699498891830444\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 234   Training loss: 0.15080444514751434\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 235   Training loss: 0.15069912374019623\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 236   Training loss: 0.17856433987617493\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 237   Training loss: 0.17451715469360352\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 238   Training loss: 0.16172349452972412\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 239   Training loss: 0.16624775528907776\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 240   Training loss: 0.16377940773963928\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 241   Training loss: 0.14328740537166595\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 242   Training loss: 0.15042994916439056\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 243   Training loss: 0.15133589506149292\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 244   Training loss: 0.1512984037399292\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 245   Training loss: 0.15450961887836456\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 246   Training loss: 0.14962507784366608\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 247   Training loss: 0.1862228661775589\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 248   Training loss: 0.157557874917984\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 249   Training loss: 0.1663815677165985\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 250   Training loss: 0.15705330669879913\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 251   Training loss: 0.1544714868068695\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 252   Training loss: 0.18182282149791718\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 253   Training loss: 0.16756115853786469\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 254   Training loss: 0.1680264174938202\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 255   Training loss: 0.16775007545948029\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 256   Training loss: 0.15916432440280914\n",
            "Precision: 66.23 +/- 38.5 %\n",
            "Recall: 27.81 +/- 20.8 %\n",
            "         \n",
            "\n",
            "Batch: 257   Training loss: 0.16674189269542694\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 258   Training loss: 0.17905911803245544\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 259   Training loss: 0.16352370381355286\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 260   Training loss: 0.16163168847560883\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 261   Training loss: 0.14303405582904816\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 262   Training loss: 0.15483449399471283\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 263   Training loss: 0.1438359171152115\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 264   Training loss: 0.17856062948703766\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 265   Training loss: 0.17373619973659515\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 266   Training loss: 0.15994131565093994\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 267   Training loss: 0.1640516221523285\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 268   Training loss: 0.15389227867126465\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 269   Training loss: 0.17712077498435974\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 270   Training loss: 0.1493997722864151\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 271   Training loss: 0.1714225709438324\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 272   Training loss: 0.15711160004138947\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 273   Training loss: 0.17757968604564667\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 274   Training loss: 0.17171348631381989\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 275   Training loss: 0.14582481980323792\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 276   Training loss: 0.14811883866786957\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 277   Training loss: 0.18437859416007996\n",
            "Precision: 66.79 +/- 35.4 %\n",
            "Recall: 33.22 +/- 20.6 %\n",
            "         \n",
            "\n",
            "Batch: 278   Training loss: 0.16495703160762787\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 279   Training loss: 0.1574709266424179\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 280   Training loss: 0.1491841971874237\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 281   Training loss: 0.14857472479343414\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 282   Training loss: 0.15433615446090698\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 283   Training loss: 0.1467638611793518\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 284   Training loss: 0.17339693009853363\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 285   Training loss: 0.15744951367378235\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 286   Training loss: 0.15462762117385864\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 287   Training loss: 0.14282795786857605\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 288   Training loss: 0.15240894258022308\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 289   Training loss: 0.16392676532268524\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 290   Training loss: 0.17495772242546082\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 291   Training loss: 0.15482589602470398\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 292   Training loss: 0.15751175582408905\n",
            "Precision: 56.34 +/- 36.0 %\n",
            "Recall: 32.38 +/- 19.6 %\n",
            "start\n",
            "         \n",
            "\n",
            "Batch: 0   Training loss: 0.19435524940490723\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 1   Training loss: 0.1568027287721634\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 2   Training loss: 0.19051747024059296\n",
            "Precision: 69.96 +/- 34.9 %\n",
            "Recall: 30.77 +/- 21.3 %\n",
            "         \n",
            "\n",
            "Batch: 3   Training loss: 0.15688268840312958\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 4   Training loss: 0.15477360785007477\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 5   Training loss: 0.17106886208057404\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 6   Training loss: 0.15350328385829926\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 7   Training loss: 0.15981422364711761\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 8   Training loss: 0.1733541041612625\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 9   Training loss: 0.1497230976819992\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 10   Training loss: 0.168735533952713\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 11   Training loss: 0.15650612115859985\n",
            "Precision: 67.04 +/- 40.3 %\n",
            "Recall: 29.20 +/- 21.7 %\n",
            "         \n",
            "\n",
            "Batch: 12   Training loss: 0.15946361422538757\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 13   Training loss: 0.15224094688892365\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 14   Training loss: 0.15409377217292786\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 15   Training loss: 0.1687316745519638\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 16   Training loss: 0.1694587618112564\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 17   Training loss: 0.15887077152729034\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 18   Training loss: 0.1626356542110443\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 19   Training loss: 0.17685666680335999\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 20   Training loss: 0.17278079688549042\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 21   Training loss: 0.16240112483501434\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 22   Training loss: 0.15236160159111023\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 23   Training loss: 0.16875675320625305\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 24   Training loss: 0.17886917293071747\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 25   Training loss: 0.1566731333732605\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 26   Training loss: 0.14925695955753326\n",
            "Precision: 65.69 +/- 40.1 %\n",
            "Recall: 29.91 +/- 22.8 %\n",
            "         \n",
            "\n",
            "Batch: 27   Training loss: 0.1689389944076538\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 28   Training loss: 0.16148161888122559\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 29   Training loss: 0.17180292308330536\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 30   Training loss: 0.15166336297988892\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 31   Training loss: 0.18604159355163574\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 32   Training loss: 0.15590839087963104\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 33   Training loss: 0.16849184036254883\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 34   Training loss: 0.17649582028388977\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 35   Training loss: 0.1627289205789566\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 36   Training loss: 0.17542234063148499\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 37   Training loss: 0.18492546677589417\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 38   Training loss: 0.17361082136631012\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 39   Training loss: 0.16967037320137024\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 40   Training loss: 0.17435595393180847\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 41   Training loss: 0.16322389245033264\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 42   Training loss: 0.14935463666915894\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 43   Training loss: 0.15544913709163666\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 44   Training loss: 0.15747956931591034\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 45   Training loss: 0.15821053087711334\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 46   Training loss: 0.17198114097118378\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 47   Training loss: 0.16750988364219666\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 48   Training loss: 0.1583658754825592\n",
            "Precision: 60.08 +/- 38.3 %\n",
            "Recall: 28.78 +/- 21.3 %\n",
            "         \n",
            "\n",
            "Batch: 49   Training loss: 0.16102156043052673\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 50   Training loss: 0.17120680212974548\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 51   Training loss: 0.1619700789451599\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 52   Training loss: 0.16264964640140533\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 53   Training loss: 0.15009793639183044\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 54   Training loss: 0.14960359036922455\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 55   Training loss: 0.17538532614707947\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 56   Training loss: 0.15470434725284576\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 57   Training loss: 0.14952540397644043\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 58   Training loss: 0.15696385502815247\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 59   Training loss: 0.16869667172431946\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 60   Training loss: 0.15567338466644287\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 61   Training loss: 0.13826048374176025\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 62   Training loss: 0.1546655297279358\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 63   Training loss: 0.14038169384002686\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 64   Training loss: 0.1548294574022293\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 65   Training loss: 0.17251229286193848\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 66   Training loss: 0.18350021541118622\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 67   Training loss: 0.16756382584571838\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 68   Training loss: 0.1750611513853073\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 69   Training loss: 0.15700754523277283\n",
            "Precision: 72.05 +/- 37.3 %\n",
            "Recall: 29.69 +/- 20.7 %\n",
            "         \n",
            "\n",
            "Batch: 70   Training loss: 0.17206422984600067\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 71   Training loss: 0.15934932231903076\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 72   Training loss: 0.15908260643482208\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 73   Training loss: 0.15680336952209473\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 74   Training loss: 0.16962462663650513\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 75   Training loss: 0.16054609417915344\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 76   Training loss: 0.1543423980474472\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 77   Training loss: 0.1577652394771576\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 78   Training loss: 0.15975011885166168\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 79   Training loss: 0.1574513465166092\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 80   Training loss: 0.17712487280368805\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 81   Training loss: 0.14243339002132416\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 82   Training loss: 0.16065560281276703\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 83   Training loss: 0.19692066311836243\n",
            "Precision: 65.11 +/- 36.6 %\n",
            "Recall: 25.57 +/- 15.5 %\n",
            "         \n",
            "\n",
            "Batch: 84   Training loss: 0.16596151888370514\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 85   Training loss: 0.17424173653125763\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 86   Training loss: 0.16800127923488617\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 87   Training loss: 0.16416700184345245\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 88   Training loss: 0.16148076951503754\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 89   Training loss: 0.1578436940908432\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 90   Training loss: 0.16631367802619934\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 91   Training loss: 0.17062723636627197\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 92   Training loss: 0.1447867453098297\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 93   Training loss: 0.1505129635334015\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 94   Training loss: 0.1525726467370987\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 95   Training loss: 0.16501985490322113\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 96   Training loss: 0.16442355513572693\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 97   Training loss: 0.15612569451332092\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 98   Training loss: 0.16349048912525177\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 99   Training loss: 0.15748247504234314\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 100   Training loss: 0.17843370139598846\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 101   Training loss: 0.16396445035934448\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 102   Training loss: 0.16628174483776093\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 103   Training loss: 0.15648230910301208\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 104   Training loss: 0.16450943052768707\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 105   Training loss: 0.15524347126483917\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 106   Training loss: 0.15884944796562195\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 107   Training loss: 0.15492883324623108\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 108   Training loss: 0.14195062220096588\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 109   Training loss: 0.17451956868171692\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 110   Training loss: 0.1755581796169281\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 111   Training loss: 0.14787942171096802\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 112   Training loss: 0.1553695648908615\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 113   Training loss: 0.15269328653812408\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 114   Training loss: 0.17242398858070374\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 115   Training loss: 0.14664246141910553\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 116   Training loss: 0.14618004858493805\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 117   Training loss: 0.15830183029174805\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 118   Training loss: 0.17372390627861023\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 119   Training loss: 0.16262856125831604\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 120   Training loss: 0.16022975742816925\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 121   Training loss: 0.18371537327766418\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 122   Training loss: 0.15591013431549072\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 123   Training loss: 0.15610522031784058\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 124   Training loss: 0.148055762052536\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 125   Training loss: 0.16843800246715546\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 126   Training loss: 0.1755293607711792\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 127   Training loss: 0.1572747677564621\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 128   Training loss: 0.15713296830654144\n",
            "Precision: 72.14 +/- 35.7 %\n",
            "Recall: 32.31 +/- 18.4 %\n",
            "         \n",
            "\n",
            "Batch: 129   Training loss: 0.18119704723358154\n",
            "Precision: 67.64 +/- 35.8 %\n",
            "Recall: 33.54 +/- 17.3 %\n",
            "         \n",
            "\n",
            "Batch: 130   Training loss: 0.17585858702659607\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 131   Training loss: 0.1587984412908554\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 132   Training loss: 0.16322214901447296\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 133   Training loss: 0.15719899535179138\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 134   Training loss: 0.14839854836463928\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 135   Training loss: 0.16508345305919647\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 136   Training loss: 0.152660071849823\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 137   Training loss: 0.1645292490720749\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 138   Training loss: 0.17052927613258362\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 139   Training loss: 0.1553048938512802\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 140   Training loss: 0.1604824960231781\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 141   Training loss: 0.14952614903450012\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 142   Training loss: 0.1781524419784546\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 143   Training loss: 0.1347796618938446\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 144   Training loss: 0.18662117421627045\n",
            "Precision: 62.22 +/- 33.4 %\n",
            "Recall: 31.83 +/- 17.9 %\n",
            "         \n",
            "\n",
            "Batch: 145   Training loss: 0.15293793380260468\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 146   Training loss: 0.14917975664138794\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 147   Training loss: 0.1605447679758072\n",
            "Precision: 66.10 +/- 38.6 %\n",
            "Recall: 27.76 +/- 20.7 %\n",
            "         \n",
            "\n",
            "Batch: 148   Training loss: 0.17622047662734985\n",
            "Precision: 62.33 +/- 44.1 %\n",
            "Recall: 23.59 +/- 18.3 %\n",
            "         \n",
            "\n",
            "Batch: 149   Training loss: 0.16607524454593658\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 150   Training loss: 0.14289450645446777\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 151   Training loss: 0.1650184839963913\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 152   Training loss: 0.15277735888957977\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 153   Training loss: 0.15279431641101837\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 154   Training loss: 0.15906886756420135\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 155   Training loss: 0.15790651738643646\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 156   Training loss: 0.1602463573217392\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 157   Training loss: 0.16183097660541534\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 158   Training loss: 0.14683914184570312\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 159   Training loss: 0.15094806253910065\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 160   Training loss: 0.16601036489009857\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 161   Training loss: 0.15657493472099304\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 162   Training loss: 0.15835262835025787\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 163   Training loss: 0.1563737541437149\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 164   Training loss: 0.15517348051071167\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 165   Training loss: 0.16695134341716766\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 166   Training loss: 0.18726783990859985\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 167   Training loss: 0.15699829161167145\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 168   Training loss: 0.1482304185628891\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 169   Training loss: 0.16411034762859344\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 170   Training loss: 0.157459557056427\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 171   Training loss: 0.15615606307983398\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 172   Training loss: 0.16146954894065857\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 173   Training loss: 0.14314024150371552\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 174   Training loss: 0.1682690680027008\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 175   Training loss: 0.1369139850139618\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 176   Training loss: 0.17361976206302643\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 177   Training loss: 0.15705075860023499\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 178   Training loss: 0.16867096722126007\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 179   Training loss: 0.16398510336875916\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 180   Training loss: 0.14065134525299072\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 181   Training loss: 0.15325811505317688\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 182   Training loss: 0.16216671466827393\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 183   Training loss: 0.15790417790412903\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 184   Training loss: 0.17696596682071686\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 185   Training loss: 0.1457136869430542\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 186   Training loss: 0.15480369329452515\n",
            "Precision: 68.18 +/- 41.4 %\n",
            "Recall: 28.71 +/- 23.3 %\n",
            "         \n",
            "\n",
            "Batch: 187   Training loss: 0.15800657868385315\n",
            "Precision: 68.25 +/- 39.4 %\n",
            "Recall: 24.62 +/- 18.0 %\n",
            "         \n",
            "\n",
            "Batch: 188   Training loss: 0.1724335253238678\n",
            "Precision: 69.32 +/- 42.2 %\n",
            "Recall: 22.95 +/- 16.5 %\n",
            "         \n",
            "\n",
            "Batch: 189   Training loss: 0.1699286252260208\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 190   Training loss: 0.146719828248024\n",
            "Precision: 75.42 +/- 35.8 %\n",
            "Recall: 28.00 +/- 16.3 %\n",
            "         \n",
            "\n",
            "Batch: 191   Training loss: 0.15595385432243347\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 192   Training loss: 0.16340260207653046\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 193   Training loss: 0.15981410443782806\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 194   Training loss: 0.1497400403022766\n",
            "Precision: 66.40 +/- 37.6 %\n",
            "Recall: 31.74 +/- 23.1 %\n",
            "         \n",
            "\n",
            "Batch: 195   Training loss: 0.15640708804130554\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 196   Training loss: 0.14504395425319672\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 197   Training loss: 0.1630125194787979\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 198   Training loss: 0.1683255136013031\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 199   Training loss: 0.18202348053455353\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 200   Training loss: 0.1556910276412964\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 201   Training loss: 0.15509071946144104\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 202   Training loss: 0.16056400537490845\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 203   Training loss: 0.15231743454933167\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 204   Training loss: 0.1673717051744461\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 205   Training loss: 0.15499171614646912\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 206   Training loss: 0.16039837896823883\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 207   Training loss: 0.1776798963546753\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 208   Training loss: 0.15885980427265167\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 209   Training loss: 0.16418883204460144\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 210   Training loss: 0.16357778012752533\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 211   Training loss: 0.16765399277210236\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 212   Training loss: 0.16427332162857056\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 213   Training loss: 0.1568136066198349\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 214   Training loss: 0.16699543595314026\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 215   Training loss: 0.15002287924289703\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 216   Training loss: 0.150868222117424\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 217   Training loss: 0.16325527429580688\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 218   Training loss: 0.1482999622821808\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 219   Training loss: 0.16286411881446838\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 220   Training loss: 0.16528651118278503\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 221   Training loss: 0.15059331059455872\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 222   Training loss: 0.14237384498119354\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 223   Training loss: 0.14214539527893066\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 224   Training loss: 0.17126420140266418\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 225   Training loss: 0.1655394285917282\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 226   Training loss: 0.1770818829536438\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 227   Training loss: 0.15480907261371613\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 228   Training loss: 0.14084941148757935\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 229   Training loss: 0.15003107488155365\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 230   Training loss: 0.15012897551059723\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 231   Training loss: 0.1734834760427475\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 232   Training loss: 0.16201934218406677\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 233   Training loss: 0.1600920557975769\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 234   Training loss: 0.1805112212896347\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 235   Training loss: 0.16521376371383667\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 236   Training loss: 0.14898017048835754\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 237   Training loss: 0.1727304905653\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 238   Training loss: 0.16362258791923523\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 239   Training loss: 0.15704603493213654\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 240   Training loss: 0.1588229387998581\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 241   Training loss: 0.17072917520999908\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 242   Training loss: 0.15984125435352325\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 243   Training loss: 0.15521672368049622\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 244   Training loss: 0.15194976329803467\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 245   Training loss: 0.1494539976119995\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 246   Training loss: 0.13432447612285614\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 247   Training loss: 0.1572093963623047\n",
            "Precision: 70.17 +/- 41.9 %\n",
            "Recall: 23.72 +/- 21.4 %\n",
            "         \n",
            "\n",
            "Batch: 248   Training loss: 0.1767764538526535\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 249   Training loss: 0.18455244600772858\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 250   Training loss: 0.16775625944137573\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 251   Training loss: 0.15402573347091675\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 252   Training loss: 0.1646845042705536\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 253   Training loss: 0.16658101975917816\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 254   Training loss: 0.1661616712808609\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 255   Training loss: 0.15016864240169525\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 256   Training loss: 0.17489126324653625\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 257   Training loss: 0.14913591742515564\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 258   Training loss: 0.16422364115715027\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 259   Training loss: 0.15084686875343323\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 260   Training loss: 0.14683401584625244\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 261   Training loss: 0.152723491191864\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 262   Training loss: 0.1706375926733017\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 263   Training loss: 0.1582498997449875\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 264   Training loss: 0.16562896966934204\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 265   Training loss: 0.15876589715480804\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 266   Training loss: 0.16053515672683716\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 267   Training loss: 0.15312224626541138\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 268   Training loss: 0.15626253187656403\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 269   Training loss: 0.1550239473581314\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 270   Training loss: 0.15576408803462982\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 271   Training loss: 0.15744875371456146\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 272   Training loss: 0.1645442247390747\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 273   Training loss: 0.15078423917293549\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 274   Training loss: 0.15546761453151703\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 275   Training loss: 0.1519840806722641\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 276   Training loss: 0.13416920602321625\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 277   Training loss: 0.1814742237329483\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 278   Training loss: 0.15164698660373688\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 279   Training loss: 0.15722374618053436\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 280   Training loss: 0.15466497838497162\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 281   Training loss: 0.15022502839565277\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 282   Training loss: 0.17502976953983307\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 283   Training loss: 0.15890268981456757\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 284   Training loss: 0.15866893529891968\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 285   Training loss: 0.1626289039850235\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 286   Training loss: 0.1567094326019287\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 287   Training loss: 0.14974436163902283\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 288   Training loss: 0.1473936140537262\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 289   Training loss: 0.17190489172935486\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 290   Training loss: 0.1642538458108902\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 291   Training loss: 0.16917458176612854\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 292   Training loss: 0.14205040037631989\n",
            "Positive samples missing, skipping\n",
            "start\n",
            "         \n",
            "\n",
            "Batch: 0   Training loss: 0.15234415233135223\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 1   Training loss: 0.165145143866539\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 2   Training loss: 0.14875714480876923\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 3   Training loss: 0.14368295669555664\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 4   Training loss: 0.1580389142036438\n",
            "Precision: 68.44 +/- 37.9 %\n",
            "Recall: 29.88 +/- 21.1 %\n",
            "         \n",
            "\n",
            "Batch: 5   Training loss: 0.1548030972480774\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 6   Training loss: 0.14841638505458832\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 7   Training loss: 0.14252832531929016\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 8   Training loss: 0.17944012582302094\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 9   Training loss: 0.14834076166152954\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 10   Training loss: 0.15644386410713196\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 11   Training loss: 0.16102008521556854\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 12   Training loss: 0.15846817195415497\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 13   Training loss: 0.1716083437204361\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 14   Training loss: 0.17020457983016968\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 15   Training loss: 0.16403579711914062\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 16   Training loss: 0.16945861279964447\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 17   Training loss: 0.16082914173603058\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 18   Training loss: 0.152027428150177\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 19   Training loss: 0.16687454283237457\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 20   Training loss: 0.14488758146762848\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 21   Training loss: 0.1594737470149994\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 22   Training loss: 0.16705641150474548\n",
            "Precision: 64.66 +/- 40.5 %\n",
            "Recall: 30.57 +/- 19.2 %\n",
            "         \n",
            "\n",
            "Batch: 23   Training loss: 0.15919619798660278\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 24   Training loss: 0.15731309354305267\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 25   Training loss: 0.1553025096654892\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 26   Training loss: 0.1691521406173706\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 27   Training loss: 0.17341797053813934\n",
            "Precision: 73.91 +/- 33.9 %\n",
            "Recall: 30.80 +/- 18.7 %\n",
            "         \n",
            "\n",
            "Batch: 28   Training loss: 0.16553199291229248\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 29   Training loss: 0.1532563716173172\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 30   Training loss: 0.16134211421012878\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 31   Training loss: 0.16139446198940277\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 32   Training loss: 0.14882367849349976\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 33   Training loss: 0.16382788121700287\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 34   Training loss: 0.14757587015628815\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 35   Training loss: 0.15902550518512726\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 36   Training loss: 0.14049731194972992\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 37   Training loss: 0.13752512633800507\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 38   Training loss: 0.15282578766345978\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 39   Training loss: 0.14428266882896423\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 40   Training loss: 0.15398800373077393\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 41   Training loss: 0.1511254608631134\n",
            "Precision: 66.97 +/- 35.7 %\n",
            "Recall: 35.97 +/- 21.3 %\n",
            "         \n",
            "\n",
            "Batch: 42   Training loss: 0.15562506020069122\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 43   Training loss: 0.15685822069644928\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 44   Training loss: 0.15905138850212097\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 45   Training loss: 0.14639559388160706\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 46   Training loss: 0.14567998051643372\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 47   Training loss: 0.1633971631526947\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 48   Training loss: 0.1581578403711319\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 49   Training loss: 0.15280960500240326\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 50   Training loss: 0.154458686709404\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 51   Training loss: 0.17160241305828094\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 52   Training loss: 0.16718508303165436\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 53   Training loss: 0.16544434428215027\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 54   Training loss: 0.1447717398405075\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 55   Training loss: 0.15235251188278198\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 56   Training loss: 0.17664365470409393\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 57   Training loss: 0.16014455258846283\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 58   Training loss: 0.15157382190227509\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 59   Training loss: 0.1404418796300888\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 60   Training loss: 0.15132369101047516\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 61   Training loss: 0.14594332873821259\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 62   Training loss: 0.16189885139465332\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 63   Training loss: 0.15638458728790283\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 64   Training loss: 0.17463372647762299\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 65   Training loss: 0.18566004931926727\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 66   Training loss: 0.16225871443748474\n",
            "Precision: 68.53 +/- 36.5 %\n",
            "Recall: 30.96 +/- 20.7 %\n",
            "         \n",
            "\n",
            "Batch: 67   Training loss: 0.16596929728984833\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 68   Training loss: 0.1704769879579544\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 69   Training loss: 0.15055350959300995\n",
            "Precision: 69.63 +/- 36.7 %\n",
            "Recall: 36.83 +/- 23.2 %\n",
            "         \n",
            "\n",
            "Batch: 70   Training loss: 0.15723024308681488\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 71   Training loss: 0.15759451687335968\n",
            "Precision: 57.06 +/- 39.3 %\n",
            "Recall: 25.29 +/- 20.2 %\n",
            "         \n",
            "\n",
            "Batch: 72   Training loss: 0.16745662689208984\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 73   Training loss: 0.16554641723632812\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 74   Training loss: 0.14071807265281677\n",
            "Precision: 83.62 +/- 30.7 %\n",
            "Recall: 33.66 +/- 20.2 %\n",
            "         \n",
            "\n",
            "Batch: 75   Training loss: 0.16800129413604736\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 76   Training loss: 0.15803074836730957\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 77   Training loss: 0.16555942595005035\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 78   Training loss: 0.151385098695755\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 79   Training loss: 0.13625648617744446\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 80   Training loss: 0.17285969853401184\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 81   Training loss: 0.14684318006038666\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 82   Training loss: 0.17252440750598907\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 83   Training loss: 0.1577778160572052\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 84   Training loss: 0.1400448977947235\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 85   Training loss: 0.14962226152420044\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 86   Training loss: 0.14896069467067719\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 87   Training loss: 0.1797959804534912\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 88   Training loss: 0.15560951828956604\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 89   Training loss: 0.17962825298309326\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 90   Training loss: 0.17051570117473602\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 91   Training loss: 0.16791701316833496\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 92   Training loss: 0.16010229289531708\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 93   Training loss: 0.17178472876548767\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 94   Training loss: 0.15200862288475037\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 95   Training loss: 0.16137318313121796\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 96   Training loss: 0.15073221921920776\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 97   Training loss: 0.1692270189523697\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 98   Training loss: 0.1617078334093094\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 99   Training loss: 0.17245285212993622\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 100   Training loss: 0.19760854542255402\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 101   Training loss: 0.17596553266048431\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 102   Training loss: 0.1658230572938919\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 103   Training loss: 0.1524953842163086\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 104   Training loss: 0.16583183407783508\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 105   Training loss: 0.14399057626724243\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 106   Training loss: 0.16047433018684387\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 107   Training loss: 0.16349202394485474\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 108   Training loss: 0.18581408262252808\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 109   Training loss: 0.15076254308223724\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 110   Training loss: 0.15357308089733124\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 111   Training loss: 0.15946096181869507\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 112   Training loss: 0.1534072309732437\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 113   Training loss: 0.15691907703876495\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 114   Training loss: 0.17194202542304993\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 115   Training loss: 0.1445731222629547\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 116   Training loss: 0.16892868280410767\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 117   Training loss: 0.16098761558532715\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 118   Training loss: 0.18457889556884766\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 119   Training loss: 0.16919711232185364\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 120   Training loss: 0.15045243501663208\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 121   Training loss: 0.15605482459068298\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 122   Training loss: 0.16825060546398163\n",
            "Precision: 66.78 +/- 38.8 %\n",
            "Recall: 31.23 +/- 19.8 %\n",
            "         \n",
            "\n",
            "Batch: 123   Training loss: 0.16792626678943634\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 124   Training loss: 0.14171995222568512\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 125   Training loss: 0.18535484373569489\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 126   Training loss: 0.15380457043647766\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 127   Training loss: 0.15140457451343536\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 128   Training loss: 0.1606406569480896\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 129   Training loss: 0.17394407093524933\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 130   Training loss: 0.14968958497047424\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 131   Training loss: 0.14483432471752167\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 132   Training loss: 0.16789284348487854\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 133   Training loss: 0.17615211009979248\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 134   Training loss: 0.16577577590942383\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 135   Training loss: 0.16006217896938324\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 136   Training loss: 0.1525043100118637\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 137   Training loss: 0.15609779953956604\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 138   Training loss: 0.16270148754119873\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 139   Training loss: 0.17300599813461304\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 140   Training loss: 0.1663479059934616\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 141   Training loss: 0.15334048867225647\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 142   Training loss: 0.13351409137248993\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 143   Training loss: 0.17948544025421143\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 144   Training loss: 0.1563938707113266\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 145   Training loss: 0.16628235578536987\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 146   Training loss: 0.1525205373764038\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 147   Training loss: 0.15833620727062225\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 148   Training loss: 0.14820000529289246\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 149   Training loss: 0.18159592151641846\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 150   Training loss: 0.15602338314056396\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 151   Training loss: 0.17249052226543427\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 152   Training loss: 0.159410297870636\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 153   Training loss: 0.17260806262493134\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 154   Training loss: 0.14353545010089874\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 155   Training loss: 0.1468779742717743\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 156   Training loss: 0.15874890983104706\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 157   Training loss: 0.16548793017864227\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 158   Training loss: 0.16360099613666534\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 159   Training loss: 0.1697508543729782\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 160   Training loss: 0.1564263552427292\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 161   Training loss: 0.18406890332698822\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 162   Training loss: 0.16750787198543549\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 163   Training loss: 0.16123276948928833\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 164   Training loss: 0.16738587617874146\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 165   Training loss: 0.14310681819915771\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 166   Training loss: 0.14921779930591583\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 167   Training loss: 0.16058103740215302\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 168   Training loss: 0.1605358123779297\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 169   Training loss: 0.1638229340314865\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 170   Training loss: 0.16962787508964539\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 171   Training loss: 0.1640162616968155\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 172   Training loss: 0.14985907077789307\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 173   Training loss: 0.14543864130973816\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 174   Training loss: 0.15605098009109497\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 175   Training loss: 0.16068774461746216\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 176   Training loss: 0.15716196596622467\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 177   Training loss: 0.1648319512605667\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 178   Training loss: 0.1680975705385208\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 179   Training loss: 0.1547461599111557\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 180   Training loss: 0.16029702126979828\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 181   Training loss: 0.14434094727039337\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 182   Training loss: 0.15800483524799347\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 183   Training loss: 0.16095679998397827\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 184   Training loss: 0.15536652505397797\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 185   Training loss: 0.1656692624092102\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 186   Training loss: 0.1363079845905304\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 187   Training loss: 0.14992418885231018\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 188   Training loss: 0.15715989470481873\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 189   Training loss: 0.16264039278030396\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 190   Training loss: 0.15273576974868774\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 191   Training loss: 0.16616462171077728\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 192   Training loss: 0.15909166634082794\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 193   Training loss: 0.14726322889328003\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 194   Training loss: 0.15809759497642517\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 195   Training loss: 0.14373081922531128\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 196   Training loss: 0.15427379310131073\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 197   Training loss: 0.17810370028018951\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 198   Training loss: 0.15827904641628265\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 199   Training loss: 0.1559373140335083\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 200   Training loss: 0.15918366611003876\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 201   Training loss: 0.15355545282363892\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 202   Training loss: 0.15626786649227142\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 203   Training loss: 0.16498352587223053\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 204   Training loss: 0.1601434201002121\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 205   Training loss: 0.19299638271331787\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 206   Training loss: 0.17268431186676025\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 207   Training loss: 0.15948714315891266\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 208   Training loss: 0.16957882046699524\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 209   Training loss: 0.15986795723438263\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 210   Training loss: 0.15251104533672333\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 211   Training loss: 0.1541958451271057\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 212   Training loss: 0.16758783161640167\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 213   Training loss: 0.1698668748140335\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 214   Training loss: 0.15121379494667053\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 215   Training loss: 0.13888663053512573\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 216   Training loss: 0.15580236911773682\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 217   Training loss: 0.15485280752182007\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 218   Training loss: 0.17645226418972015\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 219   Training loss: 0.15717214345932007\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 220   Training loss: 0.14893724024295807\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 221   Training loss: 0.1475187987089157\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 222   Training loss: 0.17561601102352142\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 223   Training loss: 0.16059157252311707\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 224   Training loss: 0.164545938372612\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 225   Training loss: 0.16098915040493011\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 226   Training loss: 0.15302634239196777\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 227   Training loss: 0.16109919548034668\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 228   Training loss: 0.1662120819091797\n",
            "Precision: 59.00 +/- 36.6 %\n",
            "Recall: 27.73 +/- 20.2 %\n",
            "         \n",
            "\n",
            "Batch: 229   Training loss: 0.15823592245578766\n",
            "Precision: 70.85 +/- 39.5 %\n",
            "Recall: 29.96 +/- 22.7 %\n",
            "         \n",
            "\n",
            "Batch: 230   Training loss: 0.18247249722480774\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 231   Training loss: 0.140413299202919\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 232   Training loss: 0.1497938334941864\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 233   Training loss: 0.15525472164154053\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 234   Training loss: 0.17497557401657104\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 235   Training loss: 0.16621583700180054\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 236   Training loss: 0.1571359485387802\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 237   Training loss: 0.15637508034706116\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 238   Training loss: 0.1428719460964203\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 239   Training loss: 0.14514213800430298\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 240   Training loss: 0.14195752143859863\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 241   Training loss: 0.16072048246860504\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 242   Training loss: 0.1433451622724533\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 243   Training loss: 0.1523868292570114\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 244   Training loss: 0.14505881071090698\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 245   Training loss: 0.16777706146240234\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 246   Training loss: 0.16339989006519318\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 247   Training loss: 0.17341190576553345\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 248   Training loss: 0.15468308329582214\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 249   Training loss: 0.16113516688346863\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 250   Training loss: 0.14574070274829865\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 251   Training loss: 0.16274969279766083\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 252   Training loss: 0.1566869020462036\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 253   Training loss: 0.15963001549243927\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 254   Training loss: 0.16098807752132416\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 255   Training loss: 0.16071350872516632\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 256   Training loss: 0.14900408685207367\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 257   Training loss: 0.1726565808057785\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 258   Training loss: 0.15926888585090637\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 259   Training loss: 0.16669245064258575\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 260   Training loss: 0.17236919701099396\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 261   Training loss: 0.15398892760276794\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 262   Training loss: 0.16220024228096008\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 263   Training loss: 0.16304254531860352\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 264   Training loss: 0.16891717910766602\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 265   Training loss: 0.16075770556926727\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 266   Training loss: 0.15908567607402802\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 267   Training loss: 0.18696460127830505\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 268   Training loss: 0.15019582211971283\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 269   Training loss: 0.16186369955539703\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 270   Training loss: 0.1575879454612732\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 271   Training loss: 0.16307063400745392\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 272   Training loss: 0.15463370084762573\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 273   Training loss: 0.16647572815418243\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 274   Training loss: 0.1664484292268753\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 275   Training loss: 0.16884303092956543\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 276   Training loss: 0.15598033368587494\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 277   Training loss: 0.16081106662750244\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 278   Training loss: 0.16496042907238007\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 279   Training loss: 0.1597023606300354\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 280   Training loss: 0.15487883985042572\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 281   Training loss: 0.1505674421787262\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 282   Training loss: 0.1522127240896225\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 283   Training loss: 0.1501941680908203\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 284   Training loss: 0.16328613460063934\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 285   Training loss: 0.16040544211864471\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 286   Training loss: 0.17726834118366241\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 287   Training loss: 0.16611909866333008\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 288   Training loss: 0.15316461026668549\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 289   Training loss: 0.16633033752441406\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 290   Training loss: 0.16095194220542908\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 291   Training loss: 0.1676226407289505\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 292   Training loss: 0.16846346855163574\n",
            "Precision: 49.85 +/- 38.6 %\n",
            "Recall: 26.28 +/- 23.8 %\n",
            "start\n",
            "         \n",
            "\n",
            "Batch: 0   Training loss: 0.13996613025665283\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 1   Training loss: 0.1642856001853943\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 2   Training loss: 0.15672490000724792\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 3   Training loss: 0.1753724068403244\n",
            "Precision: 59.52 +/- 41.8 %\n",
            "Recall: 24.64 +/- 17.6 %\n",
            "         \n",
            "\n",
            "Batch: 4   Training loss: 0.14205339550971985\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 5   Training loss: 0.144403874874115\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 6   Training loss: 0.1680164635181427\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 7   Training loss: 0.15186604857444763\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 8   Training loss: 0.15207330882549286\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 9   Training loss: 0.13580544292926788\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 10   Training loss: 0.16385811567306519\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 11   Training loss: 0.17420443892478943\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 12   Training loss: 0.15237712860107422\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 13   Training loss: 0.14503508806228638\n",
            "Precision: 59.39 +/- 40.3 %\n",
            "Recall: 32.88 +/- 23.5 %\n",
            "         \n",
            "\n",
            "Batch: 14   Training loss: 0.15649265050888062\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 15   Training loss: 0.15730556845664978\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 16   Training loss: 0.1591872274875641\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 17   Training loss: 0.1574268639087677\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 18   Training loss: 0.16460254788398743\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 19   Training loss: 0.1589500457048416\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 20   Training loss: 0.14776982367038727\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 21   Training loss: 0.14362992346286774\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 22   Training loss: 0.15487536787986755\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 23   Training loss: 0.14138182997703552\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 24   Training loss: 0.16710016131401062\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 25   Training loss: 0.16957908868789673\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 26   Training loss: 0.14190469682216644\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 27   Training loss: 0.1607728898525238\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 28   Training loss: 0.1538264900445938\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 29   Training loss: 0.15797947347164154\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 30   Training loss: 0.14205016195774078\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 31   Training loss: 0.15170665085315704\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 32   Training loss: 0.15768782794475555\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 33   Training loss: 0.157245472073555\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 34   Training loss: 0.1528012752532959\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 35   Training loss: 0.1680726855993271\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 36   Training loss: 0.17201340198516846\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 37   Training loss: 0.14914408326148987\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 38   Training loss: 0.15396180748939514\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 39   Training loss: 0.1598319113254547\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 40   Training loss: 0.15681767463684082\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 41   Training loss: 0.1326855719089508\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 42   Training loss: 0.1421252340078354\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 43   Training loss: 0.1590089052915573\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 44   Training loss: 0.15231050550937653\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 45   Training loss: 0.15319965779781342\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 46   Training loss: 0.16987347602844238\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 47   Training loss: 0.16128037869930267\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 48   Training loss: 0.17039528489112854\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 49   Training loss: 0.15394996106624603\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 50   Training loss: 0.15246468782424927\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 51   Training loss: 0.1617659628391266\n",
            "Precision: 79.38 +/- 31.6 %\n",
            "Recall: 33.17 +/- 18.4 %\n",
            "         \n",
            "\n",
            "Batch: 52   Training loss: 0.15919949114322662\n",
            "Precision: 67.94 +/- 38.4 %\n",
            "Recall: 29.03 +/- 19.4 %\n",
            "         \n",
            "\n",
            "Batch: 53   Training loss: 0.17334313690662384\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 54   Training loss: 0.153945654630661\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 55   Training loss: 0.173910990357399\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 56   Training loss: 0.14489033818244934\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 57   Training loss: 0.1577119380235672\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 58   Training loss: 0.1573084443807602\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 59   Training loss: 0.17996472120285034\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 60   Training loss: 0.1455363631248474\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 61   Training loss: 0.17396947741508484\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 62   Training loss: 0.15908007323741913\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 63   Training loss: 0.17026737332344055\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 64   Training loss: 0.15604464709758759\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 65   Training loss: 0.16716662049293518\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 66   Training loss: 0.15326279401779175\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 67   Training loss: 0.16215267777442932\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 68   Training loss: 0.1391667276620865\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 69   Training loss: 0.15688443183898926\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 70   Training loss: 0.15713642537593842\n",
            "Precision: 74.26 +/- 28.8 %\n",
            "Recall: 35.08 +/- 18.2 %\n",
            "         \n",
            "\n",
            "Batch: 71   Training loss: 0.15212467312812805\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 72   Training loss: 0.17112791538238525\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 73   Training loss: 0.1627502739429474\n",
            "Precision: 69.49 +/- 37.4 %\n",
            "Recall: 28.21 +/- 19.9 %\n",
            "         \n",
            "\n",
            "Batch: 74   Training loss: 0.14856836199760437\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 75   Training loss: 0.1539972424507141\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 76   Training loss: 0.16466264426708221\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 77   Training loss: 0.1722266972064972\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 78   Training loss: 0.1703069806098938\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 79   Training loss: 0.17184992134571075\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 80   Training loss: 0.15744157135486603\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 81   Training loss: 0.16385415196418762\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 82   Training loss: 0.1585041582584381\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 83   Training loss: 0.15275149047374725\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 84   Training loss: 0.1608372926712036\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 85   Training loss: 0.16924640536308289\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 86   Training loss: 0.15015828609466553\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 87   Training loss: 0.17209993302822113\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 88   Training loss: 0.14754770696163177\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 89   Training loss: 0.16219906508922577\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 90   Training loss: 0.1593128740787506\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 91   Training loss: 0.15335983037948608\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 92   Training loss: 0.1550227254629135\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 93   Training loss: 0.15391430258750916\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 94   Training loss: 0.14187686145305634\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 95   Training loss: 0.16436885297298431\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 96   Training loss: 0.16510824859142303\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 97   Training loss: 0.16131873428821564\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 98   Training loss: 0.15792350471019745\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 99   Training loss: 0.14880478382110596\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 100   Training loss: 0.16047754883766174\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 101   Training loss: 0.1578248292207718\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 102   Training loss: 0.15373404324054718\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 103   Training loss: 0.12799863517284393\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 104   Training loss: 0.1469481736421585\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 105   Training loss: 0.1424240916967392\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 106   Training loss: 0.15703904628753662\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 107   Training loss: 0.17489545047283173\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 108   Training loss: 0.161268413066864\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 109   Training loss: 0.1640453338623047\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 110   Training loss: 0.1483048051595688\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 111   Training loss: 0.15739624202251434\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 112   Training loss: 0.16676415503025055\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 113   Training loss: 0.14753496646881104\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 114   Training loss: 0.17374470829963684\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 115   Training loss: 0.16504833102226257\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 116   Training loss: 0.16404595971107483\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 117   Training loss: 0.15916982293128967\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 118   Training loss: 0.16234362125396729\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 119   Training loss: 0.16030040383338928\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 120   Training loss: 0.16733112931251526\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 121   Training loss: 0.1702762246131897\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 122   Training loss: 0.16147688031196594\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 123   Training loss: 0.15479342639446259\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 124   Training loss: 0.14152292907238007\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 125   Training loss: 0.14780448377132416\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 126   Training loss: 0.18449702858924866\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 127   Training loss: 0.1552293598651886\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 128   Training loss: 0.16367307305335999\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 129   Training loss: 0.15817256271839142\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 130   Training loss: 0.16393324732780457\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 131   Training loss: 0.15882009267807007\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 132   Training loss: 0.1431645303964615\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 133   Training loss: 0.17190766334533691\n",
            "Precision: 71.85 +/- 40.0 %\n",
            "Recall: 25.32 +/- 18.4 %\n",
            "         \n",
            "\n",
            "Batch: 134   Training loss: 0.15692684054374695\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 135   Training loss: 0.15481062233448029\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 136   Training loss: 0.1464417427778244\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 137   Training loss: 0.1867014616727829\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 138   Training loss: 0.1733524650335312\n",
            "Precision: 79.83 +/- 30.0 %\n",
            "Recall: 33.74 +/- 17.2 %\n",
            "         \n",
            "\n",
            "Batch: 139   Training loss: 0.15732820332050323\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 140   Training loss: 0.16425500810146332\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 141   Training loss: 0.15031935274600983\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 142   Training loss: 0.1555216908454895\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 143   Training loss: 0.1683262139558792\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 144   Training loss: 0.15284191071987152\n",
            "Precision: 82.41 +/- 23.5 %\n",
            "Recall: 37.85 +/- 19.6 %\n",
            "         \n",
            "\n",
            "Batch: 145   Training loss: 0.15149334073066711\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 146   Training loss: 0.15577836334705353\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 147   Training loss: 0.17810948193073273\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 148   Training loss: 0.13379397988319397\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 149   Training loss: 0.1670990288257599\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 150   Training loss: 0.17519353330135345\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 151   Training loss: 0.1716620922088623\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 152   Training loss: 0.16287624835968018\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 153   Training loss: 0.16817441582679749\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 154   Training loss: 0.15670953691005707\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 155   Training loss: 0.1600898951292038\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 156   Training loss: 0.1586441844701767\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 157   Training loss: 0.1707853525876999\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 158   Training loss: 0.16713230311870575\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 159   Training loss: 0.16643093526363373\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 160   Training loss: 0.15483681857585907\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 161   Training loss: 0.16751842200756073\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 162   Training loss: 0.16444383561611176\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 163   Training loss: 0.16363625228405\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 164   Training loss: 0.14749079942703247\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 165   Training loss: 0.16217277944087982\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 166   Training loss: 0.1579241305589676\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 167   Training loss: 0.16032984852790833\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 168   Training loss: 0.154896542429924\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 169   Training loss: 0.14688880741596222\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 170   Training loss: 0.15632565319538116\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 171   Training loss: 0.15891563892364502\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 172   Training loss: 0.14197349548339844\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 173   Training loss: 0.15955686569213867\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 174   Training loss: 0.15028676390647888\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 175   Training loss: 0.1698470413684845\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 176   Training loss: 0.1618170142173767\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 177   Training loss: 0.1443788707256317\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 178   Training loss: 0.1751682013273239\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 179   Training loss: 0.169327050447464\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 180   Training loss: 0.15005971491336823\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 181   Training loss: 0.16735512018203735\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 182   Training loss: 0.14967884123325348\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 183   Training loss: 0.14673985540866852\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 184   Training loss: 0.16656717658042908\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 185   Training loss: 0.14361587166786194\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 186   Training loss: 0.15968841314315796\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 187   Training loss: 0.15300984680652618\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 188   Training loss: 0.15928243100643158\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 189   Training loss: 0.1561955362558365\n",
            "Precision: 72.54 +/- 33.7 %\n",
            "Recall: 30.39 +/- 16.1 %\n",
            "         \n",
            "\n",
            "Batch: 190   Training loss: 0.15176552534103394\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 191   Training loss: 0.1688164919614792\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 192   Training loss: 0.14838020503520966\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 193   Training loss: 0.16039466857910156\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 194   Training loss: 0.17184928059577942\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 195   Training loss: 0.15994372963905334\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 196   Training loss: 0.16563822329044342\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 197   Training loss: 0.16230417788028717\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 198   Training loss: 0.15918976068496704\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 199   Training loss: 0.1620044857263565\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 200   Training loss: 0.1578768640756607\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 201   Training loss: 0.161082923412323\n",
            "Precision: 67.12 +/- 37.5 %\n",
            "Recall: 31.42 +/- 21.3 %\n",
            "         \n",
            "\n",
            "Batch: 202   Training loss: 0.1499163657426834\n",
            "Precision: 61.54 +/- 41.9 %\n",
            "Recall: 28.35 +/- 24.7 %\n",
            "         \n",
            "\n",
            "Batch: 203   Training loss: 0.15999966859817505\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 204   Training loss: 0.14638787508010864\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 205   Training loss: 0.16297559440135956\n",
            "Precision: 65.34 +/- 33.6 %\n",
            "Recall: 33.45 +/- 17.1 %\n",
            "         \n",
            "\n",
            "Batch: 206   Training loss: 0.1611240655183792\n",
            "Precision: 53.45 +/- 39.9 %\n",
            "Recall: 26.26 +/- 21.2 %\n",
            "         \n",
            "\n",
            "Batch: 207   Training loss: 0.16098792850971222\n",
            "Precision: 49.24 +/- 43.1 %\n",
            "Recall: 19.75 +/- 18.4 %\n",
            "         \n",
            "\n",
            "Batch: 208   Training loss: 0.16699597239494324\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 209   Training loss: 0.164525106549263\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 210   Training loss: 0.17386183142662048\n",
            "Precision: 71.41 +/- 38.1 %\n",
            "Recall: 31.15 +/- 23.3 %\n",
            "         \n",
            "\n",
            "Batch: 211   Training loss: 0.1691070944070816\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 212   Training loss: 0.15235814452171326\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 213   Training loss: 0.16274391114711761\n",
            "Precision: 73.87 +/- 34.7 %\n",
            "Recall: 33.90 +/- 26.6 %\n",
            "         \n",
            "\n",
            "Batch: 214   Training loss: 0.15575549006462097\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 215   Training loss: 0.15708166360855103\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 216   Training loss: 0.16704310476779938\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 217   Training loss: 0.16084587574005127\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 218   Training loss: 0.1496041715145111\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 219   Training loss: 0.14690221846103668\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 220   Training loss: 0.15966540575027466\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 221   Training loss: 0.14200899004936218\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 222   Training loss: 0.14819417893886566\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 223   Training loss: 0.15894818305969238\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 224   Training loss: 0.1823940873146057\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 225   Training loss: 0.1638517528772354\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 226   Training loss: 0.15054792165756226\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 227   Training loss: 0.158357173204422\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 228   Training loss: 0.17635507881641388\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 229   Training loss: 0.16383856534957886\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 230   Training loss: 0.1521259844303131\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 231   Training loss: 0.15208551287651062\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 232   Training loss: 0.15945357084274292\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 233   Training loss: 0.12311854958534241\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 234   Training loss: 0.1776111125946045\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 235   Training loss: 0.16931329667568207\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 236   Training loss: 0.1395157277584076\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 237   Training loss: 0.15001823008060455\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 238   Training loss: 0.16186264157295227\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 239   Training loss: 0.1711074560880661\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 240   Training loss: 0.16021186113357544\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 241   Training loss: 0.14957962930202484\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 242   Training loss: 0.1665903776884079\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 243   Training loss: 0.1569386124610901\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 244   Training loss: 0.15174834430217743\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 245   Training loss: 0.15775035321712494\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 246   Training loss: 0.16266928613185883\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 247   Training loss: 0.1590888947248459\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 248   Training loss: 0.16278021037578583\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 249   Training loss: 0.16212716698646545\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 250   Training loss: 0.15886549651622772\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 251   Training loss: 0.16833117604255676\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 252   Training loss: 0.17560948431491852\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 253   Training loss: 0.16743625700473785\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 254   Training loss: 0.14754343032836914\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 255   Training loss: 0.15724144876003265\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 256   Training loss: 0.17967180907726288\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 257   Training loss: 0.14609724283218384\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 258   Training loss: 0.15200860798358917\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 259   Training loss: 0.1601761132478714\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 260   Training loss: 0.16593100130558014\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 261   Training loss: 0.17132748663425446\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 262   Training loss: 0.15913990139961243\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 263   Training loss: 0.13324671983718872\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 264   Training loss: 0.1457563191652298\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 265   Training loss: 0.15842889249324799\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 266   Training loss: 0.14985805749893188\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 267   Training loss: 0.14197079837322235\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 268   Training loss: 0.16206921637058258\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 269   Training loss: 0.16513556241989136\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 270   Training loss: 0.14429537951946259\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 271   Training loss: 0.1516278237104416\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 272   Training loss: 0.175672709941864\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 273   Training loss: 0.16600680351257324\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 274   Training loss: 0.15663400292396545\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 275   Training loss: 0.15423159301280975\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 276   Training loss: 0.16009807586669922\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 277   Training loss: 0.15875954926013947\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 278   Training loss: 0.15153764188289642\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 279   Training loss: 0.15727441012859344\n",
            "Precision: 56.18 +/- 43.8 %\n",
            "Recall: 27.46 +/- 26.4 %\n",
            "         \n",
            "\n",
            "Batch: 280   Training loss: 0.15505412220954895\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 281   Training loss: 0.19120584428310394\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 282   Training loss: 0.16703680157661438\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 283   Training loss: 0.16925115883350372\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 284   Training loss: 0.17170049250125885\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 285   Training loss: 0.16550810635089874\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 286   Training loss: 0.14664530754089355\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 287   Training loss: 0.1513267457485199\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 288   Training loss: 0.1713835448026657\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 289   Training loss: 0.15625445544719696\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 290   Training loss: 0.17006781697273254\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 291   Training loss: 0.15399011969566345\n",
            "Positive samples missing, skipping\n",
            "         \n",
            "\n",
            "Batch: 292   Training loss: 0.1607738584280014\n",
            "Positive samples missing, skipping\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "log_every_n_batch = 1\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    print('start')\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()   # zero the gradient buffers\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "        print('         \\n')\n",
        "        if i % log_every_n_batch == 0:\n",
        "            print('Batch: {}   Training loss: {}'.format(i, loss.item()))\n",
        "            try:\n",
        "              p_m, p_s, r_m, r_s = report_precision_recall(net, data, device)\n",
        "            except:\n",
        "              print(\"Positive samples missing, skipping\")\n",
        "              continue\n",
        "            print(f'Precision: {100*p_m:.2f} +/- {100*p_s:.1f} %')\n",
        "            print(f'Recall: {100*r_m:.2f} +/- {100*r_s:.1f} %')\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Qxwv_kVdsdAy"
      },
      "outputs": [],
      "source": [
        "def test_cnn(net, testLoader, threshold=0.1):\n",
        "    correct = []\n",
        "    total = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(testLoader):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            for sample in range(len(outputs.cpu())):\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                preds = np.where(outputs[sample].cpu()>threshold)[0]\n",
        "                labels_ = np.where(labels[sample].cpu()==1)[0]\n",
        "                # print(len(preds), len(outputs))\n",
        "                if len(labels_) < 1:\n",
        "                    continue\n",
        "                intersection = set(preds) & set(labels_)\n",
        "                total.append(len(labels_))\n",
        "                correct.append(len(intersection))\n",
        "            try:\n",
        "              report_precision_recall(net, data, device)\n",
        "            except:\n",
        "              print(\"No prediction as postive, skipping..\")\n",
        "              continue\n",
        "        return np.array(correct), np.array(total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNP0lq60sdAy",
        "outputId": "6972fb05-d8fa-48d3-c2d3-4289db08f238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n"
          ]
        }
      ],
      "source": [
        "correct, total = test_cnn(net, testloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whImG5xdsdAy",
        "outputId": "7874f38d-441a-468c-b207-654c6deb0a4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2734"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "len(correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1XVus5gsdAy",
        "outputId": "929170a4-642e-4fe3-849c-a982b11f5689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23686504987980564\n",
            "0.6492449626549716\n"
          ]
        }
      ],
      "source": [
        "acc = correct / total\n",
        "\n",
        "print(acc.std())\n",
        "print(acc.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp9Fz-gAsdAy"
      },
      "source": [
        "# Cross validation with 5 folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sZwdT7HisdAz",
        "outputId": "22f988e0-d679-4e72-deda-3213fcf84055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold:  0\n",
            "Epoch:  0\n",
            "Batch: 0   Training loss: 0.6959236860275269\n",
            "Precision: 5.76 +/- 3.0 %\n",
            "Recall: 47.17 +/- 21.9 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Recall: 14.30 +/- 12.2 %\n",
            "Batch: 157   Training loss: 0.16844569146633148\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.72 +/- 15.2 %\n",
            "Batch: 158   Training loss: 0.173855260014534\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.24 +/- 14.7 %\n",
            "Batch: 159   Training loss: 0.17860834300518036\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.58 +/- 12.0 %\n",
            "Batch: 160   Training loss: 0.1828005313873291\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.34 +/- 13.6 %\n",
            "Batch: 161   Training loss: 0.18421098589897156\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.56 +/- 13.7 %\n",
            "Batch: 162   Training loss: 0.1691499799489975\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.16 +/- 15.8 %\n",
            "Batch: 163   Training loss: 0.1699882447719574\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.97 +/- 15.7 %\n",
            "Batch: 164   Training loss: 0.18881064653396606\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.90 +/- 13.6 %\n",
            "Batch: 165   Training loss: 0.18822944164276123\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.16 +/- 15.2 %\n",
            "Batch: 166   Training loss: 0.18390882015228271\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.33 +/- 16.0 %\n",
            "Batch: 167   Training loss: 0.1815018504858017\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.04 +/- 14.5 %\n",
            "Batch: 168   Training loss: 0.17417088150978088\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.41 +/- 13.1 %\n",
            "Batch: 169   Training loss: 0.189381405711174\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.61 +/- 10.9 %\n",
            "Batch: 170   Training loss: 0.17711056768894196\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.50 +/- 13.5 %\n",
            "Batch: 171   Training loss: 0.19134601950645447\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.11 +/- 13.7 %\n",
            "Batch: 172   Training loss: 0.17993219196796417\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.36 +/- 15.2 %\n",
            "Batch: 173   Training loss: 0.17821195721626282\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.34 +/- 11.1 %\n",
            "Batch: 174   Training loss: 0.17273738980293274\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.42 +/- 16.3 %\n",
            "Batch: 175   Training loss: 0.1655435413122177\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.55 +/- 13.2 %\n",
            "Batch: 176   Training loss: 0.17887677252292633\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.10 +/- 13.3 %\n",
            "Batch: 177   Training loss: 0.18433111906051636\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.86 +/- 12.8 %\n",
            "Batch: 178   Training loss: 0.17692676186561584\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.11 +/- 12.7 %\n",
            "Batch: 179   Training loss: 0.17584194242954254\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.03 +/- 14.7 %\n",
            "Batch: 180   Training loss: 0.19194336235523224\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.96 +/- 12.0 %\n",
            "Batch: 181   Training loss: 0.18404629826545715\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.03 +/- 12.5 %\n",
            "Batch: 182   Training loss: 0.18213212490081787\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.30 +/- 15.4 %\n",
            "Batch: 183   Training loss: 0.21234428882598877\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.54 +/- 11.4 %\n",
            "Batch: 184   Training loss: 0.19232115149497986\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.49 +/- 12.7 %\n",
            "Batch: 185   Training loss: 0.18399952352046967\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.03 +/- 12.4 %\n",
            "Batch: 186   Training loss: 0.19377583265304565\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.06 +/- 11.3 %\n",
            "Batch: 187   Training loss: 0.17840126156806946\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.37 +/- 13.3 %\n",
            "Batch: 188   Training loss: 0.16608619689941406\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.10 +/- 15.1 %\n",
            "Batch: 189   Training loss: 0.1736934930086136\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.59 +/- 13.9 %\n",
            "Batch: 190   Training loss: 0.1524277776479721\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 20.19 +/- 14.4 %\n",
            "Batch: 191   Training loss: 0.17631085216999054\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.26 +/- 12.0 %\n",
            "Batch: 192   Training loss: 0.18394263088703156\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.51 +/- 12.9 %\n",
            "Batch: 193   Training loss: 0.18455365300178528\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.28 +/- 13.1 %\n",
            "Batch: 194   Training loss: 0.17890606820583344\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.96 +/- 11.9 %\n",
            "Batch: 195   Training loss: 0.16014046967029572\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.32 +/- 15.8 %\n",
            "Batch: 196   Training loss: 0.18426413834095\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.28 +/- 14.5 %\n",
            "Batch: 197   Training loss: 0.19693820178508759\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.55 +/- 13.3 %\n",
            "Batch: 198   Training loss: 0.17628473043441772\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.92 +/- 13.4 %\n",
            "[6,   200] loss: 0.178\n",
            "Batch: 199   Training loss: 0.178004190325737\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.10 +/- 14.8 %\n",
            "Batch: 200   Training loss: 0.18279460072517395\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.14 +/- 15.0 %\n",
            "Batch: 201   Training loss: 0.1703895926475525\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 21.68 +/- 14.3 %\n",
            "Batch: 202   Training loss: 0.18768265843391418\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 17.22 +/- 11.2 %\n",
            "Batch: 203   Training loss: 0.17270466685295105\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.59 +/- 12.4 %\n",
            "Batch: 204   Training loss: 0.19577251374721527\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.02 +/- 12.2 %\n",
            "Batch: 205   Training loss: 0.1867113709449768\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.96 +/- 13.6 %\n",
            "Batch: 206   Training loss: 0.17885279655456543\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.65 +/- 12.5 %\n",
            "Batch: 207   Training loss: 0.17926530539989471\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.52 +/- 13.4 %\n",
            "Batch: 208   Training loss: 0.19160544872283936\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.23 +/- 12.4 %\n",
            "Batch: 209   Training loss: 0.1563168466091156\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.68 +/- 14.4 %\n",
            "Batch: 210   Training loss: 0.1647021621465683\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.18 +/- 15.0 %\n",
            "Batch: 211   Training loss: 0.18010087311267853\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.71 +/- 13.6 %\n",
            "Batch: 212   Training loss: 0.17047901451587677\n",
            "Precision: 55.56 +/- 49.7 %\n",
            "Recall: 14.26 +/- 14.5 %\n",
            "Batch: 213   Training loss: 0.16550570726394653\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.44 +/- 14.4 %\n",
            "Batch: 214   Training loss: 0.1821652352809906\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.03 +/- 15.8 %\n",
            "Batch: 215   Training loss: 0.18305103480815887\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.31 +/- 12.4 %\n",
            "Batch: 216   Training loss: 0.1844862401485443\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 11.24 +/- 15.5 %\n",
            "Batch: 217   Training loss: 0.17563356459140778\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.77 +/- 15.0 %\n",
            "Batch: 218   Training loss: 0.19176146388053894\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.51 +/- 11.0 %\n",
            "Batch: 219   Training loss: 0.18007434904575348\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.57 +/- 12.9 %\n",
            "Batch: 220   Training loss: 0.1904033124446869\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.67 +/- 13.3 %\n",
            "Batch: 221   Training loss: 0.1828693002462387\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.27 +/- 11.3 %\n",
            "Batch: 222   Training loss: 0.19829218089580536\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.56 +/- 14.7 %\n",
            "Batch: 223   Training loss: 0.19189874827861786\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.12 +/- 14.0 %\n",
            "Batch: 224   Training loss: 0.16958504915237427\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 17.74 +/- 17.0 %\n",
            "Batch: 225   Training loss: 0.17874747514724731\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.14 +/- 14.1 %\n",
            "Batch: 226   Training loss: 0.177829310297966\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.42 +/- 12.1 %\n",
            "Batch: 227   Training loss: 0.19712427258491516\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.47 +/- 10.7 %\n",
            "Batch: 228   Training loss: 0.17556990683078766\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 8.65 +/- 11.4 %\n",
            "Batch: 229   Training loss: 0.18217991292476654\n",
            "Precision: 85.71 +/- 35.0 %\n",
            "Recall: 19.48 +/- 10.3 %\n",
            "Batch: 230   Training loss: 0.18789911270141602\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 14.38 +/- 9.7 %\n",
            "Batch: 231   Training loss: 0.16499711573123932\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.30 +/- 14.8 %\n",
            "Batch: 232   Training loss: 0.17941296100616455\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.25 +/- 13.6 %\n",
            "Batch: 233   Training loss: 0.17689839005470276\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.87 +/- 13.3 %\n",
            "Batch: 234   Training loss: 0.1901707798242569\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.46 +/- 13.5 %\n",
            "Batch: 235   Training loss: 0.18165771663188934\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.38 +/- 13.9 %\n",
            "Batch: 236   Training loss: 0.19796603918075562\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.83 +/- 12.6 %\n",
            "Batch: 237   Training loss: 0.1775960773229599\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.58 +/- 12.5 %\n",
            "Batch: 238   Training loss: 0.1922689825296402\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.65 +/- 12.0 %\n",
            "Batch: 239   Training loss: 0.19291365146636963\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.79 +/- 13.6 %\n",
            "Batch: 240   Training loss: 0.17180345952510834\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.19 +/- 12.6 %\n",
            "Batch: 241   Training loss: 0.17908844351768494\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.05 +/- 14.1 %\n",
            "Batch: 242   Training loss: 0.17517685890197754\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.45 +/- 11.8 %\n",
            "Batch: 243   Training loss: 0.16650597751140594\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.69 +/- 15.0 %\n",
            "Batch: 244   Training loss: 0.17127195000648499\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.40 +/- 14.8 %\n",
            "Batch: 245   Training loss: 0.17882031202316284\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.60 +/- 14.4 %\n",
            "Batch: 246   Training loss: 0.17056645452976227\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.07 +/- 13.2 %\n",
            "Batch: 247   Training loss: 0.19308578968048096\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.83 +/- 14.1 %\n",
            "Batch: 248   Training loss: 0.19414079189300537\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.05 +/- 13.5 %\n",
            "[6,   250] loss: 0.178\n",
            "Batch: 249   Training loss: 0.17808285355567932\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.64 +/- 14.9 %\n",
            "Batch: 250   Training loss: 0.16229502856731415\n",
            "Precision: 66.67 +/- 47.1 %\n",
            "Recall: 16.40 +/- 13.9 %\n",
            "Batch: 251   Training loss: 0.17473234236240387\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.44 +/- 15.6 %\n",
            "Batch: 252   Training loss: 0.17747704684734344\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 15.66 +/- 17.7 %\n",
            "Batch: 253   Training loss: 0.16625842452049255\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.44 +/- 13.3 %\n",
            "Batch: 254   Training loss: 0.18063615262508392\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.92 +/- 12.8 %\n",
            "Batch: 255   Training loss: 0.1771959662437439\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.00 +/- 13.1 %\n",
            "Batch: 256   Training loss: 0.19061940908432007\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.08 +/- 11.7 %\n",
            "Batch: 257   Training loss: 0.1965230107307434\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.10 +/- 14.0 %\n",
            "Batch: 258   Training loss: 0.17875833809375763\n",
            "Precision: 85.71 +/- 35.0 %\n",
            "Recall: 23.38 +/- 13.7 %\n",
            "Batch: 259   Training loss: 0.1761825829744339\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 15.86 +/- 18.3 %\n",
            "Batch: 260   Training loss: 0.16834214329719543\n",
            "Precision: 74.07 +/- 43.8 %\n",
            "Recall: 18.63 +/- 14.1 %\n",
            "Batch: 261   Training loss: 0.17897075414657593\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.12 +/- 13.6 %\n",
            "Batch: 262   Training loss: 0.18420705199241638\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.02 +/- 13.4 %\n",
            "Batch: 263   Training loss: 0.18873143196105957\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.28 +/- 14.5 %\n",
            "Batch: 264   Training loss: 0.17994289100170135\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.42 +/- 13.2 %\n",
            "Batch: 265   Training loss: 0.1812485307455063\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.18 +/- 13.6 %\n",
            "Batch: 266   Training loss: 0.17886337637901306\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.03 +/- 16.0 %\n",
            "Batch: 267   Training loss: 0.16263121366500854\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.64 +/- 15.4 %\n",
            "Batch: 268   Training loss: 0.16909165680408478\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.64 +/- 12.3 %\n",
            "Batch: 269   Training loss: 0.17260366678237915\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.08 +/- 13.9 %\n",
            "Batch: 270   Training loss: 0.1633838266134262\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 11.64 +/- 17.1 %\n",
            "Batch: 271   Training loss: 0.17827145755290985\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.68 +/- 14.8 %\n",
            "Batch: 272   Training loss: 0.17565542459487915\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.57 +/- 16.4 %\n",
            "Batch: 273   Training loss: 0.19588316977024078\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.37 +/- 11.1 %\n",
            "Batch: 274   Training loss: 0.18781784176826477\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.06 +/- 11.6 %\n",
            "Batch: 275   Training loss: 0.20006020367145538\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.80 +/- 12.2 %\n",
            "Batch: 276   Training loss: 0.16181591153144836\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.63 +/- 14.8 %\n",
            "Batch: 277   Training loss: 0.1881227344274521\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.11 +/- 16.4 %\n",
            "Batch: 278   Training loss: 0.17863816022872925\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.38 +/- 12.9 %\n",
            "Batch: 279   Training loss: 0.17447637021541595\n",
            "Precision: 35.71 +/- 47.9 %\n",
            "Recall: 8.84 +/- 13.1 %\n",
            "Batch: 280   Training loss: 0.17134179174900055\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.44 +/- 16.6 %\n",
            "Batch: 281   Training loss: 0.189742311835289\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.55 +/- 15.3 %\n",
            "Batch: 282   Training loss: 0.16760481894016266\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.16 +/- 13.2 %\n",
            "Batch: 283   Training loss: 0.20175479352474213\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.40 +/- 11.9 %\n",
            "Batch: 284   Training loss: 0.20171388983726501\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.47 +/- 12.7 %\n",
            "Batch: 285   Training loss: 0.1679210364818573\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 16.01 +/- 17.1 %\n",
            "Batch: 286   Training loss: 0.17197313904762268\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.06 +/- 11.8 %\n",
            "Batch: 287   Training loss: 0.16705812513828278\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 9.43 +/- 13.5 %\n",
            "Batch: 288   Training loss: 0.17056290805339813\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.44 +/- 15.1 %\n",
            "Batch: 289   Training loss: 0.17467160522937775\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.37 +/- 13.6 %\n",
            "Batch: 290   Training loss: 0.18279188871383667\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.16 +/- 13.4 %\n",
            "Batch: 291   Training loss: 0.17529557645320892\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.91 +/- 16.8 %\n",
            "Batch: 292   Training loss: 0.18293015658855438\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 15.99 +/- 11.5 %\n",
            "Batch: 293   Training loss: 0.1834639310836792\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.46 +/- 12.7 %\n",
            "Batch: 294   Training loss: 0.18312498927116394\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.52 +/- 14.2 %\n",
            "Batch: 295   Training loss: 0.17324407398700714\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.01 +/- 15.1 %\n",
            "Batch: 296   Training loss: 0.1719796359539032\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 17.98 +/- 12.4 %\n",
            "Batch: 297   Training loss: 0.16924674808979034\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.06 +/- 13.8 %\n",
            "Batch: 298   Training loss: 0.17931920289993286\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.12 +/- 10.5 %\n",
            "[6,   300] loss: 0.181\n",
            "Batch: 299   Training loss: 0.18068347871303558\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.99 +/- 14.9 %\n",
            "Batch: 300   Training loss: 0.16293354332447052\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.78 +/- 16.7 %\n",
            "Batch: 301   Training loss: 0.17479023337364197\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.54 +/- 14.3 %\n",
            "Batch: 302   Training loss: 0.18151713907718658\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.63 +/- 12.9 %\n",
            "Batch: 303   Training loss: 0.16149193048477173\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.12 +/- 14.4 %\n",
            "Batch: 304   Training loss: 0.1721256822347641\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.21 +/- 13.6 %\n",
            "Batch: 305   Training loss: 0.17974121868610382\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.53 +/- 14.5 %\n",
            "Batch: 306   Training loss: 0.1647977977991104\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.47 +/- 14.3 %\n",
            "Batch: 307   Training loss: 0.17177866399288177\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.85 +/- 11.8 %\n",
            "Batch: 308   Training loss: 0.17566075921058655\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.52 +/- 10.8 %\n",
            "Batch: 309   Training loss: 0.17724400758743286\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.94 +/- 14.2 %\n",
            "Batch: 310   Training loss: 0.17711780965328217\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.88 +/- 15.1 %\n",
            "Batch: 311   Training loss: 0.17963500320911407\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.74 +/- 14.6 %\n",
            "Batch: 312   Training loss: 0.16353464126586914\n",
            "Precision: 40.00 +/- 49.0 %\n",
            "Recall: 9.53 +/- 12.2 %\n",
            "Epoch:  6\n",
            "Batch: 0   Training loss: 0.17792879045009613\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.52 +/- 11.3 %\n",
            "Batch: 1   Training loss: 0.17808498442173004\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.87 +/- 12.6 %\n",
            "Batch: 2   Training loss: 0.17338398098945618\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.20 +/- 12.0 %\n",
            "Batch: 3   Training loss: 0.16690769791603088\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 16.14 +/- 17.0 %\n",
            "Batch: 4   Training loss: 0.1681104600429535\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.65 +/- 13.7 %\n",
            "Batch: 5   Training loss: 0.17942409217357635\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.96 +/- 12.7 %\n",
            "Batch: 6   Training loss: 0.19303783774375916\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.22 +/- 13.5 %\n",
            "Batch: 7   Training loss: 0.15574218332767487\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 19.32 +/- 17.6 %\n",
            "Batch: 8   Training loss: 0.17427809536457062\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.65 +/- 11.6 %\n",
            "Batch: 9   Training loss: 0.17769308388233185\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.32 +/- 14.0 %\n",
            "Batch: 10   Training loss: 0.1947793960571289\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.26 +/- 12.2 %\n",
            "Batch: 11   Training loss: 0.1632663458585739\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.42 +/- 12.7 %\n",
            "Batch: 12   Training loss: 0.1767893135547638\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.72 +/- 13.4 %\n",
            "Batch: 13   Training loss: 0.17783020436763763\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.70 +/- 14.8 %\n",
            "Batch: 14   Training loss: 0.1812891662120819\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.68 +/- 12.1 %\n",
            "Batch: 15   Training loss: 0.18758124113082886\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.68 +/- 11.6 %\n",
            "Batch: 16   Training loss: 0.17249436676502228\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.47 +/- 15.1 %\n",
            "Batch: 17   Training loss: 0.17570379376411438\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.33 +/- 14.4 %\n",
            "Batch: 18   Training loss: 0.16307130455970764\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.79 +/- 13.8 %\n",
            "Batch: 19   Training loss: 0.1825750172138214\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.05 +/- 13.9 %\n",
            "Batch: 20   Training loss: 0.18683770298957825\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 10.32 +/- 9.9 %\n",
            "Batch: 21   Training loss: 0.18085920810699463\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.84 +/- 16.5 %\n",
            "Batch: 22   Training loss: 0.17952094972133636\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.01 +/- 15.4 %\n",
            "Batch: 23   Training loss: 0.17006385326385498\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 16.52 +/- 16.9 %\n",
            "Batch: 24   Training loss: 0.17616362869739532\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.25 +/- 14.5 %\n",
            "Batch: 25   Training loss: 0.1687934547662735\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.22 +/- 16.0 %\n",
            "Batch: 26   Training loss: 0.19496247172355652\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.27 +/- 12.0 %\n",
            "Batch: 27   Training loss: 0.1684701144695282\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.67 +/- 15.8 %\n",
            "Batch: 28   Training loss: 0.17146478593349457\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.18 +/- 13.1 %\n",
            "Batch: 29   Training loss: 0.18787895143032074\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.30 +/- 10.0 %\n",
            "Batch: 30   Training loss: 0.18279364705085754\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.28 +/- 14.8 %\n",
            "Batch: 31   Training loss: 0.17575329542160034\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.33 +/- 14.6 %\n",
            "Batch: 32   Training loss: 0.19042102992534637\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.37 +/- 11.2 %\n",
            "Batch: 33   Training loss: 0.1752469837665558\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.41 +/- 13.4 %\n",
            "Batch: 34   Training loss: 0.14578254520893097\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.99 +/- 14.8 %\n",
            "Batch: 35   Training loss: 0.18215379118919373\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.00 +/- 13.7 %\n",
            "Batch: 36   Training loss: 0.18548966944217682\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.69 +/- 13.2 %\n",
            "Batch: 37   Training loss: 0.18277102708816528\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.87 +/- 13.4 %\n",
            "Batch: 38   Training loss: 0.1767476350069046\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.24 +/- 14.3 %\n",
            "Batch: 39   Training loss: 0.1901041716337204\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.14 +/- 13.5 %\n",
            "Batch: 40   Training loss: 0.17242133617401123\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.84 +/- 12.6 %\n",
            "Batch: 41   Training loss: 0.1885019689798355\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.02 +/- 13.0 %\n",
            "Batch: 42   Training loss: 0.18130525946617126\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.71 +/- 14.6 %\n",
            "Batch: 43   Training loss: 0.16513268649578094\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.50 +/- 15.2 %\n",
            "Batch: 44   Training loss: 0.17800693213939667\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.52 +/- 13.5 %\n",
            "Batch: 45   Training loss: 0.17229026556015015\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.64 +/- 14.4 %\n",
            "Batch: 46   Training loss: 0.189328134059906\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 15.79 +/- 11.5 %\n",
            "Batch: 47   Training loss: 0.18212532997131348\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.14 +/- 11.1 %\n",
            "Batch: 48   Training loss: 0.17149107158184052\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.32 +/- 14.9 %\n",
            "[7,    50] loss: 0.186\n",
            "Batch: 49   Training loss: 0.18557003140449524\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.95 +/- 13.6 %\n",
            "Batch: 50   Training loss: 0.17536072432994843\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.96 +/- 15.2 %\n",
            "Batch: 51   Training loss: 0.1751575469970703\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.83 +/- 13.2 %\n",
            "Batch: 52   Training loss: 0.2006864845752716\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.44 +/- 15.8 %\n",
            "Batch: 53   Training loss: 0.1925266832113266\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.11 +/- 11.6 %\n",
            "Batch: 54   Training loss: 0.17733986675739288\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.40 +/- 17.1 %\n",
            "Batch: 55   Training loss: 0.17766216397285461\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.05 +/- 14.5 %\n",
            "Batch: 56   Training loss: 0.1662660837173462\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.82 +/- 13.9 %\n",
            "Batch: 57   Training loss: 0.18499939143657684\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.32 +/- 15.5 %\n",
            "Batch: 58   Training loss: 0.19207078218460083\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.89 +/- 11.5 %\n",
            "Batch: 59   Training loss: 0.16595061123371124\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.27 +/- 12.7 %\n",
            "Batch: 60   Training loss: 0.18917439877986908\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 15.22 +/- 9.6 %\n",
            "Batch: 61   Training loss: 0.20022985339164734\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.05 +/- 13.9 %\n",
            "Batch: 62   Training loss: 0.17485739290714264\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.63 +/- 14.6 %\n",
            "Batch: 63   Training loss: 0.16374461352825165\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.93 +/- 15.6 %\n",
            "Batch: 64   Training loss: 0.18812696635723114\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.89 +/- 13.5 %\n",
            "Batch: 65   Training loss: 0.18238620460033417\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.46 +/- 12.5 %\n",
            "Batch: 66   Training loss: 0.1797889918088913\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.23 +/- 14.3 %\n",
            "Batch: 67   Training loss: 0.17937615513801575\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.46 +/- 13.2 %\n",
            "Batch: 68   Training loss: 0.18639089167118073\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.20 +/- 11.7 %\n",
            "Batch: 69   Training loss: 0.18289925158023834\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.07 +/- 13.0 %\n",
            "Batch: 70   Training loss: 0.17712686955928802\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.44 +/- 13.6 %\n",
            "Batch: 71   Training loss: 0.17404864728450775\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.48 +/- 15.5 %\n",
            "Batch: 72   Training loss: 0.17642365396022797\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.39 +/- 12.1 %\n",
            "Batch: 73   Training loss: 0.174399271607399\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.46 +/- 15.2 %\n",
            "Batch: 74   Training loss: 0.18437838554382324\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.52 +/- 13.9 %\n",
            "Batch: 75   Training loss: 0.15159107744693756\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 23.27 +/- 14.2 %\n",
            "Batch: 76   Training loss: 0.17672397196292877\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 12.78 +/- 17.4 %\n",
            "Batch: 77   Training loss: 0.19075696170330048\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.52 +/- 15.2 %\n",
            "Batch: 78   Training loss: 0.2023923397064209\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.55 +/- 15.6 %\n",
            "Batch: 79   Training loss: 0.17995141446590424\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.25 +/- 12.4 %\n",
            "Batch: 80   Training loss: 0.18702390789985657\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.90 +/- 14.7 %\n",
            "Batch: 81   Training loss: 0.19449634850025177\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.46 +/- 14.1 %\n",
            "Batch: 82   Training loss: 0.19494575262069702\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.75 +/- 11.8 %\n",
            "Batch: 83   Training loss: 0.183254212141037\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.39 +/- 12.1 %\n",
            "Batch: 84   Training loss: 0.1689533144235611\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.09 +/- 14.8 %\n",
            "Batch: 85   Training loss: 0.18327096104621887\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.81 +/- 14.8 %\n",
            "Batch: 86   Training loss: 0.1447950005531311\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 16.91 +/- 16.7 %\n",
            "Batch: 87   Training loss: 0.19491888582706451\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.83 +/- 11.7 %\n",
            "Batch: 88   Training loss: 0.16393832862377167\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.17 +/- 15.9 %\n",
            "Batch: 89   Training loss: 0.17841993272304535\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.90 +/- 15.4 %\n",
            "Batch: 90   Training loss: 0.1665302813053131\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 18.63 +/- 10.5 %\n",
            "Batch: 91   Training loss: 0.1791216880083084\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.84 +/- 13.2 %\n",
            "Batch: 92   Training loss: 0.15788708627223969\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.86 +/- 15.5 %\n",
            "Batch: 93   Training loss: 0.18264062702655792\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.42 +/- 12.7 %\n",
            "Batch: 94   Training loss: 0.17007048428058624\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.28 +/- 16.1 %\n",
            "Batch: 95   Training loss: 0.1803017109632492\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.75 +/- 13.2 %\n",
            "Batch: 96   Training loss: 0.19144172966480255\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.48 +/- 12.1 %\n",
            "Batch: 97   Training loss: 0.18020616471767426\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.52 +/- 15.6 %\n",
            "Batch: 98   Training loss: 0.18894612789154053\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 14.30 +/- 9.6 %\n",
            "[7,   100] loss: 0.189\n",
            "Batch: 99   Training loss: 0.18901030719280243\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.55 +/- 12.2 %\n",
            "Batch: 100   Training loss: 0.1837555319070816\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 9.37 +/- 10.3 %\n",
            "Batch: 101   Training loss: 0.17061294615268707\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.24 +/- 14.9 %\n",
            "Batch: 102   Training loss: 0.17094740271568298\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.18 +/- 11.1 %\n",
            "Batch: 103   Training loss: 0.19308532774448395\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.52 +/- 13.7 %\n",
            "Batch: 104   Training loss: 0.17433789372444153\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.58 +/- 13.4 %\n",
            "Batch: 105   Training loss: 0.17748472094535828\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.14 +/- 15.5 %\n",
            "Batch: 106   Training loss: 0.17895005643367767\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.06 +/- 13.6 %\n",
            "Batch: 107   Training loss: 0.19023865461349487\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.90 +/- 12.0 %\n",
            "Batch: 108   Training loss: 0.20067206025123596\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.90 +/- 12.7 %\n",
            "Batch: 109   Training loss: 0.17369262874126434\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.40 +/- 13.4 %\n",
            "Batch: 110   Training loss: 0.18076829612255096\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.79 +/- 14.5 %\n",
            "Batch: 111   Training loss: 0.16581778228282928\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 14.36 +/- 16.1 %\n",
            "Batch: 112   Training loss: 0.17206843197345734\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.23 +/- 14.2 %\n",
            "Batch: 113   Training loss: 0.1857936829328537\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.93 +/- 13.6 %\n",
            "Batch: 114   Training loss: 0.18697687983512878\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 17.80 +/- 10.6 %\n",
            "Batch: 115   Training loss: 0.17573566734790802\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.04 +/- 12.4 %\n",
            "Batch: 116   Training loss: 0.1729917973279953\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.48 +/- 12.8 %\n",
            "Batch: 117   Training loss: 0.17630763351917267\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.77 +/- 12.4 %\n",
            "Batch: 118   Training loss: 0.1820027232170105\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.99 +/- 14.2 %\n",
            "Batch: 119   Training loss: 0.1836506873369217\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.52 +/- 13.1 %\n",
            "Batch: 120   Training loss: 0.17160740494728088\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.45 +/- 15.7 %\n",
            "Batch: 121   Training loss: 0.18265452980995178\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.49 +/- 15.9 %\n",
            "Batch: 122   Training loss: 0.18101072311401367\n",
            "Precision: 59.26 +/- 49.1 %\n",
            "Recall: 12.12 +/- 11.5 %\n",
            "Batch: 123   Training loss: 0.19122660160064697\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.61 +/- 10.3 %\n",
            "Batch: 124   Training loss: 0.15810626745224\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 10.80 +/- 14.5 %\n",
            "Batch: 125   Training loss: 0.16115371882915497\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.48 +/- 14.0 %\n",
            "Batch: 126   Training loss: 0.17928153276443481\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.07 +/- 14.8 %\n",
            "Batch: 127   Training loss: 0.17677733302116394\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.30 +/- 12.7 %\n",
            "Batch: 128   Training loss: 0.16878241300582886\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 11.50 +/- 16.5 %\n",
            "Batch: 129   Training loss: 0.17540642619132996\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.26 +/- 12.2 %\n",
            "Batch: 130   Training loss: 0.18740662932395935\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.68 +/- 12.2 %\n",
            "Batch: 131   Training loss: 0.1957722008228302\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.71 +/- 13.0 %\n",
            "Batch: 132   Training loss: 0.1752118617296219\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.91 +/- 12.7 %\n",
            "Batch: 133   Training loss: 0.18167950212955475\n",
            "Precision: 85.71 +/- 35.0 %\n",
            "Recall: 21.01 +/- 12.4 %\n",
            "Batch: 134   Training loss: 0.17701426148414612\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.00 +/- 12.3 %\n",
            "Batch: 135   Training loss: 0.19026055932044983\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.90 +/- 11.3 %\n",
            "Batch: 136   Training loss: 0.18242643773555756\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.76 +/- 12.0 %\n",
            "Batch: 137   Training loss: 0.17794518172740936\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 20.32 +/- 15.3 %\n",
            "Batch: 138   Training loss: 0.19792403280735016\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.36 +/- 12.7 %\n",
            "Batch: 139   Training loss: 0.17243504524230957\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.62 +/- 13.0 %\n",
            "Batch: 140   Training loss: 0.1755821406841278\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.13 +/- 13.4 %\n",
            "Batch: 141   Training loss: 0.16972050070762634\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.35 +/- 14.5 %\n",
            "Batch: 142   Training loss: 0.18346050381660461\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.29 +/- 13.3 %\n",
            "Batch: 143   Training loss: 0.18424652516841888\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.27 +/- 14.5 %\n",
            "Batch: 144   Training loss: 0.1922300159931183\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.35 +/- 14.6 %\n",
            "Batch: 145   Training loss: 0.17925527691841125\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.22 +/- 12.6 %\n",
            "Batch: 146   Training loss: 0.17552347481250763\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.05 +/- 15.6 %\n",
            "Batch: 147   Training loss: 0.1863410472869873\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.89 +/- 14.3 %\n",
            "Batch: 148   Training loss: 0.18755973875522614\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.96 +/- 12.3 %\n",
            "[7,   150] loss: 0.191\n",
            "Batch: 149   Training loss: 0.19050587713718414\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.72 +/- 13.2 %\n",
            "Batch: 150   Training loss: 0.1624661087989807\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.73 +/- 16.0 %\n",
            "Batch: 151   Training loss: 0.18214121460914612\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.82 +/- 13.8 %\n",
            "Batch: 152   Training loss: 0.19090257585048676\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.87 +/- 14.1 %\n",
            "Batch: 153   Training loss: 0.1783016473054886\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.70 +/- 12.7 %\n",
            "Batch: 154   Training loss: 0.18671318888664246\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.64 +/- 15.1 %\n",
            "Batch: 155   Training loss: 0.1717861294746399\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.51 +/- 15.3 %\n",
            "Batch: 156   Training loss: 0.1575307548046112\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.93 +/- 14.6 %\n",
            "Batch: 157   Training loss: 0.17096272110939026\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.81 +/- 13.4 %\n",
            "Batch: 158   Training loss: 0.17722493410110474\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.84 +/- 11.7 %\n",
            "Batch: 159   Training loss: 0.18165574967861176\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.73 +/- 15.5 %\n",
            "Batch: 160   Training loss: 0.17645083367824554\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.31 +/- 13.8 %\n",
            "Batch: 161   Training loss: 0.17051157355308533\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.38 +/- 12.9 %\n",
            "Batch: 162   Training loss: 0.1829412281513214\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.17 +/- 13.9 %\n",
            "Batch: 163   Training loss: 0.17867468297481537\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.81 +/- 14.1 %\n",
            "Batch: 164   Training loss: 0.20129276812076569\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.26 +/- 12.0 %\n",
            "Batch: 165   Training loss: 0.17004474997520447\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.84 +/- 15.1 %\n",
            "Batch: 166   Training loss: 0.18056724965572357\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.65 +/- 13.4 %\n",
            "Batch: 167   Training loss: 0.17958968877792358\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.53 +/- 12.2 %\n",
            "Batch: 168   Training loss: 0.15852443873882294\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 13.25 +/- 15.3 %\n",
            "Batch: 169   Training loss: 0.18673394620418549\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.02 +/- 13.4 %\n",
            "Batch: 170   Training loss: 0.1699228733778\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.81 +/- 13.4 %\n",
            "Batch: 171   Training loss: 0.18138812482357025\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 9.44 +/- 12.9 %\n",
            "Batch: 172   Training loss: 0.17976607382297516\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.05 +/- 12.7 %\n",
            "Batch: 173   Training loss: 0.17843565344810486\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.17 +/- 11.3 %\n",
            "Batch: 174   Training loss: 0.19591684639453888\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.55 +/- 16.4 %\n",
            "Batch: 175   Training loss: 0.16090114414691925\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 16.31 +/- 15.8 %\n",
            "Batch: 176   Training loss: 0.18851619958877563\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.43 +/- 12.5 %\n",
            "Batch: 177   Training loss: 0.18262572586536407\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.92 +/- 11.5 %\n",
            "Batch: 178   Training loss: 0.1799667477607727\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.39 +/- 12.0 %\n",
            "Batch: 179   Training loss: 0.18722520768642426\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.84 +/- 11.1 %\n",
            "Batch: 180   Training loss: 0.18180565536022186\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.80 +/- 12.7 %\n",
            "Batch: 181   Training loss: 0.17856772243976593\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.69 +/- 10.9 %\n",
            "Batch: 182   Training loss: 0.17513670027256012\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.81 +/- 15.0 %\n",
            "Batch: 183   Training loss: 0.18117932975292206\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 17.99 +/- 18.0 %\n",
            "Batch: 184   Training loss: 0.18161356449127197\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.43 +/- 13.0 %\n",
            "Batch: 185   Training loss: 0.18165601789951324\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.94 +/- 15.3 %\n",
            "Batch: 186   Training loss: 0.17817431688308716\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.51 +/- 14.1 %\n",
            "Batch: 187   Training loss: 0.17756778001785278\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.84 +/- 11.2 %\n",
            "Batch: 188   Training loss: 0.18617387115955353\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.83 +/- 14.0 %\n",
            "Batch: 189   Training loss: 0.18803298473358154\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.17 +/- 12.4 %\n",
            "Batch: 190   Training loss: 0.16707058250904083\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.55 +/- 15.8 %\n",
            "Batch: 191   Training loss: 0.17216385900974274\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.32 +/- 14.0 %\n",
            "Batch: 192   Training loss: 0.16950631141662598\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.24 +/- 15.4 %\n",
            "Batch: 193   Training loss: 0.1778305321931839\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.10 +/- 12.0 %\n",
            "Batch: 194   Training loss: 0.18630656599998474\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 17.87 +/- 11.8 %\n",
            "Batch: 195   Training loss: 0.18031452596187592\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.55 +/- 12.4 %\n",
            "Batch: 196   Training loss: 0.18904277682304382\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.92 +/- 14.5 %\n",
            "Batch: 197   Training loss: 0.19450776278972626\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.02 +/- 11.5 %\n",
            "Batch: 198   Training loss: 0.18146200478076935\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.27 +/- 12.5 %\n",
            "[7,   200] loss: 0.179\n",
            "Batch: 199   Training loss: 0.17930901050567627\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.96 +/- 14.3 %\n",
            "Batch: 200   Training loss: 0.17265351116657257\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.35 +/- 12.5 %\n",
            "Batch: 201   Training loss: 0.17446273565292358\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.27 +/- 15.7 %\n",
            "Batch: 202   Training loss: 0.18058526515960693\n",
            "Precision: 66.67 +/- 47.1 %\n",
            "Recall: 15.25 +/- 13.0 %\n",
            "Batch: 203   Training loss: 0.1769176721572876\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.21 +/- 13.2 %\n",
            "Batch: 204   Training loss: 0.1791173666715622\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.31 +/- 14.2 %\n",
            "Batch: 205   Training loss: 0.18243564665317535\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.39 +/- 15.5 %\n",
            "Batch: 206   Training loss: 0.17827485501766205\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.09 +/- 14.2 %\n",
            "Batch: 207   Training loss: 0.17285394668579102\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.27 +/- 13.7 %\n",
            "Batch: 208   Training loss: 0.17941920459270477\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.03 +/- 11.0 %\n",
            "Batch: 209   Training loss: 0.15302830934524536\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.11 +/- 14.8 %\n",
            "Batch: 210   Training loss: 0.1713598221540451\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.22 +/- 14.6 %\n",
            "Batch: 211   Training loss: 0.1744818389415741\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 9.21 +/- 12.9 %\n",
            "Batch: 212   Training loss: 0.1533483862876892\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.49 +/- 15.0 %\n",
            "Batch: 213   Training loss: 0.18505170941352844\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.59 +/- 12.6 %\n",
            "Batch: 214   Training loss: 0.1734289824962616\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.78 +/- 14.5 %\n",
            "Batch: 215   Training loss: 0.17805734276771545\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.82 +/- 13.8 %\n",
            "Batch: 216   Training loss: 0.17486616969108582\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.36 +/- 13.5 %\n",
            "Batch: 217   Training loss: 0.19123615324497223\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.18 +/- 15.3 %\n",
            "Batch: 218   Training loss: 0.17750440537929535\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.98 +/- 13.5 %\n",
            "Batch: 219   Training loss: 0.16349315643310547\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.52 +/- 14.9 %\n",
            "Batch: 220   Training loss: 0.18299421668052673\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.83 +/- 15.2 %\n",
            "Batch: 221   Training loss: 0.19994860887527466\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.07 +/- 13.3 %\n",
            "Batch: 222   Training loss: 0.16265375912189484\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 19.13 +/- 16.9 %\n",
            "Batch: 223   Training loss: 0.16949227452278137\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.85 +/- 16.3 %\n",
            "Batch: 224   Training loss: 0.17891927063465118\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 16.24 +/- 17.4 %\n",
            "Batch: 225   Training loss: 0.1768863946199417\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.06 +/- 15.5 %\n",
            "Batch: 226   Training loss: 0.15834492444992065\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.44 +/- 16.1 %\n",
            "Batch: 227   Training loss: 0.1651058942079544\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.90 +/- 14.2 %\n",
            "Batch: 228   Training loss: 0.183493509888649\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.24 +/- 12.7 %\n",
            "Batch: 229   Training loss: 0.18158259987831116\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.07 +/- 11.8 %\n",
            "Batch: 230   Training loss: 0.1721608191728592\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.89 +/- 13.5 %\n",
            "Batch: 231   Training loss: 0.17753176391124725\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.56 +/- 12.7 %\n",
            "Batch: 232   Training loss: 0.18206100165843964\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.63 +/- 13.3 %\n",
            "Batch: 233   Training loss: 0.18371395766735077\n",
            "Precision: 85.71 +/- 35.0 %\n",
            "Recall: 21.45 +/- 13.2 %\n",
            "Batch: 234   Training loss: 0.18288281559944153\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.40 +/- 11.3 %\n",
            "Batch: 235   Training loss: 0.1882663518190384\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.75 +/- 15.7 %\n",
            "Batch: 236   Training loss: 0.19531956315040588\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.20 +/- 12.4 %\n",
            "Batch: 237   Training loss: 0.17237639427185059\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.88 +/- 13.0 %\n",
            "Batch: 238   Training loss: 0.17874731123447418\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.01 +/- 11.6 %\n",
            "Batch: 239   Training loss: 0.20613880455493927\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 8.08 +/- 10.6 %\n",
            "Batch: 240   Training loss: 0.1802423745393753\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.88 +/- 13.6 %\n",
            "Batch: 241   Training loss: 0.18607553839683533\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.04 +/- 12.9 %\n",
            "Batch: 242   Training loss: 0.1725480556488037\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.76 +/- 15.3 %\n",
            "Batch: 243   Training loss: 0.16097787022590637\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.62 +/- 15.1 %\n",
            "Batch: 244   Training loss: 0.18581344187259674\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.07 +/- 12.0 %\n",
            "Batch: 245   Training loss: 0.17789755761623383\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.29 +/- 13.6 %\n",
            "Batch: 246   Training loss: 0.1899353712797165\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.99 +/- 14.9 %\n",
            "Batch: 247   Training loss: 0.1800629198551178\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.34 +/- 12.4 %\n",
            "Batch: 248   Training loss: 0.17702846229076385\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.42 +/- 16.1 %\n",
            "[7,   250] loss: 0.175\n",
            "Batch: 249   Training loss: 0.17509107291698456\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.59 +/- 13.7 %\n",
            "Batch: 250   Training loss: 0.1785106360912323\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.96 +/- 13.5 %\n",
            "Batch: 251   Training loss: 0.19787047803401947\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.01 +/- 10.4 %\n",
            "Batch: 252   Training loss: 0.18948374688625336\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.21 +/- 12.0 %\n",
            "Batch: 253   Training loss: 0.16031590104103088\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.94 +/- 14.5 %\n",
            "Batch: 254   Training loss: 0.18118123710155487\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.40 +/- 15.1 %\n",
            "Batch: 255   Training loss: 0.18181239068508148\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.08 +/- 11.5 %\n",
            "Batch: 256   Training loss: 0.17711250483989716\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.29 +/- 12.1 %\n",
            "Batch: 257   Training loss: 0.19659949839115143\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.96 +/- 13.3 %\n",
            "Batch: 258   Training loss: 0.1790875345468521\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 12.42 +/- 16.4 %\n",
            "Batch: 259   Training loss: 0.17622284591197968\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.94 +/- 16.2 %\n",
            "Batch: 260   Training loss: 0.17504426836967468\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.31 +/- 13.3 %\n",
            "Batch: 261   Training loss: 0.17839133739471436\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.63 +/- 12.6 %\n",
            "Batch: 262   Training loss: 0.17169611155986786\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.17 +/- 12.7 %\n",
            "Batch: 263   Training loss: 0.18763643503189087\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.27 +/- 12.4 %\n",
            "Batch: 264   Training loss: 0.19164928793907166\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.25 +/- 11.8 %\n",
            "Batch: 265   Training loss: 0.17737267911434174\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.77 +/- 13.4 %\n",
            "Batch: 266   Training loss: 0.19059626758098602\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.91 +/- 11.9 %\n",
            "Batch: 267   Training loss: 0.18289224803447723\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.47 +/- 13.0 %\n",
            "Batch: 268   Training loss: 0.17635098099708557\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.29 +/- 12.3 %\n",
            "Batch: 269   Training loss: 0.17366303503513336\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.69 +/- 13.1 %\n",
            "Batch: 270   Training loss: 0.19294503331184387\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.81 +/- 10.9 %\n",
            "Batch: 271   Training loss: 0.1666385531425476\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.10 +/- 13.8 %\n",
            "Batch: 272   Training loss: 0.1831783801317215\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.03 +/- 12.7 %\n",
            "Batch: 273   Training loss: 0.17149187624454498\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.37 +/- 14.9 %\n",
            "Batch: 274   Training loss: 0.1750791072845459\n",
            "Precision: 62.96 +/- 48.3 %\n",
            "Recall: 13.58 +/- 12.1 %\n",
            "Batch: 275   Training loss: 0.18430373072624207\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.68 +/- 15.2 %\n",
            "Batch: 276   Training loss: 0.1721103936433792\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.74 +/- 14.2 %\n",
            "Batch: 277   Training loss: 0.16890248656272888\n",
            "Precision: 74.07 +/- 43.8 %\n",
            "Recall: 19.04 +/- 15.0 %\n",
            "Batch: 278   Training loss: 0.1706996113061905\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.25 +/- 15.7 %\n",
            "Batch: 279   Training loss: 0.1870773434638977\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.43 +/- 14.4 %\n",
            "Batch: 280   Training loss: 0.19739195704460144\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.70 +/- 14.7 %\n",
            "Batch: 281   Training loss: 0.17466066777706146\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.41 +/- 14.6 %\n",
            "Batch: 282   Training loss: 0.17281991243362427\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.26 +/- 10.4 %\n",
            "Batch: 283   Training loss: 0.18869648873806\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.97 +/- 12.9 %\n",
            "Batch: 284   Training loss: 0.17054085433483124\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.31 +/- 14.8 %\n",
            "Batch: 285   Training loss: 0.1755226105451584\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.74 +/- 13.4 %\n",
            "Batch: 286   Training loss: 0.1897638738155365\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.19 +/- 10.9 %\n",
            "Batch: 287   Training loss: 0.16122008860111237\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.06 +/- 13.8 %\n",
            "Batch: 288   Training loss: 0.1941290646791458\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 11.26 +/- 14.8 %\n",
            "Batch: 289   Training loss: 0.18869648873806\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.02 +/- 13.0 %\n",
            "Batch: 290   Training loss: 0.18438751995563507\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.95 +/- 12.1 %\n",
            "Batch: 291   Training loss: 0.18782757222652435\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.00 +/- 11.5 %\n",
            "Batch: 292   Training loss: 0.18519526720046997\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.44 +/- 14.2 %\n",
            "Batch: 293   Training loss: 0.18027958273887634\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.68 +/- 13.8 %\n",
            "Batch: 294   Training loss: 0.17929673194885254\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.00 +/- 14.7 %\n",
            "Batch: 295   Training loss: 0.1731434166431427\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.62 +/- 13.5 %\n",
            "Batch: 296   Training loss: 0.1784803420305252\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.37 +/- 11.4 %\n",
            "Batch: 297   Training loss: 0.1896933764219284\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.69 +/- 16.4 %\n",
            "Batch: 298   Training loss: 0.17798827588558197\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.71 +/- 14.2 %\n",
            "[7,   300] loss: 0.167\n",
            "Batch: 299   Training loss: 0.16713638603687286\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.72 +/- 15.3 %\n",
            "Batch: 300   Training loss: 0.183601975440979\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.40 +/- 11.4 %\n",
            "Batch: 301   Training loss: 0.15648168325424194\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.49 +/- 14.9 %\n",
            "Batch: 302   Training loss: 0.19272206723690033\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.75 +/- 12.4 %\n",
            "Batch: 303   Training loss: 0.18488305807113647\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.74 +/- 16.1 %\n",
            "Batch: 304   Training loss: 0.19297732412815094\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.26 +/- 11.5 %\n",
            "Batch: 305   Training loss: 0.1862747222185135\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.89 +/- 15.7 %\n",
            "Batch: 306   Training loss: 0.16948050260543823\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.29 +/- 14.4 %\n",
            "Batch: 307   Training loss: 0.17638882994651794\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.28 +/- 13.7 %\n",
            "Batch: 308   Training loss: 0.17578953504562378\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.00 +/- 12.2 %\n",
            "Batch: 309   Training loss: 0.17935264110565186\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.14 +/- 13.4 %\n",
            "Batch: 310   Training loss: 0.18397678434848785\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 14.62 +/- 10.9 %\n",
            "Batch: 311   Training loss: 0.1968841254711151\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 9.32 +/- 9.6 %\n",
            "Batch: 312   Training loss: 0.18368159234523773\n",
            "Precision: 73.33 +/- 44.2 %\n",
            "Recall: 18.12 +/- 13.8 %\n",
            "Epoch:  7\n",
            "Batch: 0   Training loss: 0.16837911307811737\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 13.12 +/- 15.4 %\n",
            "Batch: 1   Training loss: 0.18904155492782593\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.59 +/- 15.0 %\n",
            "Batch: 2   Training loss: 0.17815157771110535\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.36 +/- 13.7 %\n",
            "Batch: 3   Training loss: 0.16881899535655975\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.15 +/- 13.5 %\n",
            "Batch: 4   Training loss: 0.1790744811296463\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.74 +/- 12.9 %\n",
            "Batch: 5   Training loss: 0.1731504499912262\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.49 +/- 11.5 %\n",
            "Batch: 6   Training loss: 0.17893746495246887\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.90 +/- 14.3 %\n",
            "Batch: 7   Training loss: 0.16707564890384674\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.47 +/- 14.8 %\n",
            "Batch: 8   Training loss: 0.17478959262371063\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.16 +/- 13.5 %\n",
            "Batch: 9   Training loss: 0.20392148196697235\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.38 +/- 11.8 %\n",
            "Batch: 10   Training loss: 0.1590065211057663\n",
            "Precision: 35.71 +/- 47.9 %\n",
            "Recall: 10.91 +/- 16.6 %\n",
            "Batch: 11   Training loss: 0.1753329485654831\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.93 +/- 12.9 %\n",
            "Batch: 12   Training loss: 0.1727721095085144\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.32 +/- 13.9 %\n",
            "Batch: 13   Training loss: 0.1720895618200302\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 20.53 +/- 16.2 %\n",
            "Batch: 14   Training loss: 0.16859500110149384\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 21.09 +/- 15.9 %\n",
            "Batch: 15   Training loss: 0.16780565679073334\n",
            "Precision: 81.48 +/- 38.8 %\n",
            "Recall: 18.95 +/- 12.5 %\n",
            "Batch: 16   Training loss: 0.18573343753814697\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.30 +/- 15.1 %\n",
            "Batch: 17   Training loss: 0.17826633155345917\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.49 +/- 15.1 %\n",
            "Batch: 18   Training loss: 0.1915358603000641\n",
            "Precision: 85.71 +/- 35.0 %\n",
            "Recall: 18.56 +/- 10.5 %\n",
            "Batch: 19   Training loss: 0.16803894937038422\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.48 +/- 15.4 %\n",
            "Batch: 20   Training loss: 0.18327203392982483\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.61 +/- 15.9 %\n",
            "Batch: 21   Training loss: 0.1896134316921234\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.25 +/- 13.7 %\n",
            "Batch: 22   Training loss: 0.1809157431125641\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.35 +/- 13.6 %\n",
            "Batch: 23   Training loss: 0.1935815066099167\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.36 +/- 13.3 %\n",
            "Batch: 24   Training loss: 0.1737709641456604\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.86 +/- 13.2 %\n",
            "Batch: 25   Training loss: 0.1701602339744568\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.16 +/- 13.7 %\n",
            "Batch: 26   Training loss: 0.15746743977069855\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.08 +/- 14.4 %\n",
            "Batch: 27   Training loss: 0.18130925297737122\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.42 +/- 16.5 %\n",
            "Batch: 28   Training loss: 0.16102451086044312\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.74 +/- 16.4 %\n",
            "Batch: 29   Training loss: 0.18516644835472107\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.60 +/- 11.8 %\n",
            "Batch: 30   Training loss: 0.19335173070430756\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 14.08 +/- 11.3 %\n",
            "Batch: 31   Training loss: 0.18994934856891632\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.48 +/- 10.8 %\n",
            "Batch: 32   Training loss: 0.18747127056121826\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.46 +/- 13.1 %\n",
            "Batch: 33   Training loss: 0.17035932838916779\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 17.64 +/- 18.6 %\n",
            "Batch: 34   Training loss: 0.16985343396663666\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 20.01 +/- 14.1 %\n",
            "Batch: 35   Training loss: 0.16771188378334045\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.68 +/- 13.6 %\n",
            "Batch: 36   Training loss: 0.17887428402900696\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.50 +/- 14.3 %\n",
            "Batch: 37   Training loss: 0.20252956449985504\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 8.00 +/- 10.2 %\n",
            "Batch: 38   Training loss: 0.1896391361951828\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.79 +/- 11.3 %\n",
            "Batch: 39   Training loss: 0.1675378978252411\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.70 +/- 14.8 %\n",
            "Batch: 40   Training loss: 0.1773049533367157\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.72 +/- 16.5 %\n",
            "Batch: 41   Training loss: 0.18642093241214752\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.43 +/- 11.3 %\n",
            "Batch: 42   Training loss: 0.1831764131784439\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.71 +/- 14.5 %\n",
            "Batch: 43   Training loss: 0.1634223759174347\n",
            "Precision: 59.26 +/- 49.1 %\n",
            "Recall: 14.95 +/- 14.6 %\n",
            "Batch: 44   Training loss: 0.18272456526756287\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.00 +/- 12.0 %\n",
            "Batch: 45   Training loss: 0.17009761929512024\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.94 +/- 11.2 %\n",
            "Batch: 46   Training loss: 0.184849813580513\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 8.49 +/- 11.4 %\n",
            "Batch: 47   Training loss: 0.1885627955198288\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.90 +/- 13.1 %\n",
            "Batch: 48   Training loss: 0.18644188344478607\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 14.59 +/- 10.9 %\n",
            "[8,    50] loss: 0.169\n",
            "Batch: 49   Training loss: 0.1694539338350296\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.03 +/- 14.2 %\n",
            "Batch: 50   Training loss: 0.19545941054821014\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.25 +/- 11.2 %\n",
            "Batch: 51   Training loss: 0.17487867176532745\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 11.27 +/- 14.7 %\n",
            "Batch: 52   Training loss: 0.16714681684970856\n",
            "Precision: 35.71 +/- 47.9 %\n",
            "Recall: 8.59 +/- 13.2 %\n",
            "Batch: 53   Training loss: 0.19928781688213348\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 7.86 +/- 10.3 %\n",
            "Batch: 54   Training loss: 0.18250608444213867\n",
            "Precision: 48.15 +/- 50.0 %\n",
            "Recall: 11.35 +/- 13.2 %\n",
            "Batch: 55   Training loss: 0.18361417949199677\n",
            "Precision: 85.71 +/- 35.0 %\n",
            "Recall: 18.37 +/- 10.7 %\n",
            "Batch: 56   Training loss: 0.17537905275821686\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.58 +/- 13.3 %\n",
            "Batch: 57   Training loss: 0.17733173072338104\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.91 +/- 16.1 %\n",
            "Batch: 58   Training loss: 0.16706764698028564\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.83 +/- 13.4 %\n",
            "Batch: 59   Training loss: 0.1730518639087677\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.03 +/- 12.1 %\n",
            "Batch: 60   Training loss: 0.17143748700618744\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 18.35 +/- 16.4 %\n",
            "Batch: 61   Training loss: 0.1903318166732788\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.74 +/- 15.6 %\n",
            "Batch: 62   Training loss: 0.16791489720344543\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.07 +/- 12.0 %\n",
            "Batch: 63   Training loss: 0.17096473276615143\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.34 +/- 14.0 %\n",
            "Batch: 64   Training loss: 0.19105935096740723\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.75 +/- 13.5 %\n",
            "Batch: 65   Training loss: 0.18017412722110748\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.93 +/- 13.5 %\n",
            "Batch: 66   Training loss: 0.18657706677913666\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.27 +/- 15.4 %\n",
            "Batch: 67   Training loss: 0.17675234377384186\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.45 +/- 11.3 %\n",
            "Batch: 68   Training loss: 0.18793290853500366\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.69 +/- 11.7 %\n",
            "Batch: 69   Training loss: 0.171989306807518\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.75 +/- 11.9 %\n",
            "Batch: 70   Training loss: 0.16438332200050354\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 11.80 +/- 15.6 %\n",
            "Batch: 71   Training loss: 0.18141749501228333\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.47 +/- 11.1 %\n",
            "Batch: 72   Training loss: 0.16732428967952728\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.27 +/- 13.5 %\n",
            "Batch: 73   Training loss: 0.180365189909935\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.44 +/- 13.7 %\n",
            "Batch: 74   Training loss: 0.172128826379776\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 20.47 +/- 14.7 %\n",
            "Batch: 75   Training loss: 0.15194836258888245\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 13.72 +/- 17.6 %\n",
            "Batch: 76   Training loss: 0.1840340793132782\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.34 +/- 10.1 %\n",
            "Batch: 77   Training loss: 0.17378999292850494\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.66 +/- 15.5 %\n",
            "Batch: 78   Training loss: 0.1752430498600006\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.79 +/- 11.6 %\n",
            "Batch: 79   Training loss: 0.16877147555351257\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.74 +/- 14.7 %\n",
            "Batch: 80   Training loss: 0.19304311275482178\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.00 +/- 13.4 %\n",
            "Batch: 81   Training loss: 0.17504161596298218\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.54 +/- 13.1 %\n",
            "Batch: 82   Training loss: 0.19273148477077484\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.37 +/- 12.2 %\n",
            "Batch: 83   Training loss: 0.19968107342720032\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 9.39 +/- 10.5 %\n",
            "Batch: 84   Training loss: 0.17567938566207886\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.82 +/- 13.5 %\n",
            "Batch: 85   Training loss: 0.17214028537273407\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.66 +/- 13.4 %\n",
            "Batch: 86   Training loss: 0.16990460455417633\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 21.53 +/- 15.1 %\n",
            "Batch: 87   Training loss: 0.18787507712841034\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.07 +/- 11.9 %\n",
            "Batch: 88   Training loss: 0.178249329328537\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 19.77 +/- 13.3 %\n",
            "Batch: 89   Training loss: 0.1798934042453766\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.83 +/- 11.9 %\n",
            "Batch: 90   Training loss: 0.1814267933368683\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.58 +/- 12.3 %\n",
            "Batch: 91   Training loss: 0.19132743775844574\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 14.74 +/- 12.0 %\n",
            "Batch: 92   Training loss: 0.16329969465732574\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 18.35 +/- 16.4 %\n",
            "Batch: 93   Training loss: 0.17041300237178802\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.92 +/- 14.5 %\n",
            "Batch: 94   Training loss: 0.17166881263256073\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.53 +/- 15.3 %\n",
            "Batch: 95   Training loss: 0.1780300736427307\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.55 +/- 12.4 %\n",
            "Batch: 96   Training loss: 0.18313130736351013\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 21.03 +/- 14.6 %\n",
            "Batch: 97   Training loss: 0.1929508000612259\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.09 +/- 11.8 %\n",
            "Batch: 98   Training loss: 0.1807681769132614\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.38 +/- 13.6 %\n",
            "[8,   100] loss: 0.174\n",
            "Batch: 99   Training loss: 0.17422933876514435\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.33 +/- 13.3 %\n",
            "Batch: 100   Training loss: 0.17581981420516968\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.44 +/- 12.9 %\n",
            "Batch: 101   Training loss: 0.17708507180213928\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.18 +/- 15.5 %\n",
            "Batch: 102   Training loss: 0.16782216727733612\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.59 +/- 16.1 %\n",
            "Batch: 103   Training loss: 0.17252786457538605\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 8.17 +/- 9.8 %\n",
            "Batch: 104   Training loss: 0.17645011842250824\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 9.35 +/- 10.5 %\n",
            "Batch: 105   Training loss: 0.18557532131671906\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.57 +/- 11.7 %\n",
            "Batch: 106   Training loss: 0.18105031549930573\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.38 +/- 15.0 %\n",
            "Batch: 107   Training loss: 0.19555747509002686\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.86 +/- 12.7 %\n",
            "Batch: 108   Training loss: 0.17921149730682373\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.72 +/- 12.5 %\n",
            "Batch: 109   Training loss: 0.20669840276241302\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.97 +/- 11.2 %\n",
            "Batch: 110   Training loss: 0.16746386885643005\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.33 +/- 14.3 %\n",
            "Batch: 111   Training loss: 0.17776936292648315\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.07 +/- 11.0 %\n",
            "Batch: 112   Training loss: 0.17980234324932098\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.01 +/- 15.3 %\n",
            "Batch: 113   Training loss: 0.16934402287006378\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.32 +/- 14.9 %\n",
            "Batch: 114   Training loss: 0.1780450940132141\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.92 +/- 13.0 %\n",
            "Batch: 115   Training loss: 0.19757679104804993\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.38 +/- 12.4 %\n",
            "Batch: 116   Training loss: 0.19027647376060486\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.44 +/- 12.9 %\n",
            "Batch: 117   Training loss: 0.18775667250156403\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.67 +/- 16.1 %\n",
            "Batch: 118   Training loss: 0.17905324697494507\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 9.97 +/- 11.1 %\n",
            "Batch: 119   Training loss: 0.1670047491788864\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.77 +/- 11.6 %\n",
            "Batch: 120   Training loss: 0.18797670304775238\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.23 +/- 12.8 %\n",
            "Batch: 121   Training loss: 0.19031260907649994\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.70 +/- 12.9 %\n",
            "Batch: 122   Training loss: 0.18198604881763458\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.26 +/- 14.2 %\n",
            "Batch: 123   Training loss: 0.1779651641845703\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.20 +/- 13.1 %\n",
            "Batch: 124   Training loss: 0.18543563783168793\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.07 +/- 12.6 %\n",
            "Batch: 125   Training loss: 0.17197665572166443\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.86 +/- 15.3 %\n",
            "Batch: 126   Training loss: 0.16677901148796082\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.68 +/- 14.3 %\n",
            "Batch: 127   Training loss: 0.1729859560728073\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.83 +/- 15.3 %\n",
            "Batch: 128   Training loss: 0.18002665042877197\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.48 +/- 14.5 %\n",
            "Batch: 129   Training loss: 0.18436171114444733\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.83 +/- 12.9 %\n",
            "Batch: 130   Training loss: 0.1831180453300476\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 18.90 +/- 12.7 %\n",
            "Batch: 131   Training loss: 0.17532937228679657\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.81 +/- 13.1 %\n",
            "Batch: 132   Training loss: 0.18031413853168488\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.30 +/- 17.0 %\n",
            "Batch: 133   Training loss: 0.1838480681180954\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 17.99 +/- 13.0 %\n",
            "Batch: 134   Training loss: 0.19070057570934296\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.63 +/- 12.6 %\n",
            "Batch: 135   Training loss: 0.17320279777050018\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.33 +/- 14.7 %\n",
            "Batch: 136   Training loss: 0.1725645363330841\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 14.65 +/- 17.9 %\n",
            "Batch: 137   Training loss: 0.19824333488941193\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.94 +/- 12.7 %\n",
            "Batch: 138   Training loss: 0.18758629262447357\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.14 +/- 12.1 %\n",
            "Batch: 139   Training loss: 0.164199560880661\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 16.02 +/- 15.1 %\n",
            "Batch: 140   Training loss: 0.18846480548381805\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.39 +/- 12.9 %\n",
            "Batch: 141   Training loss: 0.17214876413345337\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.24 +/- 12.9 %\n",
            "Batch: 142   Training loss: 0.16891732811927795\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 18.70 +/- 16.6 %\n",
            "Batch: 143   Training loss: 0.17598196864128113\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.29 +/- 11.2 %\n",
            "Batch: 144   Training loss: 0.19723497331142426\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.46 +/- 10.8 %\n",
            "Batch: 145   Training loss: 0.1908605396747589\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.04 +/- 15.2 %\n",
            "Batch: 146   Training loss: 0.17575620114803314\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.87 +/- 14.7 %\n",
            "Batch: 147   Training loss: 0.186163529753685\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.01 +/- 13.0 %\n",
            "Batch: 148   Training loss: 0.18024349212646484\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.44 +/- 14.0 %\n",
            "[8,   150] loss: 0.165\n",
            "Batch: 149   Training loss: 0.16548769176006317\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.66 +/- 14.9 %\n",
            "Batch: 150   Training loss: 0.17123225331306458\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.90 +/- 14.1 %\n",
            "Batch: 151   Training loss: 0.16485176980495453\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.68 +/- 16.2 %\n",
            "Batch: 152   Training loss: 0.17735490202903748\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 14.75 +/- 10.2 %\n",
            "Batch: 153   Training loss: 0.18223837018013\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.84 +/- 12.3 %\n",
            "Batch: 154   Training loss: 0.18423138558864594\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.92 +/- 12.6 %\n",
            "Batch: 155   Training loss: 0.18505248427391052\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.12 +/- 15.4 %\n",
            "Batch: 156   Training loss: 0.1599515974521637\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.82 +/- 13.6 %\n",
            "Batch: 157   Training loss: 0.1707170605659485\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.85 +/- 12.5 %\n",
            "Batch: 158   Training loss: 0.16572827100753784\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.86 +/- 15.1 %\n",
            "Batch: 159   Training loss: 0.1768307238817215\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.10 +/- 11.5 %\n",
            "Batch: 160   Training loss: 0.17999304831027985\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.54 +/- 12.5 %\n",
            "Batch: 161   Training loss: 0.1888704001903534\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.12 +/- 12.1 %\n",
            "Batch: 162   Training loss: 0.1875130832195282\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.93 +/- 13.7 %\n",
            "Batch: 163   Training loss: 0.1755826324224472\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.38 +/- 15.2 %\n",
            "Batch: 164   Training loss: 0.16966009140014648\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.35 +/- 13.9 %\n",
            "Batch: 165   Training loss: 0.1588025838136673\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.37 +/- 14.5 %\n",
            "Batch: 166   Training loss: 0.18180595338344574\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 8.97 +/- 11.7 %\n",
            "Batch: 167   Training loss: 0.16463224589824677\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.02 +/- 14.5 %\n",
            "Batch: 168   Training loss: 0.16516819596290588\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.20 +/- 14.4 %\n",
            "Batch: 169   Training loss: 0.1847761571407318\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.57 +/- 15.2 %\n",
            "Batch: 170   Training loss: 0.16270782053470612\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 22.36 +/- 18.2 %\n",
            "Batch: 171   Training loss: 0.18875941634178162\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.76 +/- 12.8 %\n",
            "Batch: 172   Training loss: 0.17037415504455566\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.89 +/- 11.6 %\n",
            "Batch: 173   Training loss: 0.17609266936779022\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.39 +/- 14.4 %\n",
            "Batch: 174   Training loss: 0.18351247906684875\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.92 +/- 15.1 %\n",
            "Batch: 175   Training loss: 0.19520655274391174\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.06 +/- 13.6 %\n",
            "Batch: 176   Training loss: 0.1809147149324417\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.71 +/- 12.4 %\n",
            "Batch: 177   Training loss: 0.16982297599315643\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.61 +/- 15.0 %\n",
            "Batch: 178   Training loss: 0.17905189096927643\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.35 +/- 11.1 %\n",
            "Batch: 179   Training loss: 0.18784362077713013\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.45 +/- 10.9 %\n",
            "Batch: 180   Training loss: 0.18074962496757507\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.89 +/- 12.6 %\n",
            "Batch: 181   Training loss: 0.18146227300167084\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.63 +/- 12.3 %\n",
            "Batch: 182   Training loss: 0.19756357371807098\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.08 +/- 13.7 %\n",
            "Batch: 183   Training loss: 0.1902191936969757\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.26 +/- 13.4 %\n",
            "Batch: 184   Training loss: 0.17838987708091736\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.09 +/- 13.7 %\n",
            "Batch: 185   Training loss: 0.17596174776554108\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 18.14 +/- 16.9 %\n",
            "Batch: 186   Training loss: 0.1928030401468277\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.59 +/- 13.0 %\n",
            "Batch: 187   Training loss: 0.18653008341789246\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.59 +/- 10.5 %\n",
            "Batch: 188   Training loss: 0.1814451366662979\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.28 +/- 13.6 %\n",
            "Batch: 189   Training loss: 0.1712931990623474\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.00 +/- 11.8 %\n",
            "Batch: 190   Training loss: 0.16530749201774597\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.86 +/- 15.9 %\n",
            "Batch: 191   Training loss: 0.17459677159786224\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.71 +/- 12.5 %\n",
            "Batch: 192   Training loss: 0.17140288650989532\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.60 +/- 14.2 %\n",
            "Batch: 193   Training loss: 0.17736032605171204\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.31 +/- 13.5 %\n",
            "Batch: 194   Training loss: 0.17183241248130798\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 15.46 +/- 17.0 %\n",
            "Batch: 195   Training loss: 0.1913899928331375\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.18 +/- 14.4 %\n",
            "Batch: 196   Training loss: 0.18402929604053497\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.66 +/- 14.8 %\n",
            "Batch: 197   Training loss: 0.17011910676956177\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.31 +/- 14.4 %\n",
            "Batch: 198   Training loss: 0.20722618699073792\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.45 +/- 11.6 %\n",
            "[8,   200] loss: 0.177\n",
            "Batch: 199   Training loss: 0.17738021910190582\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.13 +/- 13.5 %\n",
            "Batch: 200   Training loss: 0.20015296339988708\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 8.80 +/- 9.4 %\n",
            "Batch: 201   Training loss: 0.19307546317577362\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.04 +/- 11.9 %\n",
            "Batch: 202   Training loss: 0.1811569482088089\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.39 +/- 16.2 %\n",
            "Batch: 203   Training loss: 0.17925356328487396\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.16 +/- 13.5 %\n",
            "Batch: 204   Training loss: 0.16940930485725403\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.55 +/- 15.7 %\n",
            "Batch: 205   Training loss: 0.16485553979873657\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.86 +/- 13.8 %\n",
            "Batch: 206   Training loss: 0.16327229142189026\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.38 +/- 13.4 %\n",
            "Batch: 207   Training loss: 0.1826709359884262\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.68 +/- 13.4 %\n",
            "Batch: 208   Training loss: 0.16874641180038452\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.97 +/- 13.9 %\n",
            "Batch: 209   Training loss: 0.18712541460990906\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.64 +/- 11.2 %\n",
            "Batch: 210   Training loss: 0.17117583751678467\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.75 +/- 13.8 %\n",
            "Batch: 211   Training loss: 0.16750065982341766\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.77 +/- 13.1 %\n",
            "Batch: 212   Training loss: 0.1755032241344452\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.92 +/- 14.8 %\n",
            "Batch: 213   Training loss: 0.17250847816467285\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.97 +/- 14.2 %\n",
            "Batch: 214   Training loss: 0.19429275393486023\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.84 +/- 14.9 %\n",
            "Batch: 215   Training loss: 0.17850026488304138\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.35 +/- 14.1 %\n",
            "Batch: 216   Training loss: 0.18578065931797028\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.19 +/- 14.7 %\n",
            "Batch: 217   Training loss: 0.18709410727024078\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 20.45 +/- 16.1 %\n",
            "Batch: 218   Training loss: 0.19533197581768036\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.77 +/- 13.4 %\n",
            "Batch: 219   Training loss: 0.1856153905391693\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.90 +/- 10.8 %\n",
            "Batch: 220   Training loss: 0.192399799823761\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.17 +/- 14.9 %\n",
            "Batch: 221   Training loss: 0.18162864446640015\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.66 +/- 16.4 %\n",
            "Batch: 222   Training loss: 0.18754515051841736\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.91 +/- 11.7 %\n",
            "Batch: 223   Training loss: 0.187738299369812\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.57 +/- 12.7 %\n",
            "Batch: 224   Training loss: 0.1841525286436081\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.56 +/- 14.4 %\n",
            "Batch: 225   Training loss: 0.16921551525592804\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.52 +/- 13.1 %\n",
            "Batch: 226   Training loss: 0.1713274121284485\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.69 +/- 11.8 %\n",
            "Batch: 227   Training loss: 0.16858315467834473\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.85 +/- 11.4 %\n",
            "Batch: 228   Training loss: 0.1778097003698349\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.94 +/- 11.8 %\n",
            "Batch: 229   Training loss: 0.18258805572986603\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.67 +/- 14.8 %\n",
            "Batch: 230   Training loss: 0.1776903122663498\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.02 +/- 15.7 %\n",
            "Batch: 231   Training loss: 0.18372301757335663\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.06 +/- 14.6 %\n",
            "Batch: 232   Training loss: 0.1752825528383255\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.13 +/- 12.6 %\n",
            "Batch: 233   Training loss: 0.17459602653980255\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.14 +/- 15.5 %\n",
            "Batch: 234   Training loss: 0.1670619398355484\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.98 +/- 16.4 %\n",
            "Batch: 235   Training loss: 0.17383624613285065\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.00 +/- 14.9 %\n",
            "Batch: 236   Training loss: 0.17206168174743652\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.30 +/- 15.6 %\n",
            "Batch: 237   Training loss: 0.16051796078681946\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.85 +/- 13.0 %\n",
            "Batch: 238   Training loss: 0.17548736929893494\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.74 +/- 12.0 %\n",
            "Batch: 239   Training loss: 0.18741551041603088\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.86 +/- 11.9 %\n",
            "Batch: 240   Training loss: 0.18543678522109985\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.19 +/- 13.4 %\n",
            "Batch: 241   Training loss: 0.17752860486507416\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.78 +/- 16.7 %\n",
            "Batch: 242   Training loss: 0.1711546927690506\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.72 +/- 13.1 %\n",
            "Batch: 243   Training loss: 0.17891164124011993\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.77 +/- 14.0 %\n",
            "Batch: 244   Training loss: 0.1767382025718689\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.05 +/- 11.9 %\n",
            "Batch: 245   Training loss: 0.16523121297359467\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.88 +/- 14.3 %\n",
            "Batch: 246   Training loss: 0.19581645727157593\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.20 +/- 13.8 %\n",
            "Batch: 247   Training loss: 0.18034975230693817\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 18.19 +/- 17.4 %\n",
            "Batch: 248   Training loss: 0.19835640490055084\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 7.56 +/- 11.3 %\n",
            "[8,   250] loss: 0.167\n",
            "Batch: 249   Training loss: 0.1667156219482422\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.30 +/- 14.5 %\n",
            "Batch: 250   Training loss: 0.16890370845794678\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.08 +/- 14.8 %\n",
            "Batch: 251   Training loss: 0.19329115748405457\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.42 +/- 12.8 %\n",
            "Batch: 252   Training loss: 0.20462700724601746\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.29 +/- 13.2 %\n",
            "Batch: 253   Training loss: 0.1761804074048996\n",
            "Precision: 55.56 +/- 49.7 %\n",
            "Recall: 15.63 +/- 16.6 %\n",
            "Batch: 254   Training loss: 0.18448768556118011\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.43 +/- 16.3 %\n",
            "Batch: 255   Training loss: 0.1764945685863495\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.70 +/- 14.1 %\n",
            "Batch: 256   Training loss: 0.18228943645954132\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.04 +/- 12.9 %\n",
            "Batch: 257   Training loss: 0.1722559630870819\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.49 +/- 13.8 %\n",
            "Batch: 258   Training loss: 0.1926373690366745\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.05 +/- 11.9 %\n",
            "Batch: 259   Training loss: 0.1761617809534073\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.30 +/- 13.9 %\n",
            "Batch: 260   Training loss: 0.1778545379638672\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.67 +/- 15.0 %\n",
            "Batch: 261   Training loss: 0.1704903244972229\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 19.05 +/- 16.1 %\n",
            "Batch: 262   Training loss: 0.17457081377506256\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.35 +/- 10.7 %\n",
            "Batch: 263   Training loss: 0.18063569068908691\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 8.71 +/- 10.8 %\n",
            "Batch: 264   Training loss: 0.18499119579792023\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.59 +/- 11.0 %\n",
            "Batch: 265   Training loss: 0.1777961552143097\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.95 +/- 11.5 %\n",
            "Batch: 266   Training loss: 0.19804047048091888\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.55 +/- 10.9 %\n",
            "Batch: 267   Training loss: 0.19059091806411743\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.55 +/- 10.7 %\n",
            "Batch: 268   Training loss: 0.18627598881721497\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.29 +/- 11.2 %\n",
            "Batch: 269   Training loss: 0.17133547365665436\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.20 +/- 13.7 %\n",
            "Batch: 270   Training loss: 0.17789819836616516\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.46 +/- 16.4 %\n",
            "Batch: 271   Training loss: 0.19358816742897034\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 15.63 +/- 11.9 %\n",
            "Batch: 272   Training loss: 0.18729542195796967\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.40 +/- 9.7 %\n",
            "Batch: 273   Training loss: 0.18325230479240417\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.27 +/- 14.9 %\n",
            "Batch: 274   Training loss: 0.17944766581058502\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.32 +/- 13.0 %\n",
            "Batch: 275   Training loss: 0.18458570539951324\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.36 +/- 14.1 %\n",
            "Batch: 276   Training loss: 0.19363437592983246\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.57 +/- 15.2 %\n",
            "Batch: 277   Training loss: 0.1825745850801468\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.35 +/- 12.6 %\n",
            "Batch: 278   Training loss: 0.1710168719291687\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.45 +/- 13.0 %\n",
            "Batch: 279   Training loss: 0.19225461781024933\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.17 +/- 16.2 %\n",
            "Batch: 280   Training loss: 0.170895054936409\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.41 +/- 12.8 %\n",
            "Batch: 281   Training loss: 0.1668969988822937\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.63 +/- 14.4 %\n",
            "Batch: 282   Training loss: 0.19066011905670166\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 15.77 +/- 11.7 %\n",
            "Batch: 283   Training loss: 0.1759733408689499\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.14 +/- 15.6 %\n",
            "Batch: 284   Training loss: 0.1492537260055542\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 16.80 +/- 18.1 %\n",
            "Batch: 285   Training loss: 0.18753573298454285\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.25 +/- 12.3 %\n",
            "Batch: 286   Training loss: 0.1891723871231079\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.85 +/- 11.2 %\n",
            "Batch: 287   Training loss: 0.1833457201719284\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.85 +/- 11.9 %\n",
            "Batch: 288   Training loss: 0.20156674087047577\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.10 +/- 10.9 %\n",
            "Batch: 289   Training loss: 0.18336836993694305\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.70 +/- 12.4 %\n",
            "Batch: 290   Training loss: 0.17119455337524414\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.10 +/- 12.9 %\n",
            "Batch: 291   Training loss: 0.1760648936033249\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.00 +/- 11.4 %\n",
            "Batch: 292   Training loss: 0.17766302824020386\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.58 +/- 12.1 %\n",
            "Batch: 293   Training loss: 0.1712772697210312\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 21.65 +/- 13.7 %\n",
            "Batch: 294   Training loss: 0.17139175534248352\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 17.63 +/- 18.3 %\n",
            "Batch: 295   Training loss: 0.17453360557556152\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.63 +/- 14.0 %\n",
            "Batch: 296   Training loss: 0.17399920523166656\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.65 +/- 12.9 %\n",
            "Batch: 297   Training loss: 0.18042872846126556\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.64 +/- 13.8 %\n",
            "Batch: 298   Training loss: 0.18072301149368286\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.12 +/- 13.5 %\n",
            "[8,   300] loss: 0.188\n",
            "Batch: 299   Training loss: 0.18807430565357208\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.45 +/- 11.9 %\n",
            "Batch: 300   Training loss: 0.18555252254009247\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.36 +/- 16.3 %\n",
            "Batch: 301   Training loss: 0.18426986038684845\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.40 +/- 11.3 %\n",
            "Batch: 302   Training loss: 0.1870069056749344\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.20 +/- 12.3 %\n",
            "Batch: 303   Training loss: 0.18223980069160461\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.99 +/- 13.5 %\n",
            "Batch: 304   Training loss: 0.15652580559253693\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 14.14 +/- 16.5 %\n",
            "Batch: 305   Training loss: 0.16451628506183624\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.43 +/- 14.1 %\n",
            "Batch: 306   Training loss: 0.17239360511302948\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.26 +/- 13.0 %\n",
            "Batch: 307   Training loss: 0.19828401505947113\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.46 +/- 10.8 %\n",
            "Batch: 308   Training loss: 0.17499616742134094\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 19.02 +/- 17.9 %\n",
            "Batch: 309   Training loss: 0.16471967101097107\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.94 +/- 14.0 %\n",
            "Batch: 310   Training loss: 0.18949772417545319\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.58 +/- 14.0 %\n",
            "Batch: 311   Training loss: 0.18383736908435822\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 15.15 +/- 11.3 %\n",
            "Batch: 312   Training loss: 0.18761661648750305\n",
            "Precision: 46.67 +/- 49.9 %\n",
            "Recall: 10.34 +/- 12.5 %\n",
            "Epoch:  8\n",
            "Batch: 0   Training loss: 0.17422014474868774\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.88 +/- 12.7 %\n",
            "Batch: 1   Training loss: 0.1757994145154953\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.03 +/- 11.7 %\n",
            "Batch: 2   Training loss: 0.18548935651779175\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 17.81 +/- 10.1 %\n",
            "Batch: 3   Training loss: 0.20011988282203674\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.59 +/- 11.7 %\n",
            "Batch: 4   Training loss: 0.1756511926651001\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.29 +/- 13.0 %\n",
            "Batch: 5   Training loss: 0.19475796818733215\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.33 +/- 12.4 %\n",
            "Batch: 6   Training loss: 0.17746315896511078\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.30 +/- 16.4 %\n",
            "Batch: 7   Training loss: 0.19036425650119781\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.79 +/- 13.4 %\n",
            "Batch: 8   Training loss: 0.18621960282325745\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.84 +/- 12.4 %\n",
            "Batch: 9   Training loss: 0.17403794825077057\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.66 +/- 13.5 %\n",
            "Batch: 10   Training loss: 0.17744731903076172\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.63 +/- 15.6 %\n",
            "Batch: 11   Training loss: 0.17289772629737854\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.59 +/- 12.7 %\n",
            "Batch: 12   Training loss: 0.15759843587875366\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.70 +/- 15.5 %\n",
            "Batch: 13   Training loss: 0.19465456902980804\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.26 +/- 11.0 %\n",
            "Batch: 14   Training loss: 0.17945155501365662\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.64 +/- 13.5 %\n",
            "Batch: 15   Training loss: 0.17502562701702118\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.34 +/- 16.5 %\n",
            "Batch: 16   Training loss: 0.1930810511112213\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.95 +/- 15.0 %\n",
            "Batch: 17   Training loss: 0.1905933916568756\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 7.75 +/- 9.5 %\n",
            "Batch: 18   Training loss: 0.19751374423503876\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.70 +/- 10.6 %\n",
            "Batch: 19   Training loss: 0.1745910793542862\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.46 +/- 15.3 %\n",
            "Batch: 20   Training loss: 0.1765880286693573\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.02 +/- 14.0 %\n",
            "Batch: 21   Training loss: 0.1817293018102646\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.37 +/- 12.3 %\n",
            "Batch: 22   Training loss: 0.1910334974527359\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.87 +/- 12.1 %\n",
            "Batch: 23   Training loss: 0.20271919667720795\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.33 +/- 10.8 %\n",
            "Batch: 24   Training loss: 0.17688608169555664\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.70 +/- 13.0 %\n",
            "Batch: 25   Training loss: 0.17257773876190186\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.11 +/- 14.2 %\n",
            "Batch: 26   Training loss: 0.1628233641386032\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.30 +/- 14.3 %\n",
            "Batch: 27   Training loss: 0.17053256928920746\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.95 +/- 13.9 %\n",
            "Batch: 28   Training loss: 0.15978507697582245\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 17.44 +/- 16.0 %\n",
            "Batch: 29   Training loss: 0.17986434698104858\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.01 +/- 13.9 %\n",
            "Batch: 30   Training loss: 0.18312649428844452\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.87 +/- 13.8 %\n",
            "Batch: 31   Training loss: 0.18181009590625763\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.31 +/- 13.3 %\n",
            "Batch: 32   Training loss: 0.16847623884677887\n",
            "Precision: 59.26 +/- 49.1 %\n",
            "Recall: 16.53 +/- 16.3 %\n",
            "Batch: 33   Training loss: 0.16171078383922577\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 17.30 +/- 15.5 %\n",
            "Batch: 34   Training loss: 0.19020816683769226\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.75 +/- 12.4 %\n",
            "Batch: 35   Training loss: 0.17960524559020996\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.33 +/- 14.6 %\n",
            "Batch: 36   Training loss: 0.17817741632461548\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.05 +/- 13.1 %\n",
            "Batch: 37   Training loss: 0.1844431757926941\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.45 +/- 15.1 %\n",
            "Batch: 38   Training loss: 0.1882907897233963\n",
            "Precision: 66.67 +/- 47.1 %\n",
            "Recall: 16.23 +/- 15.4 %\n",
            "Batch: 39   Training loss: 0.17389237880706787\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.05 +/- 14.4 %\n",
            "Batch: 40   Training loss: 0.18462245166301727\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 20.82 +/- 14.7 %\n",
            "Batch: 41   Training loss: 0.1888180375099182\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 15.55 +/- 11.2 %\n",
            "Batch: 42   Training loss: 0.1785736083984375\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.93 +/- 11.9 %\n",
            "Batch: 43   Training loss: 0.1769009679555893\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.08 +/- 11.6 %\n",
            "Batch: 44   Training loss: 0.18635344505310059\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.09 +/- 14.0 %\n",
            "Batch: 45   Training loss: 0.1913396120071411\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.75 +/- 12.1 %\n",
            "Batch: 46   Training loss: 0.17642110586166382\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.12 +/- 13.0 %\n",
            "Batch: 47   Training loss: 0.17773845791816711\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.34 +/- 15.8 %\n",
            "Batch: 48   Training loss: 0.18233102560043335\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.73 +/- 15.0 %\n",
            "[9,    50] loss: 0.187\n",
            "Batch: 49   Training loss: 0.18708254396915436\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.41 +/- 14.9 %\n",
            "Batch: 50   Training loss: 0.18515855073928833\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.13 +/- 13.6 %\n",
            "Batch: 51   Training loss: 0.17429257929325104\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.69 +/- 12.6 %\n",
            "Batch: 52   Training loss: 0.1661146581172943\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 21.10 +/- 16.6 %\n",
            "Batch: 53   Training loss: 0.17047442495822906\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.41 +/- 15.6 %\n",
            "Batch: 54   Training loss: 0.17938955128192902\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.72 +/- 13.8 %\n",
            "Batch: 55   Training loss: 0.18970255553722382\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.59 +/- 12.8 %\n",
            "Batch: 56   Training loss: 0.17667590081691742\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 19.46 +/- 15.7 %\n",
            "Batch: 57   Training loss: 0.16977010667324066\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.45 +/- 15.2 %\n",
            "Batch: 58   Training loss: 0.17472770810127258\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.38 +/- 10.5 %\n",
            "Batch: 59   Training loss: 0.17469577491283417\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.99 +/- 14.6 %\n",
            "Batch: 60   Training loss: 0.19120316207408905\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 8.83 +/- 11.0 %\n",
            "Batch: 61   Training loss: 0.1890077143907547\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.32 +/- 13.0 %\n",
            "Batch: 62   Training loss: 0.17422598600387573\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.19 +/- 13.9 %\n",
            "Batch: 63   Training loss: 0.17675518989562988\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.73 +/- 13.8 %\n",
            "Batch: 64   Training loss: 0.17256714403629303\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.70 +/- 13.1 %\n",
            "Batch: 65   Training loss: 0.1689843237400055\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.88 +/- 14.4 %\n",
            "Batch: 66   Training loss: 0.18504832684993744\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.80 +/- 16.6 %\n",
            "Batch: 67   Training loss: 0.16554783284664154\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.18 +/- 14.5 %\n",
            "Batch: 68   Training loss: 0.16995841264724731\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.40 +/- 17.0 %\n",
            "Batch: 69   Training loss: 0.17888276278972626\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.02 +/- 11.1 %\n",
            "Batch: 70   Training loss: 0.1663919985294342\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.63 +/- 14.4 %\n",
            "Batch: 71   Training loss: 0.20310147106647491\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.03 +/- 14.4 %\n",
            "Batch: 72   Training loss: 0.17660799622535706\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.21 +/- 14.1 %\n",
            "Batch: 73   Training loss: 0.18218249082565308\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 9.94 +/- 10.8 %\n",
            "Batch: 74   Training loss: 0.17836259305477142\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.39 +/- 14.0 %\n",
            "Batch: 75   Training loss: 0.16820086538791656\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.47 +/- 13.0 %\n",
            "Batch: 76   Training loss: 0.1667192578315735\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.16 +/- 14.6 %\n",
            "Batch: 77   Training loss: 0.17863455414772034\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.66 +/- 15.1 %\n",
            "Batch: 78   Training loss: 0.18723736703395844\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.42 +/- 11.1 %\n",
            "Batch: 79   Training loss: 0.1721213012933731\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.89 +/- 13.4 %\n",
            "Batch: 80   Training loss: 0.17307686805725098\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 19.37 +/- 18.4 %\n",
            "Batch: 81   Training loss: 0.190972238779068\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 16.59 +/- 12.0 %\n",
            "Batch: 82   Training loss: 0.1916443556547165\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.10 +/- 12.2 %\n",
            "Batch: 83   Training loss: 0.2008122354745865\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.81 +/- 12.6 %\n",
            "Batch: 84   Training loss: 0.17602989077568054\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.94 +/- 13.6 %\n",
            "Batch: 85   Training loss: 0.16398650407791138\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.86 +/- 14.2 %\n",
            "Batch: 86   Training loss: 0.1858615130186081\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.49 +/- 14.8 %\n",
            "Batch: 87   Training loss: 0.17551100254058838\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 14.93 +/- 10.8 %\n",
            "Batch: 88   Training loss: 0.1862134486436844\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.72 +/- 12.5 %\n",
            "Batch: 89   Training loss: 0.17440517246723175\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 19.53 +/- 12.3 %\n",
            "Batch: 90   Training loss: 0.16766893863677979\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.63 +/- 13.9 %\n",
            "Batch: 91   Training loss: 0.16856998205184937\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.96 +/- 14.2 %\n",
            "Batch: 92   Training loss: 0.15912654995918274\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.50 +/- 13.7 %\n",
            "Batch: 93   Training loss: 0.1856909543275833\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.85 +/- 13.6 %\n",
            "Batch: 94   Training loss: 0.18687425553798676\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 14.57 +/- 11.5 %\n",
            "Batch: 95   Training loss: 0.17468635737895966\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.85 +/- 13.4 %\n",
            "Batch: 96   Training loss: 0.18583568930625916\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.00 +/- 13.0 %\n",
            "Batch: 97   Training loss: 0.18549546599388123\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.12 +/- 14.3 %\n",
            "Batch: 98   Training loss: 0.18909019231796265\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.66 +/- 10.8 %\n",
            "[9,   100] loss: 0.184\n",
            "Batch: 99   Training loss: 0.18404965102672577\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.09 +/- 14.3 %\n",
            "Batch: 100   Training loss: 0.20275041460990906\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 14.27 +/- 9.4 %\n",
            "Batch: 101   Training loss: 0.20102208852767944\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.49 +/- 13.9 %\n",
            "Batch: 102   Training loss: 0.16537483036518097\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.49 +/- 16.0 %\n",
            "Batch: 103   Training loss: 0.1747385710477829\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.95 +/- 15.1 %\n",
            "Batch: 104   Training loss: 0.17690075933933258\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.12 +/- 12.8 %\n",
            "Batch: 105   Training loss: 0.20003895461559296\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.02 +/- 12.7 %\n",
            "Batch: 106   Training loss: 0.1827823370695114\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.89 +/- 13.0 %\n",
            "Batch: 107   Training loss: 0.18238785862922668\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.81 +/- 13.2 %\n",
            "Batch: 108   Training loss: 0.17495644092559814\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.74 +/- 15.3 %\n",
            "Batch: 109   Training loss: 0.18320950865745544\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.16 +/- 13.9 %\n",
            "Batch: 110   Training loss: 0.18055258691310883\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 18.83 +/- 11.5 %\n",
            "Batch: 111   Training loss: 0.1727541983127594\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 9.05 +/- 11.9 %\n",
            "Batch: 112   Training loss: 0.18001295626163483\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.31 +/- 13.5 %\n",
            "Batch: 113   Training loss: 0.17497394979000092\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.01 +/- 13.8 %\n",
            "Batch: 114   Training loss: 0.16965098679065704\n",
            "Precision: 35.71 +/- 47.9 %\n",
            "Recall: 9.51 +/- 14.9 %\n",
            "Batch: 115   Training loss: 0.17488636076450348\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.92 +/- 14.9 %\n",
            "Batch: 116   Training loss: 0.1817856729030609\n",
            "Precision: 85.71 +/- 35.0 %\n",
            "Recall: 20.07 +/- 11.8 %\n",
            "Batch: 117   Training loss: 0.18174439668655396\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.10 +/- 14.7 %\n",
            "Batch: 118   Training loss: 0.1652616262435913\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.03 +/- 14.5 %\n",
            "Batch: 119   Training loss: 0.1827116161584854\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.82 +/- 13.0 %\n",
            "Batch: 120   Training loss: 0.1686316579580307\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.74 +/- 14.0 %\n",
            "Batch: 121   Training loss: 0.17727962136268616\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.65 +/- 14.5 %\n",
            "Batch: 122   Training loss: 0.1674782633781433\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.55 +/- 12.4 %\n",
            "Batch: 123   Training loss: 0.1890411227941513\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.15 +/- 12.2 %\n",
            "Batch: 124   Training loss: 0.17204272747039795\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.49 +/- 11.7 %\n",
            "Batch: 125   Training loss: 0.18247300386428833\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.71 +/- 14.4 %\n",
            "Batch: 126   Training loss: 0.20025277137756348\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.30 +/- 10.9 %\n",
            "Batch: 127   Training loss: 0.18622331321239471\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.39 +/- 15.0 %\n",
            "Batch: 128   Training loss: 0.18089741468429565\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.71 +/- 15.4 %\n",
            "Batch: 129   Training loss: 0.18531140685081482\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.03 +/- 15.6 %\n",
            "Batch: 130   Training loss: 0.1657732129096985\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.92 +/- 13.2 %\n",
            "Batch: 131   Training loss: 0.20132867991924286\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.85 +/- 12.6 %\n",
            "Batch: 132   Training loss: 0.17643748223781586\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.17 +/- 14.0 %\n",
            "Batch: 133   Training loss: 0.1728181391954422\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.90 +/- 15.3 %\n",
            "Batch: 134   Training loss: 0.17883437871932983\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.20 +/- 14.9 %\n",
            "Batch: 135   Training loss: 0.18160425126552582\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.11 +/- 14.4 %\n",
            "Batch: 136   Training loss: 0.19423404335975647\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 14.89 +/- 11.2 %\n",
            "Batch: 137   Training loss: 0.18843908607959747\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.38 +/- 12.5 %\n",
            "Batch: 138   Training loss: 0.17577771842479706\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.32 +/- 15.6 %\n",
            "Batch: 139   Training loss: 0.18786115944385529\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.60 +/- 11.4 %\n",
            "Batch: 140   Training loss: 0.17426316440105438\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.48 +/- 13.5 %\n",
            "Batch: 141   Training loss: 0.1893240511417389\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.17 +/- 14.5 %\n",
            "Batch: 142   Training loss: 0.1847412884235382\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.44 +/- 13.5 %\n",
            "Batch: 143   Training loss: 0.16987332701683044\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.23 +/- 14.8 %\n",
            "Batch: 144   Training loss: 0.1939152628183365\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.39 +/- 14.9 %\n",
            "Batch: 145   Training loss: 0.1706209033727646\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 18.19 +/- 16.2 %\n",
            "Batch: 146   Training loss: 0.17566591501235962\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.64 +/- 16.9 %\n",
            "Batch: 147   Training loss: 0.19197051227092743\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 18.01 +/- 12.3 %\n",
            "Batch: 148   Training loss: 0.1639518290758133\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.92 +/- 15.6 %\n",
            "[9,   150] loss: 0.167\n",
            "Batch: 149   Training loss: 0.16658414900302887\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.98 +/- 12.7 %\n",
            "Batch: 150   Training loss: 0.17549648880958557\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.67 +/- 13.7 %\n",
            "Batch: 151   Training loss: 0.17194399237632751\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.71 +/- 14.0 %\n",
            "Batch: 152   Training loss: 0.19298513233661652\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.15 +/- 11.5 %\n",
            "Batch: 153   Training loss: 0.19098912179470062\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.62 +/- 15.0 %\n",
            "Batch: 154   Training loss: 0.19538116455078125\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.53 +/- 11.2 %\n",
            "Batch: 155   Training loss: 0.16357921063899994\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.90 +/- 13.2 %\n",
            "Batch: 156   Training loss: 0.1664799004793167\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.04 +/- 11.7 %\n",
            "Batch: 157   Training loss: 0.17625191807746887\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.73 +/- 12.0 %\n",
            "Batch: 158   Training loss: 0.16432668268680573\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.14 +/- 15.4 %\n",
            "Batch: 159   Training loss: 0.18965846300125122\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.55 +/- 15.0 %\n",
            "Batch: 160   Training loss: 0.17031940817832947\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.96 +/- 14.1 %\n",
            "Batch: 161   Training loss: 0.1870764195919037\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.37 +/- 12.9 %\n",
            "Batch: 162   Training loss: 0.17663393914699554\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.10 +/- 11.4 %\n",
            "Batch: 163   Training loss: 0.17284883558750153\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.36 +/- 12.8 %\n",
            "Batch: 164   Training loss: 0.18062527477741241\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.73 +/- 13.1 %\n",
            "Batch: 165   Training loss: 0.1586901992559433\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 16.51 +/- 17.0 %\n",
            "Batch: 166   Training loss: 0.164692223072052\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 17.05 +/- 16.8 %\n",
            "Batch: 167   Training loss: 0.16869845986366272\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.09 +/- 14.6 %\n",
            "Batch: 168   Training loss: 0.19192104041576385\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.43 +/- 12.9 %\n",
            "Batch: 169   Training loss: 0.18002046644687653\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.56 +/- 13.0 %\n",
            "Batch: 170   Training loss: 0.1876835823059082\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.10 +/- 12.1 %\n",
            "Batch: 171   Training loss: 0.1847573071718216\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.59 +/- 11.9 %\n",
            "Batch: 172   Training loss: 0.16855263710021973\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.38 +/- 14.5 %\n",
            "Batch: 173   Training loss: 0.18028128147125244\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.46 +/- 12.6 %\n",
            "Batch: 174   Training loss: 0.1729789525270462\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.50 +/- 12.7 %\n",
            "Batch: 175   Training loss: 0.1853950172662735\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.33 +/- 13.1 %\n",
            "Batch: 176   Training loss: 0.17076636850833893\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.55 +/- 12.8 %\n",
            "Batch: 177   Training loss: 0.1920090913772583\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.19 +/- 12.2 %\n",
            "Batch: 178   Training loss: 0.18740355968475342\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.32 +/- 12.8 %\n",
            "Batch: 179   Training loss: 0.18779972195625305\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.05 +/- 11.7 %\n",
            "Batch: 180   Training loss: 0.1888093203306198\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.55 +/- 11.8 %\n",
            "Batch: 181   Training loss: 0.16881147027015686\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.16 +/- 13.2 %\n",
            "Batch: 182   Training loss: 0.1734020709991455\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.13 +/- 11.7 %\n",
            "Batch: 183   Training loss: 0.17663182318210602\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.42 +/- 11.8 %\n",
            "Batch: 184   Training loss: 0.1810501515865326\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 14.09 +/- 16.3 %\n",
            "Batch: 185   Training loss: 0.18356020748615265\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.04 +/- 13.6 %\n",
            "Batch: 186   Training loss: 0.17563997209072113\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 10.87 +/- 15.9 %\n",
            "Batch: 187   Training loss: 0.17053808271884918\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.88 +/- 12.2 %\n",
            "Batch: 188   Training loss: 0.16003404557704926\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 14.89 +/- 16.4 %\n",
            "Batch: 189   Training loss: 0.17517006397247314\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.70 +/- 16.0 %\n",
            "Batch: 190   Training loss: 0.19008290767669678\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.97 +/- 11.1 %\n",
            "Batch: 191   Training loss: 0.18197013437747955\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.73 +/- 13.8 %\n",
            "Batch: 192   Training loss: 0.17755760252475739\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.64 +/- 13.8 %\n",
            "Batch: 193   Training loss: 0.164171040058136\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 21.28 +/- 15.8 %\n",
            "Batch: 194   Training loss: 0.1669113039970398\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.91 +/- 15.7 %\n",
            "Batch: 195   Training loss: 0.17260035872459412\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.90 +/- 12.7 %\n",
            "Batch: 196   Training loss: 0.16714634001255035\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.52 +/- 15.9 %\n",
            "Batch: 197   Training loss: 0.19708667695522308\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.34 +/- 12.0 %\n",
            "Batch: 198   Training loss: 0.1770160049200058\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.66 +/- 12.5 %\n",
            "[9,   200] loss: 0.195\n",
            "Batch: 199   Training loss: 0.194954976439476\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.05 +/- 11.9 %\n",
            "Batch: 200   Training loss: 0.18238620460033417\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.39 +/- 12.7 %\n",
            "Batch: 201   Training loss: 0.1696222424507141\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.77 +/- 16.6 %\n",
            "Batch: 202   Training loss: 0.17568406462669373\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.42 +/- 14.7 %\n",
            "Batch: 203   Training loss: 0.2040671706199646\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.20 +/- 13.4 %\n",
            "Batch: 204   Training loss: 0.17767633497714996\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.50 +/- 11.7 %\n",
            "Batch: 205   Training loss: 0.16598081588745117\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.50 +/- 14.1 %\n",
            "Batch: 206   Training loss: 0.18369412422180176\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 14.09 +/- 15.9 %\n",
            "Batch: 207   Training loss: 0.18912965059280396\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.74 +/- 12.1 %\n",
            "Batch: 208   Training loss: 0.17738176882266998\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.44 +/- 14.5 %\n",
            "Batch: 209   Training loss: 0.17561554908752441\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.81 +/- 14.6 %\n",
            "Batch: 210   Training loss: 0.18738412857055664\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.39 +/- 11.3 %\n",
            "Batch: 211   Training loss: 0.17029891908168793\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.21 +/- 13.1 %\n",
            "Batch: 212   Training loss: 0.16786319017410278\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.94 +/- 12.8 %\n",
            "Batch: 213   Training loss: 0.1617325395345688\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.40 +/- 13.8 %\n",
            "Batch: 214   Training loss: 0.173515185713768\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.97 +/- 14.9 %\n",
            "Batch: 215   Training loss: 0.18057967722415924\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.16 +/- 14.5 %\n",
            "Batch: 216   Training loss: 0.16420385241508484\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.45 +/- 16.0 %\n",
            "Batch: 217   Training loss: 0.17713305354118347\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.60 +/- 15.0 %\n",
            "Batch: 218   Training loss: 0.1808745115995407\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.75 +/- 11.0 %\n",
            "Batch: 219   Training loss: 0.17748905718326569\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.50 +/- 12.8 %\n",
            "Batch: 220   Training loss: 0.19164693355560303\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.85 +/- 11.5 %\n",
            "Batch: 221   Training loss: 0.17630068957805634\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.96 +/- 11.9 %\n",
            "Batch: 222   Training loss: 0.17896342277526855\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.06 +/- 13.9 %\n",
            "Batch: 223   Training loss: 0.20194600522518158\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.94 +/- 13.0 %\n",
            "Batch: 224   Training loss: 0.17074453830718994\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.83 +/- 13.1 %\n",
            "Batch: 225   Training loss: 0.19147327542304993\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.09 +/- 14.7 %\n",
            "Batch: 226   Training loss: 0.17409619688987732\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.39 +/- 11.4 %\n",
            "Batch: 227   Training loss: 0.1696343719959259\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.86 +/- 15.4 %\n",
            "Batch: 228   Training loss: 0.19045600295066833\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.10 +/- 11.0 %\n",
            "Batch: 229   Training loss: 0.18725897371768951\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.42 +/- 12.4 %\n",
            "Batch: 230   Training loss: 0.16559705138206482\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.65 +/- 15.6 %\n",
            "Batch: 231   Training loss: 0.1850128471851349\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.47 +/- 11.3 %\n",
            "Batch: 232   Training loss: 0.17712977528572083\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 8.79 +/- 12.3 %\n",
            "Batch: 233   Training loss: 0.18392027914524078\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.59 +/- 12.1 %\n",
            "Batch: 234   Training loss: 0.18499571084976196\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.09 +/- 14.1 %\n",
            "Batch: 235   Training loss: 0.1783890277147293\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.67 +/- 13.6 %\n",
            "Batch: 236   Training loss: 0.1810140758752823\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.89 +/- 10.9 %\n",
            "Batch: 237   Training loss: 0.18813705444335938\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.21 +/- 13.6 %\n",
            "Batch: 238   Training loss: 0.18463130295276642\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.82 +/- 13.3 %\n",
            "Batch: 239   Training loss: 0.17657089233398438\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.01 +/- 14.0 %\n",
            "Batch: 240   Training loss: 0.18799835443496704\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.38 +/- 15.6 %\n",
            "Batch: 241   Training loss: 0.18014399707317352\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.55 +/- 14.0 %\n",
            "Batch: 242   Training loss: 0.19388122856616974\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.45 +/- 11.6 %\n",
            "Batch: 243   Training loss: 0.17363396286964417\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.38 +/- 12.4 %\n",
            "Batch: 244   Training loss: 0.17652645707130432\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.99 +/- 15.6 %\n",
            "Batch: 245   Training loss: 0.16753925383090973\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.85 +/- 15.1 %\n",
            "Batch: 246   Training loss: 0.16047443449497223\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.38 +/- 14.6 %\n",
            "Batch: 247   Training loss: 0.16769546270370483\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 21.12 +/- 16.5 %\n",
            "Batch: 248   Training loss: 0.1811470240354538\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.88 +/- 14.2 %\n",
            "[9,   250] loss: 0.182\n",
            "Batch: 249   Training loss: 0.1823451966047287\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.17 +/- 13.6 %\n",
            "Batch: 250   Training loss: 0.1682063341140747\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.68 +/- 12.7 %\n",
            "Batch: 251   Training loss: 0.15718138217926025\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 19.45 +/- 15.4 %\n",
            "Batch: 252   Training loss: 0.1700572669506073\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.83 +/- 14.3 %\n",
            "Batch: 253   Training loss: 0.1661197394132614\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.16 +/- 14.0 %\n",
            "Batch: 254   Training loss: 0.1671927571296692\n",
            "Precision: 81.48 +/- 38.8 %\n",
            "Recall: 20.98 +/- 13.8 %\n",
            "Batch: 255   Training loss: 0.1785126030445099\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.56 +/- 12.9 %\n",
            "Batch: 256   Training loss: 0.18246598541736603\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.31 +/- 15.3 %\n",
            "Batch: 257   Training loss: 0.20188018679618835\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 14.23 +/- 9.9 %\n",
            "Batch: 258   Training loss: 0.15954715013504028\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.40 +/- 15.2 %\n",
            "Batch: 259   Training loss: 0.19823378324508667\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.99 +/- 13.5 %\n",
            "Batch: 260   Training loss: 0.17407715320587158\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.18 +/- 15.0 %\n",
            "Batch: 261   Training loss: 0.19505514204502106\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.38 +/- 11.7 %\n",
            "Batch: 262   Training loss: 0.18681496381759644\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.15 +/- 9.5 %\n",
            "Batch: 263   Training loss: 0.19868707656860352\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.10 +/- 12.2 %\n",
            "Batch: 264   Training loss: 0.17982307076454163\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.30 +/- 16.3 %\n",
            "Batch: 265   Training loss: 0.17705534398555756\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.29 +/- 12.4 %\n",
            "Batch: 266   Training loss: 0.18760539591312408\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.93 +/- 12.0 %\n",
            "Batch: 267   Training loss: 0.18050126731395721\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.11 +/- 13.9 %\n",
            "Batch: 268   Training loss: 0.17400753498077393\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.40 +/- 15.6 %\n",
            "Batch: 269   Training loss: 0.17009250819683075\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.31 +/- 14.3 %\n",
            "Batch: 270   Training loss: 0.18724839389324188\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.38 +/- 15.4 %\n",
            "Batch: 271   Training loss: 0.18217524886131287\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.48 +/- 13.6 %\n",
            "Batch: 272   Training loss: 0.1799091249704361\n",
            "Precision: 35.71 +/- 47.9 %\n",
            "Recall: 7.89 +/- 11.6 %\n",
            "Batch: 273   Training loss: 0.1920488178730011\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 14.53 +/- 9.5 %\n",
            "Batch: 274   Training loss: 0.18615417182445526\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.57 +/- 12.2 %\n",
            "Batch: 275   Training loss: 0.174093097448349\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.12 +/- 13.7 %\n",
            "Batch: 276   Training loss: 0.18134896457195282\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.86 +/- 12.6 %\n",
            "Batch: 277   Training loss: 0.20255720615386963\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.76 +/- 13.1 %\n",
            "Batch: 278   Training loss: 0.1599152535200119\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.53 +/- 15.6 %\n",
            "Batch: 279   Training loss: 0.17125433683395386\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 20.83 +/- 16.7 %\n",
            "Batch: 280   Training loss: 0.17214617133140564\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.17 +/- 12.6 %\n",
            "Batch: 281   Training loss: 0.17788046598434448\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.72 +/- 15.0 %\n",
            "Batch: 282   Training loss: 0.18988487124443054\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 6.87 +/- 9.0 %\n",
            "Batch: 283   Training loss: 0.18046459555625916\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.21 +/- 16.9 %\n",
            "Batch: 284   Training loss: 0.17827710509300232\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.23 +/- 14.8 %\n",
            "Batch: 285   Training loss: 0.17756019532680511\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.13 +/- 12.9 %\n",
            "Batch: 286   Training loss: 0.1848466992378235\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.80 +/- 16.0 %\n",
            "Batch: 287   Training loss: 0.17565281689167023\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.03 +/- 14.2 %\n",
            "Batch: 288   Training loss: 0.18249571323394775\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.26 +/- 15.0 %\n",
            "Batch: 289   Training loss: 0.15903688967227936\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 14.19 +/- 16.2 %\n",
            "Batch: 290   Training loss: 0.17351658642292023\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.52 +/- 15.2 %\n",
            "Batch: 291   Training loss: 0.17447605729103088\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.40 +/- 13.5 %\n",
            "Batch: 292   Training loss: 0.18318691849708557\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.49 +/- 12.1 %\n",
            "Batch: 293   Training loss: 0.16056156158447266\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.14 +/- 15.2 %\n",
            "Batch: 294   Training loss: 0.20041903853416443\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 13.75 +/- 9.5 %\n",
            "Batch: 295   Training loss: 0.1733863651752472\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.19 +/- 14.9 %\n",
            "Batch: 296   Training loss: 0.17145447432994843\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.29 +/- 11.2 %\n",
            "Batch: 297   Training loss: 0.1769682615995407\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.07 +/- 13.4 %\n",
            "Batch: 298   Training loss: 0.16469162702560425\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.35 +/- 13.6 %\n",
            "[9,   300] loss: 0.175\n",
            "Batch: 299   Training loss: 0.17501983046531677\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.11 +/- 15.6 %\n",
            "Batch: 300   Training loss: 0.18322688341140747\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.42 +/- 15.0 %\n",
            "Batch: 301   Training loss: 0.17878961563110352\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.19 +/- 13.4 %\n",
            "Batch: 302   Training loss: 0.18802331387996674\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.47 +/- 13.2 %\n",
            "Batch: 303   Training loss: 0.1943291425704956\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 9.89 +/- 10.9 %\n",
            "Batch: 304   Training loss: 0.16710765659809113\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.61 +/- 14.7 %\n",
            "Batch: 305   Training loss: 0.1843869984149933\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.16 +/- 13.7 %\n",
            "Batch: 306   Training loss: 0.18130283057689667\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.36 +/- 13.6 %\n",
            "Batch: 307   Training loss: 0.1583022177219391\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 19.46 +/- 16.5 %\n",
            "Batch: 308   Training loss: 0.18056447803974152\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.66 +/- 11.5 %\n",
            "Batch: 309   Training loss: 0.18081270158290863\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.78 +/- 13.5 %\n",
            "Batch: 310   Training loss: 0.1666502058506012\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.80 +/- 14.6 %\n",
            "Batch: 311   Training loss: 0.1952916383743286\n",
            "Precision: 70.37 +/- 45.7 %\n",
            "Recall: 14.11 +/- 10.3 %\n",
            "Batch: 312   Training loss: 0.188873291015625\n",
            "Precision: 80.00 +/- 40.0 %\n",
            "Recall: 18.48 +/- 12.2 %\n",
            "Epoch:  9\n",
            "Batch: 0   Training loss: 0.18216168880462646\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 17.34 +/- 10.4 %\n",
            "Batch: 1   Training loss: 0.19779421389102936\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.16 +/- 14.4 %\n",
            "Batch: 2   Training loss: 0.18632784485816956\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.21 +/- 12.6 %\n",
            "Batch: 3   Training loss: 0.17063146829605103\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.52 +/- 12.0 %\n",
            "Batch: 4   Training loss: 0.1699950248003006\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.64 +/- 14.3 %\n",
            "Batch: 5   Training loss: 0.19194664061069489\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.85 +/- 13.8 %\n",
            "Batch: 6   Training loss: 0.17362074553966522\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.07 +/- 13.3 %\n",
            "Batch: 7   Training loss: 0.17270129919052124\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.22 +/- 11.7 %\n",
            "Batch: 8   Training loss: 0.18060347437858582\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.65 +/- 13.9 %\n",
            "Batch: 9   Training loss: 0.17629767954349518\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.43 +/- 13.3 %\n",
            "Batch: 10   Training loss: 0.18142038583755493\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.64 +/- 14.0 %\n",
            "Batch: 11   Training loss: 0.1808851808309555\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.12 +/- 13.6 %\n",
            "Batch: 12   Training loss: 0.1740189641714096\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.08 +/- 15.3 %\n",
            "Batch: 13   Training loss: 0.19541098177433014\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.24 +/- 15.0 %\n",
            "Batch: 14   Training loss: 0.16618980467319489\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 20.75 +/- 14.8 %\n",
            "Batch: 15   Training loss: 0.18082138895988464\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.31 +/- 13.5 %\n",
            "Batch: 16   Training loss: 0.1763608455657959\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.08 +/- 12.8 %\n",
            "Batch: 17   Training loss: 0.17829972505569458\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.29 +/- 14.0 %\n",
            "Batch: 18   Training loss: 0.18278278410434723\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.30 +/- 14.7 %\n",
            "Batch: 19   Training loss: 0.1907762587070465\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.14 +/- 13.1 %\n",
            "Batch: 20   Training loss: 0.20488475263118744\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.07 +/- 10.7 %\n",
            "Batch: 21   Training loss: 0.1729133576154709\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.64 +/- 13.3 %\n",
            "Batch: 22   Training loss: 0.1753213107585907\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 8.33 +/- 10.3 %\n",
            "Batch: 23   Training loss: 0.16434210538864136\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.20 +/- 15.1 %\n",
            "Batch: 24   Training loss: 0.17880189418792725\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 17.63 +/- 11.3 %\n",
            "Batch: 25   Training loss: 0.18934521079063416\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.71 +/- 11.1 %\n",
            "Batch: 26   Training loss: 0.18688221275806427\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.86 +/- 15.7 %\n",
            "Batch: 27   Training loss: 0.16243724524974823\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.04 +/- 14.5 %\n",
            "Batch: 28   Training loss: 0.17595385015010834\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.91 +/- 15.4 %\n",
            "Batch: 29   Training loss: 0.1742759793996811\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.83 +/- 13.9 %\n",
            "Batch: 30   Training loss: 0.18139410018920898\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.77 +/- 12.3 %\n",
            "Batch: 31   Training loss: 0.17732024192810059\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.07 +/- 12.8 %\n",
            "Batch: 32   Training loss: 0.18124035000801086\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.82 +/- 10.4 %\n",
            "Batch: 33   Training loss: 0.17253996431827545\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.24 +/- 12.2 %\n",
            "Batch: 34   Training loss: 0.1960802525281906\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.88 +/- 12.3 %\n",
            "Batch: 35   Training loss: 0.17702986299991608\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.09 +/- 12.1 %\n",
            "Batch: 36   Training loss: 0.18801696598529816\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.93 +/- 14.4 %\n",
            "Batch: 37   Training loss: 0.17828676104545593\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.69 +/- 15.1 %\n",
            "Batch: 38   Training loss: 0.1816970854997635\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.22 +/- 15.3 %\n",
            "Batch: 39   Training loss: 0.16804417967796326\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.29 +/- 14.5 %\n",
            "Batch: 40   Training loss: 0.16536442935466766\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.07 +/- 16.4 %\n",
            "Batch: 41   Training loss: 0.17903536558151245\n",
            "Precision: 62.96 +/- 48.3 %\n",
            "Recall: 13.95 +/- 13.1 %\n",
            "Batch: 42   Training loss: 0.17743000388145447\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 14.24 +/- 16.9 %\n",
            "Batch: 43   Training loss: 0.17587296664714813\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.16 +/- 13.5 %\n",
            "Batch: 44   Training loss: 0.17831028997898102\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.98 +/- 14.9 %\n",
            "Batch: 45   Training loss: 0.18872416019439697\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 10.89 +/- 12.0 %\n",
            "Batch: 46   Training loss: 0.16812369227409363\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.75 +/- 13.4 %\n",
            "Batch: 47   Training loss: 0.16378948092460632\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.55 +/- 15.2 %\n",
            "Batch: 48   Training loss: 0.19070088863372803\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.83 +/- 13.9 %\n",
            "[10,    50] loss: 0.177\n",
            "Batch: 49   Training loss: 0.17689602077007294\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.69 +/- 14.1 %\n",
            "Batch: 50   Training loss: 0.18323218822479248\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.46 +/- 15.8 %\n",
            "Batch: 51   Training loss: 0.17621228098869324\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.31 +/- 16.7 %\n",
            "Batch: 52   Training loss: 0.1856146901845932\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.90 +/- 12.1 %\n",
            "Batch: 53   Training loss: 0.18504978716373444\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.50 +/- 13.7 %\n",
            "Batch: 54   Training loss: 0.1746082603931427\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.06 +/- 13.7 %\n",
            "Batch: 55   Training loss: 0.18885087966918945\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.40 +/- 12.7 %\n",
            "Batch: 56   Training loss: 0.18290437757968903\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.04 +/- 11.9 %\n",
            "Batch: 57   Training loss: 0.17255671322345734\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.60 +/- 13.4 %\n",
            "Batch: 58   Training loss: 0.16533192992210388\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.62 +/- 12.6 %\n",
            "Batch: 59   Training loss: 0.1688500940799713\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.96 +/- 15.6 %\n",
            "Batch: 60   Training loss: 0.18084728717803955\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.94 +/- 14.2 %\n",
            "Batch: 61   Training loss: 0.18460308015346527\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.24 +/- 12.0 %\n",
            "Batch: 62   Training loss: 0.18825574219226837\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.25 +/- 11.1 %\n",
            "Batch: 63   Training loss: 0.17093481123447418\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.87 +/- 14.4 %\n",
            "Batch: 64   Training loss: 0.17709752917289734\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.34 +/- 15.7 %\n",
            "Batch: 65   Training loss: 0.1862400770187378\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.96 +/- 12.8 %\n",
            "Batch: 66   Training loss: 0.16255627572536469\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 17.41 +/- 16.5 %\n",
            "Batch: 67   Training loss: 0.16880345344543457\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.28 +/- 15.4 %\n",
            "Batch: 68   Training loss: 0.16811080276966095\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.16 +/- 13.2 %\n",
            "Batch: 69   Training loss: 0.17539472877979279\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.02 +/- 12.1 %\n",
            "Batch: 70   Training loss: 0.17945073544979095\n",
            "Precision: 35.71 +/- 47.9 %\n",
            "Recall: 8.65 +/- 13.6 %\n",
            "Batch: 71   Training loss: 0.17538964748382568\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.27 +/- 13.7 %\n",
            "Batch: 72   Training loss: 0.1758982539176941\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.22 +/- 11.4 %\n",
            "Batch: 73   Training loss: 0.17271928489208221\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.43 +/- 15.2 %\n",
            "Batch: 74   Training loss: 0.18864667415618896\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.24 +/- 13.6 %\n",
            "Batch: 75   Training loss: 0.17502588033676147\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.24 +/- 16.4 %\n",
            "Batch: 76   Training loss: 0.18266688287258148\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.35 +/- 13.0 %\n",
            "Batch: 77   Training loss: 0.18706083297729492\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.47 +/- 15.6 %\n",
            "Batch: 78   Training loss: 0.16071189939975739\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.81 +/- 15.0 %\n",
            "Batch: 79   Training loss: 0.18235260248184204\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.86 +/- 13.3 %\n",
            "Batch: 80   Training loss: 0.1836092174053192\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.48 +/- 13.1 %\n",
            "Batch: 81   Training loss: 0.18779510259628296\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.25 +/- 11.5 %\n",
            "Batch: 82   Training loss: 0.19925515353679657\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 15.50 +/- 11.3 %\n",
            "Batch: 83   Training loss: 0.1756058782339096\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.11 +/- 11.2 %\n",
            "Batch: 84   Training loss: 0.17858260869979858\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 14.70 +/- 10.7 %\n",
            "Batch: 85   Training loss: 0.19289536774158478\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.40 +/- 14.4 %\n",
            "Batch: 86   Training loss: 0.18874140083789825\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.18 +/- 15.0 %\n",
            "Batch: 87   Training loss: 0.1842462420463562\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.98 +/- 13.7 %\n",
            "Batch: 88   Training loss: 0.19351473450660706\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.70 +/- 13.2 %\n",
            "Batch: 89   Training loss: 0.18304558098316193\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.77 +/- 15.1 %\n",
            "Batch: 90   Training loss: 0.17028090357780457\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.82 +/- 11.0 %\n",
            "Batch: 91   Training loss: 0.18558238446712494\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.49 +/- 11.9 %\n",
            "Batch: 92   Training loss: 0.17968472838401794\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.35 +/- 14.4 %\n",
            "Batch: 93   Training loss: 0.1871224194765091\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.91 +/- 14.0 %\n",
            "Batch: 94   Training loss: 0.1826893389225006\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.43 +/- 11.9 %\n",
            "Batch: 95   Training loss: 0.19000405073165894\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.09 +/- 14.3 %\n",
            "Batch: 96   Training loss: 0.16980332136154175\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.28 +/- 12.1 %\n",
            "Batch: 97   Training loss: 0.18711240589618683\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.14 +/- 12.6 %\n",
            "Batch: 98   Training loss: 0.17985591292381287\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.16 +/- 14.0 %\n",
            "[10,   100] loss: 0.176\n",
            "Batch: 99   Training loss: 0.17559093236923218\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.13 +/- 16.4 %\n",
            "Batch: 100   Training loss: 0.18523792922496796\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.72 +/- 15.9 %\n",
            "Batch: 101   Training loss: 0.17457114160060883\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.80 +/- 13.5 %\n",
            "Batch: 102   Training loss: 0.15885838866233826\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 13.00 +/- 15.0 %\n",
            "Batch: 103   Training loss: 0.1779192090034485\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.08 +/- 14.1 %\n",
            "Batch: 104   Training loss: 0.17814691364765167\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.77 +/- 13.9 %\n",
            "Batch: 105   Training loss: 0.1629008799791336\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.77 +/- 14.0 %\n",
            "Batch: 106   Training loss: 0.18009379506111145\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.66 +/- 15.3 %\n",
            "Batch: 107   Training loss: 0.17324580252170563\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.59 +/- 12.6 %\n",
            "Batch: 108   Training loss: 0.17144232988357544\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.94 +/- 14.7 %\n",
            "Batch: 109   Training loss: 0.18613600730895996\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.36 +/- 13.5 %\n",
            "Batch: 110   Training loss: 0.17802688479423523\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.55 +/- 12.4 %\n",
            "Batch: 111   Training loss: 0.18610037863254547\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.28 +/- 13.0 %\n",
            "Batch: 112   Training loss: 0.17008203268051147\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.77 +/- 15.4 %\n",
            "Batch: 113   Training loss: 0.1834903508424759\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.94 +/- 14.0 %\n",
            "Batch: 114   Training loss: 0.17723672091960907\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.40 +/- 12.4 %\n",
            "Batch: 115   Training loss: 0.17203640937805176\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.72 +/- 16.7 %\n",
            "Batch: 116   Training loss: 0.18425488471984863\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.18 +/- 13.5 %\n",
            "Batch: 117   Training loss: 0.19056376814842224\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.34 +/- 11.0 %\n",
            "Batch: 118   Training loss: 0.16360647976398468\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 19.72 +/- 12.6 %\n",
            "Batch: 119   Training loss: 0.17643135786056519\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.89 +/- 13.4 %\n",
            "Batch: 120   Training loss: 0.17628207802772522\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.11 +/- 15.3 %\n",
            "Batch: 121   Training loss: 0.16888664662837982\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.00 +/- 15.5 %\n",
            "Batch: 122   Training loss: 0.1672080159187317\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.87 +/- 15.4 %\n",
            "Batch: 123   Training loss: 0.18893276154994965\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.34 +/- 11.2 %\n",
            "Batch: 124   Training loss: 0.18410131335258484\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.18 +/- 12.3 %\n",
            "Batch: 125   Training loss: 0.1619235873222351\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.56 +/- 13.8 %\n",
            "Batch: 126   Training loss: 0.19056002795696259\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.70 +/- 13.8 %\n",
            "Batch: 127   Training loss: 0.18155904114246368\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.55 +/- 12.2 %\n",
            "Batch: 128   Training loss: 0.18179672956466675\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 8.89 +/- 10.6 %\n",
            "Batch: 129   Training loss: 0.17088685929775238\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.82 +/- 14.1 %\n",
            "Batch: 130   Training loss: 0.15672197937965393\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 17.41 +/- 16.5 %\n",
            "Batch: 131   Training loss: 0.17720919847488403\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.20 +/- 12.0 %\n",
            "Batch: 132   Training loss: 0.18004679679870605\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.47 +/- 14.1 %\n",
            "Batch: 133   Training loss: 0.18010248243808746\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 11.77 +/- 16.0 %\n",
            "Batch: 134   Training loss: 0.1827298104763031\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.48 +/- 12.7 %\n",
            "Batch: 135   Training loss: 0.17503710091114044\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 19.59 +/- 13.6 %\n",
            "Batch: 136   Training loss: 0.19174349308013916\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.21 +/- 12.1 %\n",
            "Batch: 137   Training loss: 0.1836816668510437\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.07 +/- 13.7 %\n",
            "Batch: 138   Training loss: 0.16867688298225403\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.41 +/- 15.5 %\n",
            "Batch: 139   Training loss: 0.16509026288986206\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.92 +/- 16.6 %\n",
            "Batch: 140   Training loss: 0.191367045044899\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.10 +/- 12.2 %\n",
            "Batch: 141   Training loss: 0.16909676790237427\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.61 +/- 13.1 %\n",
            "Batch: 142   Training loss: 0.16890475153923035\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.85 +/- 13.9 %\n",
            "Batch: 143   Training loss: 0.19185799360275269\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.48 +/- 13.4 %\n",
            "Batch: 144   Training loss: 0.19796521961688995\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 9.49 +/- 10.6 %\n",
            "Batch: 145   Training loss: 0.16602449119091034\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.39 +/- 13.1 %\n",
            "Batch: 146   Training loss: 0.17734254896640778\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.12 +/- 14.6 %\n",
            "Batch: 147   Training loss: 0.17385968565940857\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.78 +/- 10.3 %\n",
            "Batch: 148   Training loss: 0.1697080135345459\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.00 +/- 15.6 %\n",
            "[10,   150] loss: 0.175\n",
            "Batch: 149   Training loss: 0.17476403713226318\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.83 +/- 11.9 %\n",
            "Batch: 150   Training loss: 0.17669755220413208\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.16 +/- 12.8 %\n",
            "Batch: 151   Training loss: 0.18366186320781708\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.36 +/- 15.1 %\n",
            "Batch: 152   Training loss: 0.17796283960342407\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.62 +/- 15.7 %\n",
            "Batch: 153   Training loss: 0.1753925383090973\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.16 +/- 14.5 %\n",
            "Batch: 154   Training loss: 0.1857571303844452\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.17 +/- 12.8 %\n",
            "Batch: 155   Training loss: 0.1769753247499466\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.79 +/- 14.0 %\n",
            "Batch: 156   Training loss: 0.16911742091178894\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 15.62 +/- 17.4 %\n",
            "Batch: 157   Training loss: 0.19867251813411713\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.77 +/- 14.1 %\n",
            "Batch: 158   Training loss: 0.16982430219650269\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.01 +/- 15.4 %\n",
            "Batch: 159   Training loss: 0.19205358624458313\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.35 +/- 11.7 %\n",
            "Batch: 160   Training loss: 0.17786256968975067\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.63 +/- 15.2 %\n",
            "Batch: 161   Training loss: 0.17563451826572418\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 11.04 +/- 14.8 %\n",
            "Batch: 162   Training loss: 0.18074457347393036\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.63 +/- 14.6 %\n",
            "Batch: 163   Training loss: 0.18676859140396118\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.45 +/- 14.6 %\n",
            "Batch: 164   Training loss: 0.16732099652290344\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.86 +/- 16.4 %\n",
            "Batch: 165   Training loss: 0.18884706497192383\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.65 +/- 12.3 %\n",
            "Batch: 166   Training loss: 0.1750541627407074\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.20 +/- 14.3 %\n",
            "Batch: 167   Training loss: 0.1733514368534088\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.83 +/- 14.1 %\n",
            "Batch: 168   Training loss: 0.18024344742298126\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.47 +/- 13.1 %\n",
            "Batch: 169   Training loss: 0.17972351610660553\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.27 +/- 11.2 %\n",
            "Batch: 170   Training loss: 0.16901607811450958\n",
            "Precision: 74.07 +/- 43.8 %\n",
            "Recall: 18.36 +/- 13.5 %\n",
            "Batch: 171   Training loss: 0.19215452671051025\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.53 +/- 14.0 %\n",
            "Batch: 172   Training loss: 0.19810526072978973\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.71 +/- 11.1 %\n",
            "Batch: 173   Training loss: 0.19669683277606964\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.20 +/- 10.7 %\n",
            "Batch: 174   Training loss: 0.1600986123085022\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.67 +/- 15.1 %\n",
            "Batch: 175   Training loss: 0.1856819987297058\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.84 +/- 10.6 %\n",
            "Batch: 176   Training loss: 0.18379893898963928\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.37 +/- 14.5 %\n",
            "Batch: 177   Training loss: 0.20147603750228882\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.83 +/- 12.9 %\n",
            "Batch: 178   Training loss: 0.16225311160087585\n",
            "Precision: 39.29 +/- 48.8 %\n",
            "Recall: 8.58 +/- 11.8 %\n",
            "Batch: 179   Training loss: 0.1831081062555313\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 20.00 +/- 15.8 %\n",
            "Batch: 180   Training loss: 0.15953420102596283\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.86 +/- 15.8 %\n",
            "Batch: 181   Training loss: 0.18491828441619873\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 14.72 +/- 10.8 %\n",
            "Batch: 182   Training loss: 0.17744754254817963\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.44 +/- 14.2 %\n",
            "Batch: 183   Training loss: 0.18532131612300873\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.23 +/- 14.5 %\n",
            "Batch: 184   Training loss: 0.17979173362255096\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.00 +/- 16.1 %\n",
            "Batch: 185   Training loss: 0.19602112472057343\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.62 +/- 12.8 %\n",
            "Batch: 186   Training loss: 0.20994246006011963\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.45 +/- 12.8 %\n",
            "Batch: 187   Training loss: 0.15764495730400085\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.58 +/- 12.0 %\n",
            "Batch: 188   Training loss: 0.17749619483947754\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 14.60 +/- 17.5 %\n",
            "Batch: 189   Training loss: 0.16967494785785675\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.04 +/- 11.3 %\n",
            "Batch: 190   Training loss: 0.19503076374530792\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.26 +/- 12.0 %\n",
            "Batch: 191   Training loss: 0.1789034605026245\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 16.00 +/- 16.4 %\n",
            "Batch: 192   Training loss: 0.16305391490459442\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.28 +/- 13.9 %\n",
            "Batch: 193   Training loss: 0.18408691883087158\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.65 +/- 14.4 %\n",
            "Batch: 194   Training loss: 0.17710773646831512\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.92 +/- 13.2 %\n",
            "Batch: 195   Training loss: 0.18036417663097382\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.43 +/- 13.7 %\n",
            "Batch: 196   Training loss: 0.19131819903850555\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 9.37 +/- 10.7 %\n",
            "Batch: 197   Training loss: 0.21408219635486603\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.54 +/- 12.3 %\n",
            "Batch: 198   Training loss: 0.18557794392108917\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.22 +/- 15.1 %\n",
            "[10,   200] loss: 0.192\n",
            "Batch: 199   Training loss: 0.1921406239271164\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.71 +/- 14.8 %\n",
            "Batch: 200   Training loss: 0.19243483245372772\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.56 +/- 15.2 %\n",
            "Batch: 201   Training loss: 0.17034506797790527\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.38 +/- 14.6 %\n",
            "Batch: 202   Training loss: 0.1809341162443161\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.82 +/- 12.5 %\n",
            "Batch: 203   Training loss: 0.18040692806243896\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.28 +/- 13.2 %\n",
            "Batch: 204   Training loss: 0.16112087666988373\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.70 +/- 14.8 %\n",
            "Batch: 205   Training loss: 0.17618012428283691\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 20.31 +/- 13.8 %\n",
            "Batch: 206   Training loss: 0.17014506459236145\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.32 +/- 15.0 %\n",
            "Batch: 207   Training loss: 0.17655564844608307\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.52 +/- 14.4 %\n",
            "Batch: 208   Training loss: 0.16840246319770813\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 15.23 +/- 16.4 %\n",
            "Batch: 209   Training loss: 0.1681351661682129\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 20.26 +/- 15.8 %\n",
            "Batch: 210   Training loss: 0.1905728280544281\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.26 +/- 10.8 %\n",
            "Batch: 211   Training loss: 0.18694163858890533\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.19 +/- 15.3 %\n",
            "Batch: 212   Training loss: 0.1765725314617157\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.26 +/- 14.9 %\n",
            "Batch: 213   Training loss: 0.18329282104969025\n",
            "Precision: 55.56 +/- 49.7 %\n",
            "Recall: 11.57 +/- 11.7 %\n",
            "Batch: 214   Training loss: 0.17820703983306885\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.24 +/- 12.1 %\n",
            "Batch: 215   Training loss: 0.16380783915519714\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.46 +/- 14.0 %\n",
            "Batch: 216   Training loss: 0.18562662601470947\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.29 +/- 15.7 %\n",
            "Batch: 217   Training loss: 0.16291600465774536\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.46 +/- 11.4 %\n",
            "Batch: 218   Training loss: 0.16813872754573822\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.62 +/- 15.0 %\n",
            "Batch: 219   Training loss: 0.1793937385082245\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.28 +/- 12.2 %\n",
            "Batch: 220   Training loss: 0.18858754634857178\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 14.35 +/- 12.2 %\n",
            "Batch: 221   Training loss: 0.16588926315307617\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 19.54 +/- 14.7 %\n",
            "Batch: 222   Training loss: 0.17020735144615173\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.19 +/- 12.3 %\n",
            "Batch: 223   Training loss: 0.200779527425766\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.58 +/- 12.3 %\n",
            "Batch: 224   Training loss: 0.17671264708042145\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.33 +/- 14.5 %\n",
            "Batch: 225   Training loss: 0.17892780900001526\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.16 +/- 11.9 %\n",
            "Batch: 226   Training loss: 0.1668166220188141\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.16 +/- 13.8 %\n",
            "Batch: 227   Training loss: 0.17862728238105774\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.64 +/- 11.8 %\n",
            "Batch: 228   Training loss: 0.19513337314128876\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 12.70 +/- 11.0 %\n",
            "Batch: 229   Training loss: 0.18704675137996674\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.03 +/- 12.5 %\n",
            "Batch: 230   Training loss: 0.16651836037635803\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.98 +/- 10.8 %\n",
            "Batch: 231   Training loss: 0.15555550158023834\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.33 +/- 14.8 %\n",
            "Batch: 232   Training loss: 0.1783008575439453\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.69 +/- 12.6 %\n",
            "Batch: 233   Training loss: 0.18115763366222382\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.53 +/- 14.7 %\n",
            "Batch: 234   Training loss: 0.17365027964115143\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.60 +/- 15.8 %\n",
            "Batch: 235   Training loss: 0.17917220294475555\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.95 +/- 15.3 %\n",
            "Batch: 236   Training loss: 0.1857379823923111\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.90 +/- 15.3 %\n",
            "Batch: 237   Training loss: 0.17212419211864471\n",
            "Precision: 74.07 +/- 43.8 %\n",
            "Recall: 20.32 +/- 16.6 %\n",
            "Batch: 238   Training loss: 0.18673324584960938\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.68 +/- 11.3 %\n",
            "Batch: 239   Training loss: 0.1886911690235138\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.85 +/- 14.1 %\n",
            "Batch: 240   Training loss: 0.1660574972629547\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.56 +/- 12.3 %\n",
            "Batch: 241   Training loss: 0.18049803376197815\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.29 +/- 12.8 %\n",
            "Batch: 242   Training loss: 0.1907949149608612\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.43 +/- 12.5 %\n",
            "Batch: 243   Training loss: 0.18337836861610413\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.74 +/- 11.8 %\n",
            "Batch: 244   Training loss: 0.17379532754421234\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.27 +/- 11.2 %\n",
            "Batch: 245   Training loss: 0.1793367564678192\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.12 +/- 13.1 %\n",
            "Batch: 246   Training loss: 0.18673266470432281\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 18.98 +/- 13.6 %\n",
            "Batch: 247   Training loss: 0.1582135111093521\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 22.08 +/- 16.6 %\n",
            "Batch: 248   Training loss: 0.16699694097042084\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.61 +/- 16.5 %\n",
            "[10,   250] loss: 0.176\n",
            "Batch: 249   Training loss: 0.17564716935157776\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.30 +/- 13.7 %\n",
            "Batch: 250   Training loss: 0.1640641987323761\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.26 +/- 13.9 %\n",
            "Batch: 251   Training loss: 0.18396280705928802\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.62 +/- 11.9 %\n",
            "Batch: 252   Training loss: 0.172977477312088\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.34 +/- 15.5 %\n",
            "Batch: 253   Training loss: 0.16066651046276093\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.96 +/- 15.1 %\n",
            "Batch: 254   Training loss: 0.1771596223115921\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.04 +/- 13.1 %\n",
            "Batch: 255   Training loss: 0.18515585362911224\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.24 +/- 12.7 %\n",
            "Batch: 256   Training loss: 0.17020700871944427\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.36 +/- 15.0 %\n",
            "Batch: 257   Training loss: 0.2003515511751175\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.30 +/- 15.0 %\n",
            "Batch: 258   Training loss: 0.1745932400226593\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.37 +/- 15.8 %\n",
            "Batch: 259   Training loss: 0.19457827508449554\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.79 +/- 13.4 %\n",
            "Batch: 260   Training loss: 0.17827080190181732\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.20 +/- 13.3 %\n",
            "Batch: 261   Training loss: 0.17877240478992462\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.29 +/- 13.2 %\n",
            "Batch: 262   Training loss: 0.19699601829051971\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.17 +/- 11.1 %\n",
            "Batch: 263   Training loss: 0.17635931074619293\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.78 +/- 13.9 %\n",
            "Batch: 264   Training loss: 0.1715361624956131\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.28 +/- 12.7 %\n",
            "Batch: 265   Training loss: 0.19121843576431274\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.58 +/- 10.6 %\n",
            "Batch: 266   Training loss: 0.1987437605857849\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.65 +/- 12.6 %\n",
            "Batch: 267   Training loss: 0.18561358749866486\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.20 +/- 11.9 %\n",
            "Batch: 268   Training loss: 0.18886931240558624\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.52 +/- 13.2 %\n",
            "Batch: 269   Training loss: 0.19170919060707092\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 15.80 +/- 11.3 %\n",
            "Batch: 270   Training loss: 0.17944294214248657\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.09 +/- 13.5 %\n",
            "Batch: 271   Training loss: 0.18758434057235718\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.48 +/- 11.0 %\n",
            "Batch: 272   Training loss: 0.16835644841194153\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.79 +/- 15.8 %\n",
            "Batch: 273   Training loss: 0.18510371446609497\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.83 +/- 13.0 %\n",
            "Batch: 274   Training loss: 0.18738360702991486\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.39 +/- 10.9 %\n",
            "Batch: 275   Training loss: 0.17995689809322357\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.74 +/- 14.5 %\n",
            "Batch: 276   Training loss: 0.15853282809257507\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.84 +/- 14.3 %\n",
            "Batch: 277   Training loss: 0.18449901044368744\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.05 +/- 14.9 %\n",
            "Batch: 278   Training loss: 0.183738112449646\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.41 +/- 12.2 %\n",
            "Batch: 279   Training loss: 0.1633930504322052\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.79 +/- 13.7 %\n",
            "Batch: 280   Training loss: 0.18073111772537231\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.74 +/- 14.2 %\n",
            "Batch: 281   Training loss: 0.19594842195510864\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 8.50 +/- 10.3 %\n",
            "Batch: 282   Training loss: 0.1706845760345459\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.41 +/- 14.3 %\n",
            "Batch: 283   Training loss: 0.1909625083208084\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.36 +/- 12.8 %\n",
            "Batch: 284   Training loss: 0.17059196531772614\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.25 +/- 15.4 %\n",
            "Batch: 285   Training loss: 0.16969434916973114\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.37 +/- 15.3 %\n",
            "Batch: 286   Training loss: 0.17821383476257324\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.19 +/- 14.1 %\n",
            "Batch: 287   Training loss: 0.18676075339317322\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.47 +/- 14.2 %\n",
            "Batch: 288   Training loss: 0.19265897572040558\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.95 +/- 12.0 %\n",
            "Batch: 289   Training loss: 0.1736975908279419\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.61 +/- 13.6 %\n",
            "Batch: 290   Training loss: 0.18435166776180267\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.31 +/- 11.3 %\n",
            "Batch: 291   Training loss: 0.18930105865001678\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.68 +/- 10.4 %\n",
            "Batch: 292   Training loss: 0.18051545321941376\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.33 +/- 14.9 %\n",
            "Batch: 293   Training loss: 0.18114084005355835\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 15.98 +/- 10.9 %\n",
            "Batch: 294   Training loss: 0.1802872270345688\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.55 +/- 11.1 %\n",
            "Batch: 295   Training loss: 0.17772212624549866\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.07 +/- 11.1 %\n",
            "Batch: 296   Training loss: 0.18265385925769806\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.33 +/- 11.6 %\n",
            "Batch: 297   Training loss: 0.16020213067531586\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 16.19 +/- 15.9 %\n",
            "Batch: 298   Training loss: 0.1775445193052292\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.77 +/- 14.2 %\n",
            "[10,   300] loss: 0.190\n",
            "Batch: 299   Training loss: 0.19003839790821075\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.69 +/- 13.3 %\n",
            "Batch: 300   Training loss: 0.1896488517522812\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.22 +/- 12.7 %\n",
            "Batch: 301   Training loss: 0.18883320689201355\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.64 +/- 14.2 %\n",
            "Batch: 302   Training loss: 0.17638790607452393\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.28 +/- 12.5 %\n",
            "Batch: 303   Training loss: 0.18624337017536163\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 15.77 +/- 12.3 %\n",
            "Batch: 304   Training loss: 0.18442712724208832\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.46 +/- 14.6 %\n",
            "Batch: 305   Training loss: 0.16885289549827576\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 12.59 +/- 16.9 %\n",
            "Batch: 306   Training loss: 0.17478890717029572\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.22 +/- 12.4 %\n",
            "Batch: 307   Training loss: 0.17341063916683197\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 20.26 +/- 16.6 %\n",
            "Batch: 308   Training loss: 0.1673407405614853\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.88 +/- 16.0 %\n",
            "Batch: 309   Training loss: 0.17527982592582703\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.88 +/- 14.4 %\n",
            "Batch: 310   Training loss: 0.19370949268341064\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.30 +/- 12.6 %\n",
            "Batch: 311   Training loss: 0.15941891074180603\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.45 +/- 14.5 %\n",
            "Batch: 312   Training loss: 0.17863747477531433\n",
            "Precision: 53.33 +/- 49.9 %\n",
            "Recall: 13.06 +/- 14.1 %\n",
            "Accuracy: \n",
            "0.142 +/- 0.07\n",
            "\n",
            "Fold:  2\n",
            "Epoch:  0\n",
            "Batch: 0   Training loss: 0.7135718464851379\n",
            "Precision: 7.36 +/- 4.1 %\n",
            "Recall: 57.45 +/- 18.7 %\n",
            "Batch: 1   Training loss: 0.6984517574310303\n",
            "Precision: 8.96 +/- 4.2 %\n",
            "Recall: 57.39 +/- 13.8 %\n",
            "Batch: 2   Training loss: 0.6852623224258423\n",
            "Precision: 8.93 +/- 4.1 %\n",
            "Recall: 58.49 +/- 14.8 %\n",
            "Batch: 3   Training loss: 0.6740989685058594\n",
            "Precision: 9.45 +/- 3.8 %\n",
            "Recall: 53.53 +/- 13.7 %\n",
            "Batch: 4   Training loss: 0.6636962294578552\n",
            "Precision: 10.09 +/- 5.7 %\n",
            "Recall: 49.27 +/- 16.9 %\n",
            "Batch: 5   Training loss: 0.6542717218399048\n",
            "Precision: 12.97 +/- 5.6 %\n",
            "Recall: 59.02 +/- 17.7 %\n",
            "Batch: 6   Training loss: 0.6448943018913269\n",
            "Precision: 12.87 +/- 6.7 %\n",
            "Recall: 56.07 +/- 18.1 %\n",
            "Batch: 7   Training loss: 0.6345580220222473\n",
            "Precision: 11.90 +/- 6.3 %\n",
            "Recall: 58.24 +/- 18.5 %\n",
            "Batch: 8   Training loss: 0.6252024173736572\n",
            "Precision: 12.12 +/- 5.6 %\n",
            "Recall: 59.15 +/- 16.9 %\n",
            "Batch: 9   Training loss: 0.6195390820503235\n",
            "Precision: 11.41 +/- 5.1 %\n",
            "Recall: 45.41 +/- 16.8 %\n",
            "Batch: 10   Training loss: 0.6074647903442383\n",
            "Precision: 11.55 +/- 5.9 %\n",
            "Recall: 49.70 +/- 15.5 %\n",
            "Batch: 11   Training loss: 0.6019203662872314\n",
            "Precision: 12.33 +/- 7.0 %\n",
            "Recall: 42.04 +/- 17.3 %\n",
            "Batch: 12   Training loss: 0.5928105115890503\n",
            "Precision: 10.42 +/- 6.6 %\n",
            "Recall: 26.77 +/- 16.3 %\n",
            "Batch: 13   Training loss: 0.5800092220306396\n",
            "Precision: 9.94 +/- 5.9 %\n",
            "Recall: 29.91 +/- 16.8 %\n",
            "Batch: 14   Training loss: 0.572655975818634\n",
            "Precision: 11.20 +/- 6.8 %\n",
            "Recall: 27.19 +/- 17.0 %\n",
            "Batch: 15   Training loss: 0.5618488192558289\n",
            "Precision: 10.00 +/- 6.9 %\n",
            "Recall: 27.98 +/- 20.4 %\n",
            "Batch: 16   Training loss: 0.5515848398208618\n",
            "Precision: 9.77 +/- 6.7 %\n",
            "Recall: 21.53 +/- 13.7 %\n",
            "Batch: 17   Training loss: 0.5433931946754456\n",
            "Precision: 13.03 +/- 8.5 %\n",
            "Recall: 24.17 +/- 16.2 %\n",
            "Batch: 18   Training loss: 0.5365558862686157\n",
            "Precision: 10.46 +/- 8.4 %\n",
            "Recall: 17.97 +/- 17.5 %\n",
            "Batch: 19   Training loss: 0.5265136957168579\n",
            "Precision: 8.79 +/- 8.7 %\n",
            "Recall: 14.52 +/- 16.1 %\n",
            "Batch: 20   Training loss: 0.5204649567604065\n",
            "Precision: 14.88 +/- 9.8 %\n",
            "Recall: 20.51 +/- 15.8 %\n",
            "Batch: 21   Training loss: 0.5154726505279541\n",
            "Precision: 15.26 +/- 10.3 %\n",
            "Recall: 16.53 +/- 12.5 %\n",
            "Batch: 22   Training loss: 0.500948965549469\n",
            "Precision: 13.49 +/- 13.4 %\n",
            "Recall: 14.63 +/- 16.9 %\n",
            "Batch: 23   Training loss: 0.48734959959983826\n",
            "Precision: 15.48 +/- 13.1 %\n",
            "Recall: 22.12 +/- 21.3 %\n",
            "Batch: 24   Training loss: 0.48492586612701416\n",
            "Precision: 17.41 +/- 14.3 %\n",
            "Recall: 15.44 +/- 12.8 %\n",
            "Batch: 25   Training loss: 0.4740181863307953\n",
            "Precision: 26.19 +/- 13.7 %\n",
            "Recall: 20.89 +/- 14.6 %\n",
            "Batch: 26   Training loss: 0.4650256037712097\n",
            "Precision: 23.81 +/- 18.1 %\n",
            "Recall: 17.27 +/- 13.9 %\n",
            "Batch: 27   Training loss: 0.4625191390514374\n",
            "Precision: 22.62 +/- 20.0 %\n",
            "Recall: 14.47 +/- 13.6 %\n",
            "Batch: 28   Training loss: 0.4503304064273834\n",
            "Precision: 23.81 +/- 19.1 %\n",
            "Recall: 15.89 +/- 13.3 %\n",
            "Batch: 29   Training loss: 0.4396616220474243\n",
            "Precision: 21.43 +/- 17.7 %\n",
            "Recall: 19.71 +/- 20.7 %\n",
            "Batch: 30   Training loss: 0.44009867310523987\n",
            "Precision: 28.57 +/- 21.0 %\n",
            "Recall: 13.83 +/- 10.5 %\n",
            "Batch: 31   Training loss: 0.4262112081050873\n",
            "Precision: 27.14 +/- 17.9 %\n",
            "Recall: 21.34 +/- 21.2 %\n",
            "Batch: 32   Training loss: 0.4217170774936676\n",
            "Precision: 40.48 +/- 32.6 %\n",
            "Recall: 14.29 +/- 12.6 %\n",
            "Batch: 33   Training loss: 0.41763103008270264\n",
            "Precision: 50.00 +/- 28.9 %\n",
            "Recall: 16.27 +/- 11.6 %\n",
            "Batch: 34   Training loss: 0.40871697664260864\n",
            "Precision: 40.48 +/- 32.6 %\n",
            "Recall: 13.63 +/- 12.3 %\n",
            "Batch: 35   Training loss: 0.4012829661369324\n",
            "Precision: 42.86 +/- 31.9 %\n",
            "Recall: 15.91 +/- 14.8 %\n",
            "Batch: 36   Training loss: 0.39327189326286316\n",
            "Precision: 47.62 +/- 30.1 %\n",
            "Recall: 17.61 +/- 13.9 %\n",
            "Batch: 37   Training loss: 0.39029520750045776\n",
            "Precision: 40.48 +/- 32.6 %\n",
            "Recall: 13.23 +/- 12.8 %\n",
            "Batch: 38   Training loss: 0.3858310878276825\n",
            "Precision: 35.71 +/- 33.2 %\n",
            "Recall: 12.42 +/- 13.8 %\n",
            "Batch: 39   Training loss: 0.38085341453552246\n",
            "Precision: 40.48 +/- 32.6 %\n",
            "Recall: 12.39 +/- 11.7 %\n",
            "Batch: 40   Training loss: 0.3694174885749817\n",
            "Precision: 47.62 +/- 30.1 %\n",
            "Recall: 17.30 +/- 13.4 %\n",
            "Batch: 41   Training loss: 0.3664435148239136\n",
            "Precision: 47.62 +/- 30.1 %\n",
            "Recall: 16.02 +/- 13.8 %\n",
            "Batch: 42   Training loss: 0.3711215853691101\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.65 +/- 13.3 %\n",
            "Batch: 43   Training loss: 0.35437995195388794\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.92 +/- 13.3 %\n",
            "Batch: 44   Training loss: 0.35815292596817017\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.38 +/- 12.2 %\n",
            "Batch: 45   Training loss: 0.34371471405029297\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.23 +/- 13.7 %\n",
            "Batch: 46   Training loss: 0.34273841977119446\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.40 +/- 11.5 %\n",
            "Batch: 47   Training loss: 0.34594786167144775\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.59 +/- 11.1 %\n",
            "Batch: 48   Training loss: 0.3213590383529663\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 18.72 +/- 16.4 %\n",
            "[1,    50] loss: 0.329\n",
            "Batch: 49   Training loss: 0.32890599966049194\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.45 +/- 14.7 %\n",
            "Batch: 50   Training loss: 0.3277547061443329\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.46 +/- 11.5 %\n",
            "Batch: 51   Training loss: 0.3194631338119507\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.69 +/- 15.6 %\n",
            "Batch: 52   Training loss: 0.32457873225212097\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.80 +/- 12.5 %\n",
            "Batch: 53   Training loss: 0.3151811957359314\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.39 +/- 12.4 %\n",
            "Batch: 54   Training loss: 0.3149648904800415\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 9.98 +/- 10.8 %\n",
            "Batch: 55   Training loss: 0.3083397150039673\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.44 +/- 13.3 %\n",
            "Batch: 56   Training loss: 0.3106554448604584\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.26 +/- 10.3 %\n",
            "Batch: 57   Training loss: 0.29505452513694763\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.90 +/- 13.2 %\n",
            "Batch: 58   Training loss: 0.29857000708580017\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.11 +/- 11.0 %\n",
            "Batch: 59   Training loss: 0.2890150845050812\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.28 +/- 12.9 %\n",
            "Batch: 60   Training loss: 0.29336974024772644\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.85 +/- 11.9 %\n",
            "Batch: 61   Training loss: 0.2874607741832733\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.96 +/- 12.1 %\n",
            "Batch: 62   Training loss: 0.2760406732559204\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.70 +/- 14.8 %\n",
            "Batch: 63   Training loss: 0.2798732817173004\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.70 +/- 16.5 %\n",
            "Batch: 64   Training loss: 0.28247591853141785\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.85 +/- 13.2 %\n",
            "Batch: 65   Training loss: 0.27996185421943665\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.03 +/- 10.9 %\n",
            "Batch: 66   Training loss: 0.2662559151649475\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 19.42 +/- 15.2 %\n",
            "Batch: 67   Training loss: 0.26278436183929443\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.28 +/- 15.5 %\n",
            "Batch: 68   Training loss: 0.27095362544059753\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.39 +/- 14.4 %\n",
            "Batch: 69   Training loss: 0.26442378759384155\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.27 +/- 15.1 %\n",
            "Batch: 70   Training loss: 0.278244286775589\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.80 +/- 13.5 %\n",
            "Batch: 71   Training loss: 0.27170878648757935\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 9.99 +/- 10.8 %\n",
            "Batch: 72   Training loss: 0.2542327344417572\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.88 +/- 13.4 %\n",
            "Batch: 73   Training loss: 0.2554520070552826\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.02 +/- 11.3 %\n",
            "Batch: 74   Training loss: 0.24626633524894714\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.91 +/- 14.7 %\n",
            "Batch: 75   Training loss: 0.2515932619571686\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 18.02 +/- 15.4 %\n",
            "Batch: 76   Training loss: 0.2450852394104004\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.28 +/- 13.3 %\n",
            "Batch: 77   Training loss: 0.25093239545822144\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.24 +/- 15.3 %\n",
            "Batch: 78   Training loss: 0.24336035549640656\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.69 +/- 13.4 %\n",
            "Batch: 79   Training loss: 0.2528245747089386\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.90 +/- 12.9 %\n",
            "Batch: 80   Training loss: 0.2406051903963089\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.76 +/- 16.5 %\n",
            "Batch: 81   Training loss: 0.25227904319763184\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.45 +/- 15.2 %\n",
            "Batch: 82   Training loss: 0.23553746938705444\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.49 +/- 16.2 %\n",
            "Batch: 83   Training loss: 0.2246091365814209\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.87 +/- 15.6 %\n",
            "Batch: 84   Training loss: 0.23162642121315002\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.72 +/- 14.5 %\n",
            "Batch: 85   Training loss: 0.2325119823217392\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.19 +/- 11.3 %\n",
            "Batch: 86   Training loss: 0.2225511074066162\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.32 +/- 15.5 %\n",
            "Batch: 87   Training loss: 0.24544039368629456\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.12 +/- 14.5 %\n",
            "Batch: 88   Training loss: 0.23428063094615936\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.89 +/- 13.4 %\n",
            "Batch: 89   Training loss: 0.22998185455799103\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.61 +/- 13.2 %\n",
            "Batch: 90   Training loss: 0.24568668007850647\n",
            "Precision: 35.71 +/- 47.9 %\n",
            "Recall: 7.15 +/- 10.7 %\n",
            "Batch: 91   Training loss: 0.22548307478427887\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.92 +/- 15.8 %\n",
            "Batch: 92   Training loss: 0.2279200404882431\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.13 +/- 13.8 %\n",
            "Batch: 93   Training loss: 0.23647619783878326\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.97 +/- 12.9 %\n",
            "Batch: 94   Training loss: 0.23387491703033447\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.83 +/- 11.1 %\n",
            "Batch: 95   Training loss: 0.21742753684520721\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.07 +/- 15.0 %\n",
            "Batch: 96   Training loss: 0.21860620379447937\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.49 +/- 13.7 %\n",
            "Batch: 97   Training loss: 0.22120805084705353\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.36 +/- 13.7 %\n",
            "Batch: 98   Training loss: 0.2244575470685959\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.41 +/- 13.0 %\n",
            "[1,   100] loss: 0.238\n",
            "Batch: 99   Training loss: 0.2380104511976242\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.66 +/- 11.2 %\n",
            "Batch: 100   Training loss: 0.2226368486881256\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.49 +/- 12.3 %\n",
            "Batch: 101   Training loss: 0.22396627068519592\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.23 +/- 14.9 %\n",
            "Batch: 102   Training loss: 0.23183868825435638\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.00 +/- 13.3 %\n",
            "Batch: 103   Training loss: 0.22227151691913605\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 11.71 +/- 9.9 %\n",
            "Batch: 104   Training loss: 0.2201157659292221\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 11.97 +/- 12.2 %\n",
            "Batch: 105   Training loss: 0.22146184742450714\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.50 +/- 13.5 %\n",
            "Batch: 106   Training loss: 0.21320734918117523\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.00 +/- 13.0 %\n",
            "Batch: 107   Training loss: 0.20932339131832123\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 18.91 +/- 17.9 %\n",
            "Batch: 108   Training loss: 0.2187090665102005\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.31 +/- 14.0 %\n",
            "Batch: 109   Training loss: 0.2188269942998886\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.50 +/- 12.6 %\n",
            "Batch: 110   Training loss: 0.20268474519252777\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.85 +/- 14.9 %\n",
            "Batch: 111   Training loss: 0.20866705477237701\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.29 +/- 14.2 %\n",
            "Batch: 112   Training loss: 0.20876413583755493\n",
            "Precision: 62.96 +/- 48.3 %\n",
            "Recall: 14.88 +/- 13.2 %\n",
            "Batch: 113   Training loss: 0.2026059627532959\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.77 +/- 12.8 %\n",
            "Batch: 114   Training loss: 0.21893736720085144\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.35 +/- 12.7 %\n",
            "Batch: 115   Training loss: 0.20994682610034943\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.80 +/- 14.1 %\n",
            "Batch: 116   Training loss: 0.20420455932617188\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.71 +/- 14.8 %\n",
            "Batch: 117   Training loss: 0.2087874859571457\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.62 +/- 13.7 %\n",
            "Batch: 118   Training loss: 0.2002866268157959\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.82 +/- 11.7 %\n",
            "Batch: 119   Training loss: 0.20586153864860535\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.75 +/- 15.0 %\n",
            "Batch: 120   Training loss: 0.21301232278347015\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.53 +/- 13.6 %\n",
            "Batch: 121   Training loss: 0.2032639980316162\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.99 +/- 12.4 %\n",
            "Batch: 122   Training loss: 0.21197661757469177\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.57 +/- 13.7 %\n",
            "Batch: 123   Training loss: 0.2013673633337021\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.27 +/- 14.8 %\n",
            "Batch: 124   Training loss: 0.20112547278404236\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.50 +/- 14.9 %\n",
            "Batch: 125   Training loss: 0.2146182805299759\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.41 +/- 14.7 %\n",
            "Batch: 126   Training loss: 0.20926055312156677\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.44 +/- 14.3 %\n",
            "Batch: 127   Training loss: 0.20153094828128815\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 9.54 +/- 11.5 %\n",
            "Batch: 128   Training loss: 0.21360112726688385\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.79 +/- 12.5 %\n",
            "Batch: 129   Training loss: 0.19471591711044312\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.13 +/- 13.1 %\n",
            "Batch: 130   Training loss: 0.21109136939048767\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.68 +/- 12.5 %\n",
            "Batch: 131   Training loss: 0.19563432037830353\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.54 +/- 14.0 %\n",
            "Batch: 132   Training loss: 0.20269034802913666\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 11.96 +/- 14.0 %\n",
            "Batch: 133   Training loss: 0.2136079967021942\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 17.67 +/- 11.0 %\n",
            "Batch: 134   Training loss: 0.20542468130588531\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 12.93 +/- 10.0 %\n",
            "Batch: 135   Training loss: 0.1906479001045227\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.84 +/- 14.6 %\n",
            "Batch: 136   Training loss: 0.20309074223041534\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.67 +/- 13.6 %\n",
            "Batch: 137   Training loss: 0.20237311720848083\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 16.45 +/- 16.6 %\n",
            "Batch: 138   Training loss: 0.21036572754383087\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.60 +/- 12.5 %\n",
            "Batch: 139   Training loss: 0.20642471313476562\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.06 +/- 14.4 %\n",
            "Batch: 140   Training loss: 0.19873958826065063\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.61 +/- 13.0 %\n",
            "Batch: 141   Training loss: 0.21332582831382751\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.94 +/- 16.2 %\n",
            "Batch: 142   Training loss: 0.2139434665441513\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.36 +/- 13.0 %\n",
            "Batch: 143   Training loss: 0.2088310569524765\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.50 +/- 12.8 %\n",
            "Batch: 144   Training loss: 0.1945052593946457\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.32 +/- 12.8 %\n",
            "Batch: 145   Training loss: 0.18905898928642273\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 12.68 +/- 15.5 %\n",
            "Batch: 146   Training loss: 0.19584022462368011\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.62 +/- 12.1 %\n",
            "Batch: 147   Training loss: 0.20248381793498993\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.45 +/- 13.1 %\n",
            "Batch: 148   Training loss: 0.18130533397197723\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.22 +/- 15.0 %\n",
            "[1,   150] loss: 0.191\n",
            "Batch: 149   Training loss: 0.19051769375801086\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.54 +/- 13.7 %\n",
            "Batch: 150   Training loss: 0.1961040198802948\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.87 +/- 11.8 %\n",
            "Batch: 151   Training loss: 0.18653512001037598\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.64 +/- 12.8 %\n",
            "Batch: 152   Training loss: 0.18989995121955872\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.87 +/- 14.7 %\n",
            "Batch: 153   Training loss: 0.19224496185779572\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 13.23 +/- 13.5 %\n",
            "Batch: 154   Training loss: 0.18563666939735413\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.63 +/- 12.6 %\n",
            "Batch: 155   Training loss: 0.20140260457992554\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.65 +/- 13.9 %\n",
            "Batch: 156   Training loss: 0.19046427309513092\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.37 +/- 16.5 %\n",
            "Batch: 157   Training loss: 0.18957693874835968\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.67 +/- 12.5 %\n",
            "Batch: 158   Training loss: 0.19256743788719177\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.77 +/- 12.6 %\n",
            "Batch: 159   Training loss: 0.18885058164596558\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 13.66 +/- 15.1 %\n",
            "Batch: 160   Training loss: 0.1984259933233261\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 16.60 +/- 17.0 %\n",
            "Batch: 161   Training loss: 0.18530942499637604\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 18.21 +/- 14.2 %\n",
            "Batch: 162   Training loss: 0.1929960697889328\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.95 +/- 10.8 %\n",
            "Batch: 163   Training loss: 0.18928086757659912\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.29 +/- 14.6 %\n",
            "Batch: 164   Training loss: 0.1792972832918167\n",
            "Precision: 62.96 +/- 48.3 %\n",
            "Recall: 15.95 +/- 14.1 %\n",
            "Batch: 165   Training loss: 0.20841827988624573\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.13 +/- 12.6 %\n",
            "Batch: 166   Training loss: 0.19751682877540588\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.17 +/- 11.8 %\n",
            "Batch: 167   Training loss: 0.1996740698814392\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.84 +/- 13.1 %\n",
            "Batch: 168   Training loss: 0.18941323459148407\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 11.76 +/- 13.2 %\n",
            "Batch: 169   Training loss: 0.19248968362808228\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.00 +/- 15.5 %\n",
            "Batch: 170   Training loss: 0.2024557739496231\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.82 +/- 12.6 %\n",
            "Batch: 171   Training loss: 0.1881365329027176\n",
            "Precision: 55.56 +/- 49.7 %\n",
            "Recall: 13.93 +/- 14.3 %\n",
            "Batch: 172   Training loss: 0.18142741918563843\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.21 +/- 14.6 %\n",
            "Batch: 173   Training loss: 0.20741860568523407\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 7.82 +/- 9.8 %\n",
            "Batch: 174   Training loss: 0.18314066529273987\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.33 +/- 14.4 %\n",
            "Batch: 175   Training loss: 0.18762893974781036\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 11.34 +/- 15.3 %\n",
            "Batch: 176   Training loss: 0.18657459318637848\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 20.61 +/- 14.7 %\n",
            "Batch: 177   Training loss: 0.17917804419994354\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 22.96 +/- 15.7 %\n",
            "Batch: 178   Training loss: 0.20584839582443237\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.41 +/- 13.0 %\n",
            "Batch: 179   Training loss: 0.20903488993644714\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.08 +/- 11.0 %\n",
            "Batch: 180   Training loss: 0.19232603907585144\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 18.76 +/- 12.9 %\n",
            "Batch: 181   Training loss: 0.18235640227794647\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.57 +/- 11.8 %\n",
            "Batch: 182   Training loss: 0.18560898303985596\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.22 +/- 13.6 %\n",
            "Batch: 183   Training loss: 0.1824437379837036\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.92 +/- 14.4 %\n",
            "Batch: 184   Training loss: 0.18340328335762024\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.30 +/- 14.6 %\n",
            "Batch: 185   Training loss: 0.19266824424266815\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.16 +/- 10.7 %\n",
            "Batch: 186   Training loss: 0.18129415810108185\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.88 +/- 12.9 %\n",
            "Batch: 187   Training loss: 0.19949369132518768\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.55 +/- 14.4 %\n",
            "Batch: 188   Training loss: 0.216059610247612\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 11.69 +/- 10.5 %\n",
            "Batch: 189   Training loss: 0.18209517002105713\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.01 +/- 14.5 %\n",
            "Batch: 190   Training loss: 0.1810147613286972\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.70 +/- 14.6 %\n",
            "Batch: 191   Training loss: 0.18963052332401276\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 9.56 +/- 10.9 %\n",
            "Batch: 192   Training loss: 0.18707112967967987\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.12 +/- 12.3 %\n",
            "Batch: 193   Training loss: 0.19029071927070618\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.41 +/- 16.0 %\n",
            "Batch: 194   Training loss: 0.19482450187206268\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.15 +/- 13.1 %\n",
            "Batch: 195   Training loss: 0.19043493270874023\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.10 +/- 12.9 %\n",
            "Batch: 196   Training loss: 0.1996985375881195\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.76 +/- 14.1 %\n",
            "Batch: 197   Training loss: 0.20055359601974487\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.12 +/- 14.4 %\n",
            "Batch: 198   Training loss: 0.1939089149236679\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 11.11 +/- 14.8 %\n",
            "[1,   200] loss: 0.171\n",
            "Batch: 199   Training loss: 0.1710444688796997\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 15.39 +/- 16.5 %\n",
            "Batch: 200   Training loss: 0.16981400549411774\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 19.03 +/- 16.7 %\n",
            "Batch: 201   Training loss: 0.17691056430339813\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 11.05 +/- 13.8 %\n",
            "Batch: 202   Training loss: 0.18655814230442047\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 14.31 +/- 14.6 %\n",
            "Batch: 203   Training loss: 0.186964213848114\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.99 +/- 11.7 %\n",
            "Batch: 204   Training loss: 0.19713442027568817\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.81 +/- 11.3 %\n",
            "Batch: 205   Training loss: 0.1909802109003067\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 13.91 +/- 11.6 %\n",
            "Batch: 206   Training loss: 0.182213693857193\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.86 +/- 10.4 %\n",
            "Batch: 207   Training loss: 0.17462477087974548\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 18.55 +/- 16.9 %\n",
            "Batch: 208   Training loss: 0.18814443051815033\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.17 +/- 15.4 %\n",
            "Batch: 209   Training loss: 0.19388867914676666\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.46 +/- 12.6 %\n",
            "Batch: 210   Training loss: 0.17915278673171997\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 10.44 +/- 10.4 %\n",
            "Batch: 211   Training loss: 0.1948269009590149\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 15.91 +/- 12.3 %\n",
            "Batch: 212   Training loss: 0.19962827861309052\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.38 +/- 12.2 %\n",
            "Batch: 213   Training loss: 0.1800653636455536\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.27 +/- 14.1 %\n",
            "Batch: 214   Training loss: 0.18833743035793304\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 18.06 +/- 12.7 %\n",
            "Batch: 215   Training loss: 0.1873631477355957\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 15.29 +/- 12.7 %\n",
            "Batch: 216   Training loss: 0.1902775764465332\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 16.59 +/- 14.0 %\n",
            "Batch: 217   Training loss: 0.2029721438884735\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 14.81 +/- 13.9 %\n",
            "Batch: 218   Training loss: 0.1851184070110321\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.27 +/- 13.8 %\n",
            "Batch: 219   Training loss: 0.1850971132516861\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 14.12 +/- 10.6 %\n",
            "Batch: 220   Training loss: 0.1868683397769928\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.61 +/- 17.2 %\n",
            "Batch: 221   Training loss: 0.1689961701631546\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 14.23 +/- 13.6 %\n",
            "Batch: 222   Training loss: 0.18817628920078278\n",
            "Precision: 35.71 +/- 47.9 %\n",
            "Recall: 8.84 +/- 13.1 %\n",
            "Batch: 223   Training loss: 0.1973266303539276\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 16.06 +/- 15.3 %\n",
            "Batch: 224   Training loss: 0.20201031863689423\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.96 +/- 13.4 %\n",
            "Batch: 225   Training loss: 0.18068177998065948\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 14.26 +/- 16.4 %\n",
            "Batch: 226   Training loss: 0.18188290297985077\n",
            "Precision: 57.14 +/- 49.5 %\n",
            "Recall: 12.97 +/- 13.0 %\n",
            "Batch: 227   Training loss: 0.1906598061323166\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.24 +/- 12.9 %\n",
            "Batch: 228   Training loss: 0.18839199841022491\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.58 +/- 11.2 %\n",
            "Batch: 229   Training loss: 0.18984152376651764\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 10.71 +/- 13.0 %\n",
            "Batch: 230   Training loss: 0.18810908496379852\n",
            "Precision: 50.00 +/- 50.0 %\n",
            "Recall: 12.61 +/- 15.7 %\n",
            "Batch: 231   Training loss: 0.18380041420459747\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.50 +/- 14.0 %\n",
            "Batch: 232   Training loss: 0.180243119597435\n",
            "Precision: 46.43 +/- 49.9 %\n",
            "Recall: 10.63 +/- 13.5 %\n",
            "Batch: 233   Training loss: 0.18958108127117157\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 16.17 +/- 12.1 %\n",
            "Batch: 234   Training loss: 0.1809033304452896\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.56 +/- 13.2 %\n",
            "Batch: 235   Training loss: 0.19013908505439758\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 12.05 +/- 11.2 %\n",
            "Batch: 236   Training loss: 0.18749769032001495\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 13.96 +/- 14.1 %\n",
            "Batch: 237   Training loss: 0.17399094998836517\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 12.69 +/- 13.6 %\n",
            "Batch: 238   Training loss: 0.19714874029159546\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.59 +/- 11.2 %\n",
            "Batch: 239   Training loss: 0.169640451669693\n",
            "Precision: 42.86 +/- 49.5 %\n",
            "Recall: 10.14 +/- 12.4 %\n",
            "Batch: 240   Training loss: 0.18725818395614624\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 15.27 +/- 13.1 %\n",
            "Batch: 241   Training loss: 0.19713464379310608\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 17.59 +/- 13.7 %\n",
            "Batch: 242   Training loss: 0.17563578486442566\n",
            "Precision: 64.29 +/- 47.9 %\n",
            "Recall: 17.64 +/- 16.5 %\n",
            "Batch: 243   Training loss: 0.1791524738073349\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 16.07 +/- 12.6 %\n",
            "Batch: 244   Training loss: 0.1891918182373047\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 9.52 +/- 9.7 %\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-e221f3ae026e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-70205beb876c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input type {npimg.dtype} is not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2705\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2706\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2707\u001b[0;31m     \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2708\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "cv = KFold(n_splits=5)\n",
        "cv.get_n_splits(images, tokens)\n",
        "\n",
        "ACC = []\n",
        "HIST = []\n",
        "\n",
        "BS = 28\n",
        "log_every_n_batch=1\n",
        "\n",
        "data = ImgTokenDataset(images, tokens, transform=transform, target_transform=target_transform, vocab=vocab)\n",
        "for fold, (train_index, test_index) in enumerate(cv.split(images)):\n",
        "    \n",
        "    HIST.append([])\n",
        "    \n",
        "    print('Fold: ', fold)\n",
        "    # Setup data\n",
        "\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_index)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      data, \n",
        "                      batch_size=BS, sampler=train_subsampler)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "                      data,\n",
        "                      batch_size=BS, sampler=test_subsampler)\n",
        "\n",
        "    # Model\n",
        "    net = Net(input_shape, vocab)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "    net.to(device)\n",
        "    # Train\n",
        "    for epoch in range(10):  # loop over the dataset multiple times\n",
        "        print('Epoch: ', epoch)\n",
        "        for i, d in enumerate(trainloader, 0):\n",
        "            inputs, labels = d\n",
        "        \n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            #inputs = inputs.cuda()\n",
        "            #labels = labels.cuda()\n",
        "            optimizer.zero_grad()   # zero the gradient buffers\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            if i % 50 == 49:\n",
        "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, loss.item()))\n",
        "                HIST[fold].append(loss.item())\n",
        "\n",
        "            if i % log_every_n_batch == 0:\n",
        "              print('Batch: {}   Training loss: {}'.format(i, loss.item()))\n",
        "              p_m, p_s, r_m, r_s = report_precision_recall(net, d, device)\n",
        "              print(f'Precision: {100*p_m:.2f} +/- {100*p_s:.1f} %')\n",
        "              print(f'Recall: {100*r_m:.2f} +/- {100*r_s:.1f} %')\n",
        "            \n",
        "                \n",
        "    correct, total = test_cnn(net, testloader)\n",
        "    acc = correct / total\n",
        "\n",
        "    print('Accuracy: ')\n",
        "    print(f'{acc.mean():.3f} +/- {(acc.std()/2):.2f}')\n",
        "    print()\n",
        "    ACC.append(acc)\n",
        "    \n",
        "                      \n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "UXyo6LnSsdAz",
        "outputId": "d2d4aaaf-1fe1-46ab-d4b0-1a4fe443a02c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-fff7d713cca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmean_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmean_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ACC' is not defined"
          ]
        }
      ],
      "source": [
        "mean_accs = []\n",
        "for vals in ACC:\n",
        "    mean_accs.append(np.mean(vals))\n",
        "\n",
        "print(np.mean(mean_accs))\n",
        "print(np.std(mean_accs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "sxVYaSuAsdAz",
        "outputId": "425d2065-83aa-4121-c115-fedc3747e35c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-2c05de2a4fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHIST\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIST\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'fold {fold}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# plt.xticks(labels=str(50*np.arange(len(HIST[fold]))))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "for fold in range(5):\n",
        "    plt.plot(list(range(len(HIST[fold]))), HIST[fold], label=f'fold {fold}', alpha=0.5)\n",
        "    \n",
        "# plt.xticks(labels=str(50*np.arange(len(HIST[fold]))))\n",
        "plt.xlabel('iterations * 50')\n",
        "plt.ylabel('training loss')\n",
        "plt.legend()\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StratifiedShuffleSplit with 10 folds"
      ],
      "metadata": {
        "id": "uHsE4Da7GxXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Experimenting with StratifiedShuffleSplit since this will make sure that during training, a percentage of samples for each class is preserved, so better training for each data labels."
      ],
      "metadata": {
        "id": "RKPHXzYrH6hJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = {}\n",
        "for token in tokens:\n",
        "  d[token] = d.get(token, 0) + 1"
      ],
      "metadata": {
        "id": "E8mxwaCGkOPM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sufficient_data_available = []\n",
        "for key in d:\n",
        "  if d[key] > 1:\n",
        "    sufficient_data_available.append(key)"
      ],
      "metadata": {
        "id": "pm9khcgxkQkJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_filtered = []\n",
        "images_filtered = []\n",
        "for idx in range(len(tokens)):\n",
        "  token = tokens[idx]\n",
        "  image = images[idx]\n",
        "  if token in sufficient_data_available:\n",
        "    tokens_filtered.append(token)\n",
        "    images_filtered.append(image)"
      ],
      "metadata": {
        "id": "LoIb0MVllRAl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "heuxCjUDsdAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0754c93-48ef-4bae-a151-f5fe9df0a9b9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold:  0\n",
            "Epoch:  0\n",
            "Batch: 0   Training loss: 0.6882122159004211\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Precision: 66.55 +/- 34.2 %\n",
            "Recall: 21.99 +/- 13.5 %\n",
            "Batch: 68   Training loss: 0.18326899409294128\n",
            "Precision: 69.05 +/- 34.4 %\n",
            "Recall: 20.22 +/- 12.9 %\n",
            "Batch: 69   Training loss: 0.16616378724575043\n",
            "Precision: 50.00 +/- 35.8 %\n",
            "Recall: 19.02 +/- 15.4 %\n",
            "Batch: 70   Training loss: 0.17073678970336914\n",
            "Precision: 64.88 +/- 30.1 %\n",
            "Recall: 29.54 +/- 13.8 %\n",
            "Batch: 71   Training loss: 0.1668853610754013\n",
            "Precision: 50.89 +/- 38.9 %\n",
            "Recall: 24.00 +/- 19.6 %\n",
            "Batch: 72   Training loss: 0.174329474568367\n",
            "Precision: 69.94 +/- 35.9 %\n",
            "Recall: 24.27 +/- 13.7 %\n",
            "Batch: 73   Training loss: 0.16692447662353516\n",
            "Precision: 62.44 +/- 36.6 %\n",
            "Recall: 24.07 +/- 16.0 %\n",
            "Batch: 74   Training loss: 0.1948278546333313\n",
            "Precision: 62.68 +/- 33.2 %\n",
            "Recall: 22.17 +/- 12.9 %\n",
            "Batch: 75   Training loss: 0.17241822183132172\n",
            "Precision: 74.70 +/- 41.8 %\n",
            "Recall: 24.64 +/- 17.2 %\n",
            "Batch: 76   Training loss: 0.178675577044487\n",
            "Precision: 65.00 +/- 36.9 %\n",
            "Recall: 22.89 +/- 15.6 %\n",
            "Batch: 77   Training loss: 0.18019519746303558\n",
            "Precision: 62.50 +/- 32.2 %\n",
            "Recall: 24.38 +/- 13.7 %\n",
            "Batch: 78   Training loss: 0.19194753468036652\n",
            "Precision: 67.86 +/- 37.4 %\n",
            "Recall: 21.47 +/- 15.1 %\n",
            "Batch: 79   Training loss: 0.1737263947725296\n",
            "Precision: 56.79 +/- 46.1 %\n",
            "Recall: 17.06 +/- 17.9 %\n",
            "Batch: 80   Training loss: 0.1667906939983368\n",
            "Precision: 66.43 +/- 33.4 %\n",
            "Recall: 20.73 +/- 10.0 %\n",
            "Batch: 81   Training loss: 0.14623655378818512\n",
            "Precision: 69.35 +/- 34.2 %\n",
            "Recall: 30.00 +/- 16.8 %\n",
            "Batch: 82   Training loss: 0.1907459795475006\n",
            "Precision: 62.08 +/- 38.8 %\n",
            "Recall: 18.10 +/- 12.5 %\n",
            "Batch: 83   Training loss: 0.17071303725242615\n",
            "Precision: 63.10 +/- 47.4 %\n",
            "Recall: 15.78 +/- 14.1 %\n",
            "Batch: 84   Training loss: 0.16844552755355835\n",
            "Precision: 62.50 +/- 41.3 %\n",
            "Recall: 20.17 +/- 14.9 %\n",
            "Batch: 85   Training loss: 0.17679812014102936\n",
            "Precision: 66.67 +/- 33.5 %\n",
            "Recall: 24.45 +/- 13.4 %\n",
            "Batch: 86   Training loss: 0.18005679547786713\n",
            "Precision: 60.12 +/- 38.4 %\n",
            "Recall: 19.71 +/- 13.0 %\n",
            "Batch: 87   Training loss: 0.17580044269561768\n",
            "Precision: 66.37 +/- 33.5 %\n",
            "Recall: 25.14 +/- 15.5 %\n",
            "Batch: 88   Training loss: 0.1755608469247818\n",
            "Precision: 60.71 +/- 35.1 %\n",
            "Recall: 23.80 +/- 13.8 %\n",
            "Batch: 89   Training loss: 0.14997154474258423\n",
            "Precision: 52.47 +/- 41.4 %\n",
            "Recall: 19.57 +/- 16.3 %\n",
            "Batch: 90   Training loss: 0.1674199402332306\n",
            "Precision: 62.92 +/- 38.6 %\n",
            "Recall: 22.21 +/- 16.4 %\n",
            "Batch: 91   Training loss: 0.15815359354019165\n",
            "Precision: 66.67 +/- 42.7 %\n",
            "Recall: 19.75 +/- 15.4 %\n",
            "Batch: 92   Training loss: 0.16742351651191711\n",
            "Precision: 56.52 +/- 45.5 %\n",
            "Recall: 18.14 +/- 16.9 %\n",
            "Epoch:  2\n",
            "Batch: 0   Training loss: 0.17619240283966064\n",
            "Precision: 47.20 +/- 39.7 %\n",
            "Recall: 16.70 +/- 14.2 %\n",
            "Batch: 1   Training loss: 0.1792793571949005\n",
            "Precision: 61.79 +/- 34.1 %\n",
            "Recall: 24.32 +/- 12.5 %\n",
            "Batch: 2   Training loss: 0.18676716089248657\n",
            "Precision: 59.52 +/- 38.9 %\n",
            "Recall: 20.82 +/- 15.0 %\n",
            "Batch: 3   Training loss: 0.17205668985843658\n",
            "Precision: 67.56 +/- 40.9 %\n",
            "Recall: 21.23 +/- 15.6 %\n",
            "Batch: 4   Training loss: 0.16291719675064087\n",
            "Precision: 61.73 +/- 27.2 %\n",
            "Recall: 33.88 +/- 14.9 %\n",
            "Batch: 5   Training loss: 0.19221776723861694\n",
            "Precision: 52.96 +/- 30.8 %\n",
            "Recall: 28.49 +/- 16.3 %\n",
            "Batch: 6   Training loss: 0.19410373270511627\n",
            "Precision: 57.62 +/- 37.6 %\n",
            "Recall: 21.87 +/- 16.3 %\n",
            "Batch: 7   Training loss: 0.1640557199716568\n",
            "Precision: 67.74 +/- 35.8 %\n",
            "Recall: 27.99 +/- 17.5 %\n",
            "Batch: 8   Training loss: 0.16799473762512207\n",
            "Precision: 58.93 +/- 34.0 %\n",
            "Recall: 24.95 +/- 14.9 %\n",
            "Batch: 9   Training loss: 0.1841876357793808\n",
            "Precision: 59.82 +/- 31.2 %\n",
            "Recall: 21.84 +/- 11.4 %\n",
            "Batch: 10   Training loss: 0.17582884430885315\n",
            "Precision: 63.39 +/- 35.0 %\n",
            "Recall: 25.11 +/- 15.0 %\n",
            "Batch: 11   Training loss: 0.18285886943340302\n",
            "Precision: 70.24 +/- 37.1 %\n",
            "Recall: 21.17 +/- 13.4 %\n",
            "Batch: 12   Training loss: 0.16436725854873657\n",
            "Precision: 62.98 +/- 41.8 %\n",
            "Recall: 19.82 +/- 14.3 %\n",
            "Batch: 13   Training loss: 0.17868374288082123\n",
            "Precision: 48.45 +/- 35.0 %\n",
            "Recall: 24.88 +/- 17.7 %\n",
            "Batch: 14   Training loss: 0.16255447268486023\n",
            "Precision: 56.37 +/- 33.6 %\n",
            "Recall: 33.07 +/- 21.5 %\n",
            "Batch: 15   Training loss: 0.18653278052806854\n",
            "Precision: 68.33 +/- 33.0 %\n",
            "Recall: 25.04 +/- 13.6 %\n",
            "Batch: 16   Training loss: 0.16775836050510406\n",
            "Precision: 75.48 +/- 35.2 %\n",
            "Recall: 23.92 +/- 15.1 %\n",
            "Batch: 17   Training loss: 0.1779101938009262\n",
            "Precision: 61.73 +/- 34.9 %\n",
            "Recall: 20.72 +/- 12.4 %\n",
            "Batch: 18   Training loss: 0.20005269348621368\n",
            "Precision: 69.64 +/- 36.7 %\n",
            "Recall: 21.81 +/- 12.3 %\n",
            "Batch: 19   Training loss: 0.18855787813663483\n",
            "Precision: 70.54 +/- 31.0 %\n",
            "Recall: 24.84 +/- 16.7 %\n",
            "Batch: 20   Training loss: 0.1815222203731537\n",
            "Precision: 57.14 +/- 41.6 %\n",
            "Recall: 19.30 +/- 20.9 %\n",
            "Batch: 21   Training loss: 0.1684214323759079\n",
            "Precision: 64.52 +/- 39.9 %\n",
            "Recall: 21.40 +/- 15.5 %\n",
            "Batch: 22   Training loss: 0.18925902247428894\n",
            "Precision: 60.36 +/- 38.0 %\n",
            "Recall: 21.01 +/- 13.9 %\n",
            "Batch: 23   Training loss: 0.17012566328048706\n",
            "Precision: 68.15 +/- 34.2 %\n",
            "Recall: 25.71 +/- 15.8 %\n",
            "Batch: 24   Training loss: 0.1604510098695755\n",
            "Precision: 63.27 +/- 33.7 %\n",
            "Recall: 28.21 +/- 20.7 %\n",
            "Batch: 25   Training loss: 0.18082962930202484\n",
            "Precision: 48.81 +/- 45.0 %\n",
            "Recall: 14.16 +/- 14.4 %\n",
            "Batch: 26   Training loss: 0.156523659825325\n",
            "Precision: 66.67 +/- 42.7 %\n",
            "Recall: 17.40 +/- 13.7 %\n",
            "Batch: 27   Training loss: 0.17217682301998138\n",
            "Precision: 64.88 +/- 38.4 %\n",
            "Recall: 18.97 +/- 13.7 %\n",
            "Batch: 28   Training loss: 0.1682756543159485\n",
            "Precision: 61.90 +/- 35.6 %\n",
            "Recall: 24.50 +/- 15.9 %\n",
            "Batch: 29   Training loss: 0.17712195217609406\n",
            "Precision: 68.15 +/- 33.2 %\n",
            "Recall: 22.75 +/- 13.1 %\n",
            "Batch: 30   Training loss: 0.16633877158164978\n",
            "Precision: 60.12 +/- 38.2 %\n",
            "Recall: 20.41 +/- 14.5 %\n",
            "Batch: 31   Training loss: 0.15936017036437988\n",
            "Precision: 53.75 +/- 37.9 %\n",
            "Recall: 20.65 +/- 15.3 %\n",
            "Batch: 32   Training loss: 0.1803615391254425\n",
            "Precision: 59.23 +/- 40.3 %\n",
            "Recall: 16.83 +/- 13.3 %\n",
            "Batch: 33   Training loss: 0.16942931711673737\n",
            "Precision: 66.67 +/- 41.8 %\n",
            "Recall: 18.99 +/- 14.5 %\n",
            "Batch: 34   Training loss: 0.1738194078207016\n",
            "Precision: 65.48 +/- 44.1 %\n",
            "Recall: 16.05 +/- 12.8 %\n",
            "Batch: 35   Training loss: 0.17085564136505127\n",
            "Precision: 82.14 +/- 35.1 %\n",
            "Recall: 21.18 +/- 14.4 %\n",
            "Batch: 36   Training loss: 0.16207638382911682\n",
            "Precision: 67.86 +/- 40.3 %\n",
            "Recall: 19.07 +/- 12.5 %\n",
            "Batch: 37   Training loss: 0.16943150758743286\n",
            "Precision: 63.69 +/- 42.7 %\n",
            "Recall: 20.95 +/- 16.5 %\n",
            "Batch: 38   Training loss: 0.1753794550895691\n",
            "Precision: 67.86 +/- 40.5 %\n",
            "Recall: 19.59 +/- 12.7 %\n",
            "Batch: 39   Training loss: 0.15898558497428894\n",
            "Precision: 51.19 +/- 45.0 %\n",
            "Recall: 14.02 +/- 13.3 %\n",
            "Batch: 40   Training loss: 0.1778457909822464\n",
            "Precision: 72.92 +/- 42.8 %\n",
            "Recall: 19.09 +/- 15.9 %\n",
            "Batch: 41   Training loss: 0.1686355024576187\n",
            "Precision: 69.05 +/- 42.7 %\n",
            "Recall: 18.39 +/- 15.1 %\n",
            "Batch: 42   Training loss: 0.16426151990890503\n",
            "Precision: 67.26 +/- 44.6 %\n",
            "Recall: 18.98 +/- 14.9 %\n",
            "Batch: 43   Training loss: 0.15739254653453827\n",
            "Precision: 65.77 +/- 38.3 %\n",
            "Recall: 21.97 +/- 14.8 %\n",
            "Batch: 44   Training loss: 0.18330040574073792\n",
            "Precision: 64.29 +/- 36.7 %\n",
            "Recall: 23.02 +/- 16.6 %\n",
            "Batch: 45   Training loss: 0.18696287274360657\n",
            "Precision: 60.12 +/- 39.4 %\n",
            "Recall: 18.12 +/- 13.8 %\n",
            "Batch: 46   Training loss: 0.16673459112644196\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 15.07 +/- 16.1 %\n",
            "Batch: 47   Training loss: 0.16616135835647583\n",
            "Precision: 76.19 +/- 41.6 %\n",
            "Recall: 19.59 +/- 14.4 %\n",
            "Batch: 48   Training loss: 0.17403830587863922\n",
            "Precision: 67.86 +/- 45.0 %\n",
            "Recall: 15.41 +/- 13.0 %\n",
            "[3,    50] loss: 0.181\n",
            "Batch: 49   Training loss: 0.18109002709388733\n",
            "Precision: 70.54 +/- 42.1 %\n",
            "Recall: 16.62 +/- 11.4 %\n",
            "Batch: 50   Training loss: 0.17053040862083435\n",
            "Precision: 55.36 +/- 43.1 %\n",
            "Recall: 15.80 +/- 14.3 %\n",
            "Batch: 51   Training loss: 0.15666697919368744\n",
            "Precision: 60.42 +/- 38.4 %\n",
            "Recall: 22.92 +/- 16.0 %\n",
            "Batch: 52   Training loss: 0.1730375438928604\n",
            "Precision: 75.30 +/- 36.3 %\n",
            "Recall: 22.17 +/- 12.5 %\n",
            "Batch: 53   Training loss: 0.18651336431503296\n",
            "Precision: 73.81 +/- 41.2 %\n",
            "Recall: 18.14 +/- 13.8 %\n",
            "Batch: 54   Training loss: 0.16990159451961517\n",
            "Precision: 67.86 +/- 45.0 %\n",
            "Recall: 17.22 +/- 12.8 %\n",
            "Batch: 55   Training loss: 0.18039238452911377\n",
            "Precision: 67.86 +/- 45.0 %\n",
            "Recall: 17.37 +/- 14.7 %\n",
            "Batch: 56   Training loss: 0.17612206935882568\n",
            "Precision: 62.68 +/- 41.7 %\n",
            "Recall: 19.59 +/- 15.5 %\n",
            "Batch: 57   Training loss: 0.17359884083271027\n",
            "Precision: 75.77 +/- 31.8 %\n",
            "Recall: 24.87 +/- 14.3 %\n",
            "Batch: 58   Training loss: 0.18186888098716736\n",
            "Precision: 60.30 +/- 38.5 %\n",
            "Recall: 18.54 +/- 12.7 %\n",
            "Batch: 59   Training loss: 0.17518343031406403\n",
            "Precision: 74.40 +/- 33.1 %\n",
            "Recall: 28.42 +/- 16.1 %\n",
            "Batch: 60   Training loss: 0.17174270749092102\n",
            "Precision: 61.90 +/- 40.3 %\n",
            "Recall: 19.18 +/- 14.9 %\n",
            "Batch: 61   Training loss: 0.18055960536003113\n",
            "Precision: 69.05 +/- 41.7 %\n",
            "Recall: 16.04 +/- 9.8 %\n",
            "Batch: 62   Training loss: 0.18247069418430328\n",
            "Precision: 66.96 +/- 32.2 %\n",
            "Recall: 22.44 +/- 12.3 %\n",
            "Batch: 63   Training loss: 0.17473582923412323\n",
            "Precision: 68.04 +/- 33.9 %\n",
            "Recall: 22.93 +/- 13.3 %\n",
            "Batch: 64   Training loss: 0.17968854308128357\n",
            "Precision: 61.25 +/- 39.4 %\n",
            "Recall: 21.54 +/- 15.2 %\n",
            "Batch: 65   Training loss: 0.1745060384273529\n",
            "Precision: 61.55 +/- 34.6 %\n",
            "Recall: 27.69 +/- 16.7 %\n",
            "Batch: 66   Training loss: 0.16668251156806946\n",
            "Precision: 55.65 +/- 39.3 %\n",
            "Recall: 23.82 +/- 17.4 %\n",
            "Batch: 67   Training loss: 0.1719467043876648\n",
            "Precision: 66.55 +/- 36.7 %\n",
            "Recall: 23.61 +/- 15.9 %\n",
            "Batch: 68   Training loss: 0.17291222512722015\n",
            "Precision: 77.68 +/- 34.5 %\n",
            "Recall: 24.68 +/- 14.5 %\n",
            "Batch: 69   Training loss: 0.18102993071079254\n",
            "Precision: 63.39 +/- 31.5 %\n",
            "Recall: 22.12 +/- 12.6 %\n",
            "Batch: 70   Training loss: 0.17244163155555725\n",
            "Precision: 66.07 +/- 30.9 %\n",
            "Recall: 25.32 +/- 13.8 %\n",
            "Batch: 71   Training loss: 0.1594497412443161\n",
            "Precision: 60.00 +/- 36.6 %\n",
            "Recall: 25.95 +/- 17.1 %\n",
            "Batch: 72   Training loss: 0.18074463307857513\n",
            "Precision: 59.52 +/- 33.2 %\n",
            "Recall: 25.65 +/- 15.2 %\n",
            "Batch: 73   Training loss: 0.16845981776714325\n",
            "Precision: 63.87 +/- 33.0 %\n",
            "Recall: 22.59 +/- 12.9 %\n",
            "Batch: 74   Training loss: 0.17920082807540894\n",
            "Precision: 63.99 +/- 35.8 %\n",
            "Recall: 19.81 +/- 12.8 %\n",
            "Batch: 75   Training loss: 0.16272231936454773\n",
            "Precision: 62.80 +/- 36.2 %\n",
            "Recall: 23.81 +/- 15.6 %\n",
            "Batch: 76   Training loss: 0.16823787987232208\n",
            "Precision: 72.02 +/- 32.4 %\n",
            "Recall: 26.64 +/- 15.9 %\n",
            "Batch: 77   Training loss: 0.16635021567344666\n",
            "Precision: 63.99 +/- 37.9 %\n",
            "Recall: 25.97 +/- 16.6 %\n",
            "Batch: 78   Training loss: 0.18367113173007965\n",
            "Precision: 67.86 +/- 35.3 %\n",
            "Recall: 24.04 +/- 16.3 %\n",
            "Batch: 79   Training loss: 0.17611652612686157\n",
            "Precision: 69.94 +/- 38.7 %\n",
            "Recall: 22.89 +/- 15.0 %\n",
            "Batch: 80   Training loss: 0.18089810013771057\n",
            "Precision: 68.33 +/- 41.5 %\n",
            "Recall: 20.67 +/- 15.0 %\n",
            "Batch: 81   Training loss: 0.16442549228668213\n",
            "Precision: 78.87 +/- 35.8 %\n",
            "Recall: 21.92 +/- 11.9 %\n",
            "Batch: 82   Training loss: 0.17463746666908264\n",
            "Precision: 63.10 +/- 39.2 %\n",
            "Recall: 17.68 +/- 11.7 %\n",
            "Batch: 83   Training loss: 0.16905100643634796\n",
            "Precision: 69.05 +/- 38.8 %\n",
            "Recall: 19.90 +/- 14.4 %\n",
            "Batch: 84   Training loss: 0.16563740372657776\n",
            "Precision: 45.83 +/- 45.6 %\n",
            "Recall: 12.14 +/- 13.4 %\n",
            "Batch: 85   Training loss: 0.18199887871742249\n",
            "Precision: 63.10 +/- 37.5 %\n",
            "Recall: 21.08 +/- 15.0 %\n",
            "Batch: 86   Training loss: 0.1659708023071289\n",
            "Precision: 44.64 +/- 37.9 %\n",
            "Recall: 16.08 +/- 14.6 %\n",
            "Batch: 87   Training loss: 0.15811586380004883\n",
            "Precision: 55.95 +/- 42.8 %\n",
            "Recall: 20.23 +/- 18.0 %\n",
            "Batch: 88   Training loss: 0.1655096411705017\n",
            "Precision: 83.33 +/- 32.7 %\n",
            "Recall: 19.39 +/- 8.8 %\n",
            "Batch: 89   Training loss: 0.161707803606987\n",
            "Precision: 64.58 +/- 38.8 %\n",
            "Recall: 17.49 +/- 11.9 %\n",
            "Batch: 90   Training loss: 0.1811634600162506\n",
            "Precision: 76.19 +/- 36.6 %\n",
            "Recall: 21.21 +/- 13.9 %\n",
            "Batch: 91   Training loss: 0.1635487675666809\n",
            "Precision: 51.85 +/- 45.7 %\n",
            "Recall: 13.92 +/- 14.6 %\n",
            "Batch: 92   Training loss: 0.17868594825267792\n",
            "Precision: 60.87 +/- 36.9 %\n",
            "Recall: 20.63 +/- 15.5 %\n",
            "Epoch:  3\n",
            "Batch: 0   Training loss: 0.16992443799972534\n",
            "Precision: 65.77 +/- 41.5 %\n",
            "Recall: 21.43 +/- 15.0 %\n",
            "Batch: 1   Training loss: 0.17390641570091248\n",
            "Precision: 70.54 +/- 29.3 %\n",
            "Recall: 23.35 +/- 12.2 %\n",
            "Batch: 2   Training loss: 0.1627190262079239\n",
            "Precision: 66.73 +/- 39.3 %\n",
            "Recall: 25.05 +/- 17.0 %\n",
            "Batch: 3   Training loss: 0.16281209886074066\n",
            "Precision: 78.27 +/- 36.1 %\n",
            "Recall: 26.41 +/- 15.4 %\n",
            "Batch: 4   Training loss: 0.17424751818180084\n",
            "Precision: 68.21 +/- 37.0 %\n",
            "Recall: 25.44 +/- 15.6 %\n",
            "Batch: 5   Training loss: 0.17033322155475616\n",
            "Precision: 67.86 +/- 37.4 %\n",
            "Recall: 24.28 +/- 15.4 %\n",
            "Batch: 6   Training loss: 0.15877003967761993\n",
            "Precision: 60.18 +/- 30.8 %\n",
            "Recall: 27.60 +/- 17.1 %\n",
            "Batch: 7   Training loss: 0.16705268621444702\n",
            "Precision: 74.88 +/- 33.9 %\n",
            "Recall: 25.90 +/- 14.1 %\n",
            "Batch: 8   Training loss: 0.16793932020664215\n",
            "Precision: 73.75 +/- 31.8 %\n",
            "Recall: 27.04 +/- 15.6 %\n",
            "Batch: 9   Training loss: 0.1775444597005844\n",
            "Precision: 54.94 +/- 41.4 %\n",
            "Recall: 14.22 +/- 11.1 %\n",
            "Batch: 10   Training loss: 0.16410334408283234\n",
            "Precision: 78.93 +/- 31.8 %\n",
            "Recall: 28.97 +/- 14.0 %\n",
            "Batch: 11   Training loss: 0.178848534822464\n",
            "Precision: 68.75 +/- 38.1 %\n",
            "Recall: 19.79 +/- 12.4 %\n",
            "Batch: 12   Training loss: 0.16803888976573944\n",
            "Precision: 63.81 +/- 36.5 %\n",
            "Recall: 27.08 +/- 15.8 %\n",
            "Batch: 13   Training loss: 0.17750012874603271\n",
            "Precision: 63.57 +/- 30.1 %\n",
            "Recall: 25.85 +/- 14.5 %\n",
            "Batch: 14   Training loss: 0.17303667962551117\n",
            "Precision: 65.48 +/- 35.0 %\n",
            "Recall: 23.90 +/- 14.4 %\n",
            "Batch: 15   Training loss: 0.1520867943763733\n",
            "Precision: 64.29 +/- 39.5 %\n",
            "Recall: 23.37 +/- 17.9 %\n",
            "Batch: 16   Training loss: 0.17408457398414612\n",
            "Precision: 70.71 +/- 41.2 %\n",
            "Recall: 19.71 +/- 14.3 %\n",
            "Batch: 17   Training loss: 0.1824251264333725\n",
            "Precision: 58.69 +/- 41.8 %\n",
            "Recall: 19.02 +/- 15.3 %\n",
            "Batch: 18   Training loss: 0.1794222593307495\n",
            "Precision: 66.96 +/- 33.0 %\n",
            "Recall: 25.30 +/- 15.0 %\n",
            "Batch: 19   Training loss: 0.16121406853199005\n",
            "Precision: 69.94 +/- 35.4 %\n",
            "Recall: 31.05 +/- 21.0 %\n",
            "Batch: 20   Training loss: 0.18788470327854156\n",
            "Precision: 63.99 +/- 30.1 %\n",
            "Recall: 27.77 +/- 20.6 %\n",
            "Batch: 21   Training loss: 0.18449164927005768\n",
            "Precision: 58.51 +/- 35.2 %\n",
            "Recall: 22.92 +/- 14.9 %\n",
            "Batch: 22   Training loss: 0.16788654029369354\n",
            "Precision: 58.04 +/- 42.6 %\n",
            "Recall: 20.56 +/- 16.4 %\n",
            "Batch: 23   Training loss: 0.15505088865756989\n",
            "Precision: 56.79 +/- 46.1 %\n",
            "Recall: 15.99 +/- 15.6 %\n",
            "Batch: 24   Training loss: 0.1741974800825119\n",
            "Precision: 61.90 +/- 43.4 %\n",
            "Recall: 16.79 +/- 13.8 %\n",
            "Batch: 25   Training loss: 0.16425751149654388\n",
            "Precision: 60.71 +/- 46.3 %\n",
            "Recall: 17.64 +/- 16.6 %\n",
            "Batch: 26   Training loss: 0.16918650269508362\n",
            "Precision: 71.73 +/- 39.5 %\n",
            "Recall: 20.26 +/- 13.9 %\n",
            "Batch: 27   Training loss: 0.18340447545051575\n",
            "Precision: 60.48 +/- 42.7 %\n",
            "Recall: 17.68 +/- 15.4 %\n",
            "Batch: 28   Training loss: 0.16694501042366028\n",
            "Precision: 46.73 +/- 44.1 %\n",
            "Recall: 17.08 +/- 18.1 %\n",
            "Batch: 29   Training loss: 0.18108944594860077\n",
            "Precision: 67.86 +/- 42.2 %\n",
            "Recall: 16.42 +/- 12.4 %\n",
            "Batch: 30   Training loss: 0.17691770195960999\n",
            "Precision: 83.33 +/- 35.1 %\n",
            "Recall: 20.66 +/- 13.7 %\n",
            "Batch: 31   Training loss: 0.17361252009868622\n",
            "Precision: 66.67 +/- 46.3 %\n",
            "Recall: 15.56 +/- 13.1 %\n",
            "Batch: 32   Training loss: 0.1780407875776291\n",
            "Precision: 76.19 +/- 38.7 %\n",
            "Recall: 17.24 +/- 12.5 %\n",
            "Batch: 33   Training loss: 0.14853206276893616\n",
            "Precision: 72.92 +/- 39.9 %\n",
            "Recall: 22.35 +/- 15.1 %\n",
            "Batch: 34   Training loss: 0.16693545877933502\n",
            "Precision: 58.93 +/- 39.6 %\n",
            "Recall: 22.19 +/- 17.4 %\n",
            "Batch: 35   Training loss: 0.16734544932842255\n",
            "Precision: 61.61 +/- 39.7 %\n",
            "Recall: 18.77 +/- 13.2 %\n",
            "Batch: 36   Training loss: 0.17241552472114563\n",
            "Precision: 64.58 +/- 41.0 %\n",
            "Recall: 19.20 +/- 13.4 %\n",
            "Batch: 37   Training loss: 0.17706559598445892\n",
            "Precision: 72.62 +/- 40.9 %\n",
            "Recall: 18.71 +/- 13.5 %\n",
            "Batch: 38   Training loss: 0.17860911786556244\n",
            "Precision: 72.98 +/- 35.7 %\n",
            "Recall: 21.36 +/- 14.5 %\n",
            "Batch: 39   Training loss: 0.17083294689655304\n",
            "Precision: 57.92 +/- 41.9 %\n",
            "Recall: 16.38 +/- 15.3 %\n",
            "Batch: 40   Training loss: 0.17305494844913483\n",
            "Precision: 60.89 +/- 39.7 %\n",
            "Recall: 21.12 +/- 17.5 %\n",
            "Batch: 41   Training loss: 0.17106813192367554\n",
            "Precision: 50.65 +/- 44.7 %\n",
            "Recall: 13.87 +/- 13.4 %\n",
            "Batch: 42   Training loss: 0.16840526461601257\n",
            "Precision: 64.17 +/- 36.0 %\n",
            "Recall: 22.40 +/- 15.8 %\n",
            "Batch: 43   Training loss: 0.16596898436546326\n",
            "Precision: 74.11 +/- 34.0 %\n",
            "Recall: 27.07 +/- 14.6 %\n",
            "Batch: 44   Training loss: 0.1840900331735611\n",
            "Precision: 61.61 +/- 42.5 %\n",
            "Recall: 18.99 +/- 15.0 %\n",
            "Batch: 45   Training loss: 0.1539568305015564\n",
            "Precision: 58.63 +/- 41.3 %\n",
            "Recall: 20.40 +/- 15.3 %\n",
            "Batch: 46   Training loss: 0.16959047317504883\n",
            "Precision: 65.65 +/- 41.5 %\n",
            "Recall: 19.06 +/- 13.9 %\n",
            "Batch: 47   Training loss: 0.19001471996307373\n",
            "Precision: 61.25 +/- 42.0 %\n",
            "Recall: 17.44 +/- 14.6 %\n",
            "Batch: 48   Training loss: 0.1559739112854004\n",
            "Precision: 58.27 +/- 42.7 %\n",
            "Recall: 20.91 +/- 16.9 %\n",
            "[4,    50] loss: 0.169\n",
            "Batch: 49   Training loss: 0.1694929301738739\n",
            "Precision: 63.15 +/- 42.5 %\n",
            "Recall: 21.79 +/- 16.8 %\n",
            "Batch: 50   Training loss: 0.18044941127300262\n",
            "Precision: 48.33 +/- 40.1 %\n",
            "Recall: 16.47 +/- 15.2 %\n",
            "Batch: 51   Training loss: 0.1806165874004364\n",
            "Precision: 65.48 +/- 37.8 %\n",
            "Recall: 24.71 +/- 20.2 %\n",
            "Batch: 52   Training loss: 0.16579820215702057\n",
            "Precision: 67.86 +/- 39.5 %\n",
            "Recall: 22.90 +/- 15.9 %\n",
            "Batch: 53   Training loss: 0.16746437549591064\n",
            "Precision: 52.38 +/- 44.9 %\n",
            "Recall: 16.22 +/- 16.4 %\n",
            "Batch: 54   Training loss: 0.1638098955154419\n",
            "Precision: 71.79 +/- 38.1 %\n",
            "Recall: 20.95 +/- 14.5 %\n",
            "Batch: 55   Training loss: 0.17318908870220184\n",
            "Precision: 63.10 +/- 39.1 %\n",
            "Recall: 21.38 +/- 16.4 %\n",
            "Batch: 56   Training loss: 0.1836533546447754\n",
            "Precision: 62.50 +/- 42.3 %\n",
            "Recall: 17.14 +/- 12.5 %\n",
            "Batch: 57   Training loss: 0.18301057815551758\n",
            "Precision: 74.40 +/- 28.3 %\n",
            "Recall: 24.50 +/- 15.0 %\n",
            "Batch: 58   Training loss: 0.1778501570224762\n",
            "Precision: 57.32 +/- 39.2 %\n",
            "Recall: 18.26 +/- 16.5 %\n",
            "Batch: 59   Training loss: 0.18124878406524658\n",
            "Precision: 76.13 +/- 36.9 %\n",
            "Recall: 21.84 +/- 13.2 %\n",
            "Batch: 60   Training loss: 0.1677430123090744\n",
            "Precision: 55.95 +/- 42.4 %\n",
            "Recall: 21.13 +/- 16.9 %\n",
            "Batch: 61   Training loss: 0.17984794080257416\n",
            "Precision: 66.96 +/- 34.3 %\n",
            "Recall: 22.21 +/- 12.5 %\n",
            "Batch: 62   Training loss: 0.17257720232009888\n",
            "Precision: 64.58 +/- 33.6 %\n",
            "Recall: 24.54 +/- 14.9 %\n",
            "Batch: 63   Training loss: 0.17157575488090515\n",
            "Precision: 64.88 +/- 36.4 %\n",
            "Recall: 24.54 +/- 16.1 %\n",
            "Batch: 64   Training loss: 0.1724761575460434\n",
            "Precision: 68.75 +/- 36.4 %\n",
            "Recall: 20.01 +/- 12.2 %\n",
            "Batch: 65   Training loss: 0.16049399971961975\n",
            "Precision: 65.18 +/- 36.8 %\n",
            "Recall: 26.23 +/- 17.0 %\n",
            "Batch: 66   Training loss: 0.1570589244365692\n",
            "Precision: 47.68 +/- 40.1 %\n",
            "Recall: 16.89 +/- 15.1 %\n",
            "Batch: 67   Training loss: 0.16851088404655457\n",
            "Precision: 55.95 +/- 46.3 %\n",
            "Recall: 15.98 +/- 15.1 %\n",
            "Batch: 68   Training loss: 0.17604024708271027\n",
            "Precision: 65.48 +/- 39.6 %\n",
            "Recall: 17.99 +/- 11.0 %\n",
            "Batch: 69   Training loss: 0.16458572447299957\n",
            "Precision: 62.68 +/- 42.9 %\n",
            "Recall: 22.39 +/- 18.6 %\n",
            "Batch: 70   Training loss: 0.16062477231025696\n",
            "Precision: 70.86 +/- 40.9 %\n",
            "Recall: 19.09 +/- 13.3 %\n",
            "Batch: 71   Training loss: 0.17414700984954834\n",
            "Precision: 75.30 +/- 41.4 %\n",
            "Recall: 21.80 +/- 17.0 %\n",
            "Batch: 72   Training loss: 0.1777912974357605\n",
            "Precision: 57.74 +/- 43.7 %\n",
            "Recall: 16.40 +/- 14.4 %\n",
            "Batch: 73   Training loss: 0.16943658888339996\n",
            "Precision: 46.13 +/- 44.0 %\n",
            "Recall: 12.64 +/- 13.3 %\n",
            "Batch: 74   Training loss: 0.1697978675365448\n",
            "Precision: 68.21 +/- 40.0 %\n",
            "Recall: 23.83 +/- 16.2 %\n",
            "Batch: 75   Training loss: 0.16874712705612183\n",
            "Precision: 60.00 +/- 39.4 %\n",
            "Recall: 21.63 +/- 16.7 %\n",
            "Batch: 76   Training loss: 0.16922102868556976\n",
            "Precision: 55.95 +/- 44.0 %\n",
            "Recall: 17.29 +/- 15.7 %\n",
            "Batch: 77   Training loss: 0.1658962070941925\n",
            "Precision: 74.76 +/- 34.5 %\n",
            "Recall: 22.95 +/- 13.6 %\n",
            "Batch: 78   Training loss: 0.1790338009595871\n",
            "Precision: 58.93 +/- 41.9 %\n",
            "Recall: 20.22 +/- 17.5 %\n",
            "Batch: 79   Training loss: 0.18835151195526123\n",
            "Precision: 68.93 +/- 40.4 %\n",
            "Recall: 18.40 +/- 14.0 %\n",
            "Batch: 80   Training loss: 0.17089220881462097\n",
            "Precision: 71.48 +/- 38.4 %\n",
            "Recall: 24.68 +/- 16.8 %\n",
            "Batch: 81   Training loss: 0.16666388511657715\n",
            "Precision: 58.99 +/- 41.5 %\n",
            "Recall: 18.33 +/- 15.1 %\n",
            "Batch: 82   Training loss: 0.16774073243141174\n",
            "Precision: 68.15 +/- 39.1 %\n",
            "Recall: 25.77 +/- 16.9 %\n",
            "Batch: 83   Training loss: 0.16555319726467133\n",
            "Precision: 76.96 +/- 34.1 %\n",
            "Recall: 25.00 +/- 15.3 %\n",
            "Batch: 84   Training loss: 0.1663055717945099\n",
            "Precision: 70.71 +/- 41.2 %\n",
            "Recall: 22.34 +/- 16.8 %\n",
            "Batch: 85   Training loss: 0.1711786836385727\n",
            "Precision: 62.86 +/- 41.8 %\n",
            "Recall: 18.59 +/- 15.8 %\n",
            "Batch: 86   Training loss: 0.17602458596229553\n",
            "Precision: 76.19 +/- 33.4 %\n",
            "Recall: 20.40 +/- 10.7 %\n",
            "Batch: 87   Training loss: 0.17411547899246216\n",
            "Precision: 64.58 +/- 38.8 %\n",
            "Recall: 21.86 +/- 17.4 %\n",
            "Batch: 88   Training loss: 0.1581091284751892\n",
            "Precision: 48.69 +/- 46.8 %\n",
            "Recall: 16.42 +/- 17.8 %\n",
            "Batch: 89   Training loss: 0.17307566106319427\n",
            "Precision: 73.21 +/- 33.4 %\n",
            "Recall: 24.20 +/- 14.2 %\n",
            "Batch: 90   Training loss: 0.16446846723556519\n",
            "Precision: 64.88 +/- 39.4 %\n",
            "Recall: 22.30 +/- 17.3 %\n",
            "Batch: 91   Training loss: 0.1677909940481186\n",
            "Precision: 59.52 +/- 40.2 %\n",
            "Recall: 24.01 +/- 20.8 %\n",
            "Batch: 92   Training loss: 0.19183321297168732\n",
            "Precision: 68.12 +/- 39.9 %\n",
            "Recall: 17.11 +/- 12.4 %\n",
            "Epoch:  4\n",
            "Batch: 0   Training loss: 0.16997681558132172\n",
            "Precision: 72.62 +/- 40.9 %\n",
            "Recall: 19.66 +/- 14.2 %\n",
            "Batch: 1   Training loss: 0.18201571702957153\n",
            "Precision: 72.92 +/- 38.7 %\n",
            "Recall: 20.52 +/- 13.7 %\n",
            "Batch: 2   Training loss: 0.18135523796081543\n",
            "Precision: 79.76 +/- 28.6 %\n",
            "Recall: 23.78 +/- 12.5 %\n",
            "Batch: 3   Training loss: 0.18569399416446686\n",
            "Precision: 70.12 +/- 30.4 %\n",
            "Recall: 23.29 +/- 12.6 %\n",
            "Batch: 4   Training loss: 0.16737522184848785\n",
            "Precision: 52.38 +/- 36.3 %\n",
            "Recall: 20.17 +/- 15.9 %\n",
            "Batch: 5   Training loss: 0.1744326651096344\n",
            "Precision: 59.40 +/- 39.3 %\n",
            "Recall: 21.77 +/- 16.7 %\n",
            "Batch: 6   Training loss: 0.19241254031658173\n",
            "Precision: 75.60 +/- 32.7 %\n",
            "Recall: 21.42 +/- 12.0 %\n",
            "Batch: 7   Training loss: 0.17523351311683655\n",
            "Precision: 67.26 +/- 38.9 %\n",
            "Recall: 18.51 +/- 12.7 %\n",
            "Batch: 8   Training loss: 0.1676151305437088\n",
            "Precision: 69.11 +/- 36.3 %\n",
            "Recall: 28.01 +/- 20.7 %\n",
            "Batch: 9   Training loss: 0.16843080520629883\n",
            "Precision: 68.75 +/- 30.6 %\n",
            "Recall: 26.28 +/- 14.0 %\n",
            "Batch: 10   Training loss: 0.16352011263370514\n",
            "Precision: 70.48 +/- 37.1 %\n",
            "Recall: 27.25 +/- 15.8 %\n",
            "Batch: 11   Training loss: 0.16275860369205475\n",
            "Precision: 63.57 +/- 43.3 %\n",
            "Recall: 20.65 +/- 16.0 %\n",
            "Batch: 12   Training loss: 0.17185470461845398\n",
            "Precision: 63.45 +/- 41.5 %\n",
            "Recall: 19.93 +/- 16.3 %\n",
            "Batch: 13   Training loss: 0.15369559824466705\n",
            "Precision: 62.50 +/- 34.3 %\n",
            "Recall: 22.91 +/- 14.5 %\n",
            "Batch: 14   Training loss: 0.16682010889053345\n",
            "Precision: 52.86 +/- 41.5 %\n",
            "Recall: 16.95 +/- 14.9 %\n",
            "Batch: 15   Training loss: 0.16519907116889954\n",
            "Precision: 63.10 +/- 44.0 %\n",
            "Recall: 17.08 +/- 16.5 %\n",
            "Batch: 16   Training loss: 0.1744733452796936\n",
            "Precision: 65.65 +/- 41.5 %\n",
            "Recall: 19.77 +/- 16.5 %\n",
            "Batch: 17   Training loss: 0.164521262049675\n",
            "Precision: 61.79 +/- 44.5 %\n",
            "Recall: 21.59 +/- 18.9 %\n",
            "Batch: 18   Training loss: 0.16904523968696594\n",
            "Precision: 68.93 +/- 37.0 %\n",
            "Recall: 25.82 +/- 17.0 %\n",
            "Batch: 19   Training loss: 0.16667556762695312\n",
            "Precision: 54.88 +/- 37.6 %\n",
            "Recall: 23.08 +/- 18.1 %\n",
            "Batch: 20   Training loss: 0.1573423445224762\n",
            "Precision: 62.62 +/- 40.8 %\n",
            "Recall: 25.32 +/- 18.6 %\n",
            "Batch: 21   Training loss: 0.17876940965652466\n",
            "Precision: 67.20 +/- 37.8 %\n",
            "Recall: 21.08 +/- 16.4 %\n",
            "Batch: 22   Training loss: 0.18038886785507202\n",
            "Precision: 71.43 +/- 40.5 %\n",
            "Recall: 18.76 +/- 13.0 %\n",
            "Batch: 23   Training loss: 0.17024199664592743\n",
            "Precision: 67.86 +/- 45.0 %\n",
            "Recall: 18.84 +/- 16.4 %\n",
            "Batch: 24   Training loss: 0.16782569885253906\n",
            "Precision: 59.88 +/- 39.0 %\n",
            "Recall: 20.22 +/- 15.4 %\n",
            "Batch: 25   Training loss: 0.16671998798847198\n",
            "Precision: 68.93 +/- 31.9 %\n",
            "Recall: 27.26 +/- 14.5 %\n",
            "Batch: 26   Training loss: 0.16692526638507843\n",
            "Precision: 70.24 +/- 31.5 %\n",
            "Recall: 28.02 +/- 20.0 %\n",
            "Batch: 27   Training loss: 0.15859554708003998\n",
            "Precision: 64.88 +/- 43.7 %\n",
            "Recall: 17.84 +/- 14.0 %\n",
            "Batch: 28   Training loss: 0.15695837140083313\n",
            "Precision: 65.95 +/- 43.3 %\n",
            "Recall: 16.88 +/- 14.0 %\n",
            "Batch: 29   Training loss: 0.1637176126241684\n",
            "Precision: 64.29 +/- 46.2 %\n",
            "Recall: 14.79 +/- 11.7 %\n",
            "Batch: 30   Training loss: 0.16251587867736816\n",
            "Precision: 66.43 +/- 41.8 %\n",
            "Recall: 22.22 +/- 17.6 %\n",
            "Batch: 31   Training loss: 0.15608979761600494\n",
            "Precision: 56.55 +/- 43.0 %\n",
            "Recall: 15.40 +/- 12.8 %\n",
            "Batch: 32   Training loss: 0.1777094602584839\n",
            "Precision: 63.69 +/- 41.3 %\n",
            "Recall: 17.71 +/- 12.9 %\n",
            "Batch: 33   Training loss: 0.1857994794845581\n",
            "Precision: 65.12 +/- 40.7 %\n",
            "Recall: 21.45 +/- 17.1 %\n",
            "Batch: 34   Training loss: 0.1689653843641281\n",
            "Precision: 58.04 +/- 43.6 %\n",
            "Recall: 19.27 +/- 16.1 %\n",
            "Batch: 35   Training loss: 0.16466890275478363\n",
            "Precision: 71.13 +/- 39.1 %\n",
            "Recall: 24.63 +/- 20.4 %\n",
            "Batch: 36   Training loss: 0.16467750072479248\n",
            "Precision: 65.06 +/- 38.9 %\n",
            "Recall: 19.30 +/- 13.7 %\n",
            "Batch: 37   Training loss: 0.17286011576652527\n",
            "Precision: 82.14 +/- 28.8 %\n",
            "Recall: 20.21 +/- 9.7 %\n",
            "Batch: 38   Training loss: 0.1703353375196457\n",
            "Precision: 68.15 +/- 42.3 %\n",
            "Recall: 21.64 +/- 17.3 %\n",
            "Batch: 39   Training loss: 0.17643475532531738\n",
            "Precision: 71.61 +/- 35.9 %\n",
            "Recall: 22.21 +/- 12.4 %\n",
            "Batch: 40   Training loss: 0.16234251856803894\n",
            "Precision: 56.43 +/- 41.6 %\n",
            "Recall: 22.58 +/- 17.6 %\n",
            "Batch: 41   Training loss: 0.1554194837808609\n",
            "Precision: 54.82 +/- 34.3 %\n",
            "Recall: 24.78 +/- 16.0 %\n",
            "Batch: 42   Training loss: 0.16128778457641602\n",
            "Precision: 54.35 +/- 44.0 %\n",
            "Recall: 17.11 +/- 16.2 %\n",
            "Batch: 43   Training loss: 0.17479905486106873\n",
            "Precision: 70.42 +/- 35.1 %\n",
            "Recall: 24.61 +/- 14.8 %\n",
            "Batch: 44   Training loss: 0.17219896614551544\n",
            "Precision: 73.81 +/- 38.9 %\n",
            "Recall: 21.03 +/- 13.2 %\n",
            "Batch: 45   Training loss: 0.18089047074317932\n",
            "Precision: 81.13 +/- 32.6 %\n",
            "Recall: 23.96 +/- 15.0 %\n",
            "Batch: 46   Training loss: 0.19314663112163544\n",
            "Precision: 61.25 +/- 35.8 %\n",
            "Recall: 22.50 +/- 15.5 %\n",
            "Batch: 47   Training loss: 0.17364826798439026\n",
            "Precision: 62.14 +/- 36.9 %\n",
            "Recall: 23.49 +/- 15.1 %\n",
            "Batch: 48   Training loss: 0.17494869232177734\n",
            "Precision: 50.36 +/- 40.8 %\n",
            "Recall: 18.88 +/- 17.1 %\n",
            "[5,    50] loss: 0.191\n",
            "Batch: 49   Training loss: 0.19103486835956573\n",
            "Precision: 57.62 +/- 40.9 %\n",
            "Recall: 15.57 +/- 14.6 %\n",
            "Batch: 50   Training loss: 0.17875640094280243\n",
            "Precision: 73.10 +/- 37.9 %\n",
            "Recall: 19.71 +/- 14.9 %\n",
            "Batch: 51   Training loss: 0.16093428432941437\n",
            "Precision: 70.83 +/- 40.7 %\n",
            "Recall: 20.29 +/- 14.8 %\n",
            "Batch: 52   Training loss: 0.1811821013689041\n",
            "Precision: 57.44 +/- 44.5 %\n",
            "Recall: 12.41 +/- 10.0 %\n",
            "Batch: 53   Training loss: 0.16739176213741302\n",
            "Precision: 55.06 +/- 43.0 %\n",
            "Recall: 16.19 +/- 14.9 %\n",
            "Batch: 54   Training loss: 0.16451044380664825\n",
            "Precision: 63.99 +/- 41.2 %\n",
            "Recall: 22.11 +/- 20.3 %\n",
            "Batch: 55   Training loss: 0.17357705533504486\n",
            "Precision: 60.12 +/- 43.7 %\n",
            "Recall: 17.63 +/- 15.7 %\n",
            "Batch: 56   Training loss: 0.16724462807178497\n",
            "Precision: 56.55 +/- 43.5 %\n",
            "Recall: 16.59 +/- 15.7 %\n",
            "Batch: 57   Training loss: 0.18584215641021729\n",
            "Precision: 77.08 +/- 35.0 %\n",
            "Recall: 21.98 +/- 13.5 %\n",
            "Batch: 58   Training loss: 0.17280042171478271\n",
            "Precision: 54.70 +/- 41.4 %\n",
            "Recall: 20.74 +/- 17.1 %\n",
            "Batch: 59   Training loss: 0.17847202718257904\n",
            "Precision: 68.33 +/- 37.0 %\n",
            "Recall: 26.95 +/- 16.5 %\n",
            "Batch: 60   Training loss: 0.16532041132450104\n",
            "Precision: 60.95 +/- 33.7 %\n",
            "Recall: 25.18 +/- 16.3 %\n",
            "Batch: 61   Training loss: 0.17016854882240295\n",
            "Precision: 60.77 +/- 39.6 %\n",
            "Recall: 20.42 +/- 15.5 %\n",
            "Batch: 62   Training loss: 0.18331435322761536\n",
            "Precision: 70.24 +/- 41.2 %\n",
            "Recall: 15.83 +/- 11.0 %\n",
            "Batch: 63   Training loss: 0.15970122814178467\n",
            "Precision: 65.43 +/- 42.0 %\n",
            "Recall: 19.92 +/- 16.2 %\n",
            "Batch: 64   Training loss: 0.15566052496433258\n",
            "Precision: 71.43 +/- 43.4 %\n",
            "Recall: 19.92 +/- 15.0 %\n",
            "Batch: 65   Training loss: 0.16954299807548523\n",
            "Precision: 66.73 +/- 42.4 %\n",
            "Recall: 20.86 +/- 15.6 %\n",
            "Batch: 66   Training loss: 0.16983474791049957\n",
            "Precision: 70.24 +/- 38.6 %\n",
            "Recall: 25.52 +/- 16.9 %\n",
            "Batch: 67   Training loss: 0.17734138667583466\n",
            "Precision: 64.52 +/- 37.1 %\n",
            "Recall: 23.19 +/- 15.5 %\n",
            "Batch: 68   Training loss: 0.17189261317253113\n",
            "Precision: 57.92 +/- 42.9 %\n",
            "Recall: 19.21 +/- 15.6 %\n",
            "Batch: 69   Training loss: 0.16102555394172668\n",
            "Precision: 64.64 +/- 38.5 %\n",
            "Recall: 23.24 +/- 17.9 %\n",
            "Batch: 70   Training loss: 0.17024677991867065\n",
            "Precision: 70.24 +/- 40.2 %\n",
            "Recall: 19.40 +/- 13.8 %\n",
            "Batch: 71   Training loss: 0.1680251806974411\n",
            "Precision: 66.67 +/- 44.5 %\n",
            "Recall: 16.96 +/- 14.3 %\n",
            "Batch: 72   Training loss: 0.18120916187763214\n",
            "Precision: 68.33 +/- 41.4 %\n",
            "Recall: 19.93 +/- 17.0 %\n",
            "Batch: 73   Training loss: 0.17949730157852173\n",
            "Precision: 68.63 +/- 44.2 %\n",
            "Recall: 17.63 +/- 14.6 %\n",
            "Batch: 74   Training loss: 0.178659588098526\n",
            "Precision: 67.86 +/- 42.2 %\n",
            "Recall: 19.18 +/- 14.2 %\n",
            "Batch: 75   Training loss: 0.17432212829589844\n",
            "Precision: 70.30 +/- 39.9 %\n",
            "Recall: 21.84 +/- 15.9 %\n",
            "Batch: 76   Training loss: 0.15869057178497314\n",
            "Precision: 47.74 +/- 42.4 %\n",
            "Recall: 20.04 +/- 19.0 %\n",
            "Batch: 77   Training loss: 0.1580241322517395\n",
            "Precision: 69.70 +/- 36.2 %\n",
            "Recall: 24.16 +/- 15.0 %\n",
            "Batch: 78   Training loss: 0.170497864484787\n",
            "Precision: 82.86 +/- 26.4 %\n",
            "Recall: 24.90 +/- 12.6 %\n",
            "Batch: 79   Training loss: 0.1736692488193512\n",
            "Precision: 69.52 +/- 41.8 %\n",
            "Recall: 17.09 +/- 12.0 %\n",
            "Batch: 80   Training loss: 0.1762845665216446\n",
            "Precision: 67.62 +/- 36.9 %\n",
            "Recall: 22.03 +/- 14.3 %\n",
            "Batch: 81   Training loss: 0.1791115552186966\n",
            "Precision: 58.93 +/- 37.6 %\n",
            "Recall: 20.75 +/- 16.2 %\n",
            "Batch: 82   Training loss: 0.15916316211223602\n",
            "Precision: 48.21 +/- 32.4 %\n",
            "Recall: 19.43 +/- 14.8 %\n",
            "Batch: 83   Training loss: 0.163257896900177\n",
            "Precision: 54.70 +/- 40.0 %\n",
            "Recall: 24.93 +/- 19.2 %\n",
            "Batch: 84   Training loss: 0.18309161067008972\n",
            "Precision: 61.37 +/- 32.0 %\n",
            "Recall: 26.27 +/- 14.5 %\n",
            "Batch: 85   Training loss: 0.16156111657619476\n",
            "Precision: 68.57 +/- 26.6 %\n",
            "Recall: 33.57 +/- 13.6 %\n",
            "Batch: 86   Training loss: 0.17708028852939606\n",
            "Precision: 55.30 +/- 28.8 %\n",
            "Recall: 28.23 +/- 17.9 %\n",
            "Batch: 87   Training loss: 0.19135673344135284\n",
            "Precision: 66.96 +/- 32.7 %\n",
            "Recall: 28.79 +/- 17.4 %\n",
            "Batch: 88   Training loss: 0.1710772067308426\n",
            "Precision: 62.96 +/- 33.6 %\n",
            "Recall: 29.10 +/- 17.1 %\n",
            "Batch: 89   Training loss: 0.1708349883556366\n",
            "Precision: 58.81 +/- 30.4 %\n",
            "Recall: 26.39 +/- 14.6 %\n",
            "Batch: 90   Training loss: 0.16867083311080933\n",
            "Precision: 61.31 +/- 37.1 %\n",
            "Recall: 26.02 +/- 16.2 %\n",
            "Batch: 91   Training loss: 0.16572286188602448\n",
            "Precision: 52.50 +/- 38.0 %\n",
            "Recall: 20.31 +/- 15.3 %\n",
            "Batch: 92   Training loss: 0.15679168701171875\n",
            "Precision: 57.32 +/- 33.5 %\n",
            "Recall: 24.82 +/- 16.4 %\n",
            "Epoch:  5\n",
            "Batch: 0   Training loss: 0.1914571225643158\n",
            "Precision: 60.12 +/- 32.2 %\n",
            "Recall: 21.65 +/- 13.9 %\n",
            "Batch: 1   Training loss: 0.1637028306722641\n",
            "Precision: 50.00 +/- 35.9 %\n",
            "Recall: 21.29 +/- 16.0 %\n",
            "Batch: 2   Training loss: 0.17201855778694153\n",
            "Precision: 66.37 +/- 36.7 %\n",
            "Recall: 23.97 +/- 14.5 %\n",
            "Batch: 3   Training loss: 0.16367219388484955\n",
            "Precision: 63.99 +/- 37.3 %\n",
            "Recall: 22.95 +/- 17.7 %\n",
            "Batch: 4   Training loss: 0.16668732464313507\n",
            "Precision: 60.42 +/- 40.7 %\n",
            "Recall: 20.98 +/- 16.2 %\n",
            "Batch: 5   Training loss: 0.17368654906749725\n",
            "Precision: 66.73 +/- 35.7 %\n",
            "Recall: 21.15 +/- 13.7 %\n",
            "Batch: 6   Training loss: 0.16595415771007538\n",
            "Precision: 62.16 +/- 31.7 %\n",
            "Recall: 28.26 +/- 15.3 %\n",
            "Batch: 7   Training loss: 0.17319220304489136\n",
            "Precision: 66.49 +/- 28.8 %\n",
            "Recall: 26.18 +/- 15.3 %\n",
            "Batch: 8   Training loss: 0.1862080693244934\n",
            "Precision: 57.62 +/- 40.9 %\n",
            "Recall: 17.56 +/- 15.4 %\n",
            "Batch: 9   Training loss: 0.17273996770381927\n",
            "Precision: 73.57 +/- 36.3 %\n",
            "Recall: 22.44 +/- 12.6 %\n",
            "Batch: 10   Training loss: 0.16188153624534607\n",
            "Precision: 64.58 +/- 38.2 %\n",
            "Recall: 23.88 +/- 15.8 %\n",
            "Batch: 11   Training loss: 0.16937439143657684\n",
            "Precision: 64.88 +/- 35.5 %\n",
            "Recall: 28.34 +/- 20.6 %\n",
            "Batch: 12   Training loss: 0.1751193106174469\n",
            "Precision: 63.39 +/- 40.2 %\n",
            "Recall: 20.43 +/- 14.1 %\n",
            "Batch: 13   Training loss: 0.1729118674993515\n",
            "Precision: 56.85 +/- 40.1 %\n",
            "Recall: 19.99 +/- 15.7 %\n",
            "Batch: 14   Training loss: 0.1583528220653534\n",
            "Precision: 64.58 +/- 33.8 %\n",
            "Recall: 26.89 +/- 17.1 %\n",
            "Batch: 15   Training loss: 0.1689717173576355\n",
            "Precision: 62.68 +/- 35.3 %\n",
            "Recall: 21.63 +/- 13.3 %\n",
            "Batch: 16   Training loss: 0.18603835999965668\n",
            "Precision: 61.43 +/- 39.9 %\n",
            "Recall: 21.07 +/- 16.3 %\n",
            "Batch: 17   Training loss: 0.1794641613960266\n",
            "Precision: 68.39 +/- 32.6 %\n",
            "Recall: 23.69 +/- 14.0 %\n",
            "Batch: 18   Training loss: 0.15853671729564667\n",
            "Precision: 57.02 +/- 42.2 %\n",
            "Recall: 18.55 +/- 16.1 %\n",
            "Batch: 19   Training loss: 0.16863830387592316\n",
            "Precision: 71.13 +/- 36.1 %\n",
            "Recall: 24.06 +/- 15.8 %\n",
            "Batch: 20   Training loss: 0.16118769347667694\n",
            "Precision: 66.37 +/- 37.1 %\n",
            "Recall: 24.74 +/- 16.0 %\n",
            "Batch: 21   Training loss: 0.1826179474592209\n",
            "Precision: 62.38 +/- 31.9 %\n",
            "Recall: 23.33 +/- 14.2 %\n",
            "Batch: 22   Training loss: 0.17570257186889648\n",
            "Precision: 63.58 +/- 35.9 %\n",
            "Recall: 24.10 +/- 19.4 %\n",
            "Batch: 23   Training loss: 0.17890560626983643\n",
            "Precision: 67.26 +/- 36.4 %\n",
            "Recall: 24.54 +/- 15.0 %\n",
            "Batch: 24   Training loss: 0.1709529459476471\n",
            "Precision: 68.33 +/- 39.4 %\n",
            "Recall: 23.51 +/- 14.9 %\n",
            "Batch: 25   Training loss: 0.16743578016757965\n",
            "Precision: 79.35 +/- 20.2 %\n",
            "Recall: 31.53 +/- 11.3 %\n",
            "Batch: 26   Training loss: 0.1497088372707367\n",
            "Precision: 38.45 +/- 38.8 %\n",
            "Recall: 16.29 +/- 16.8 %\n",
            "Batch: 27   Training loss: 0.17208048701286316\n",
            "Precision: 64.23 +/- 33.2 %\n",
            "Recall: 25.02 +/- 14.1 %\n",
            "Batch: 28   Training loss: 0.1682834029197693\n",
            "Precision: 60.12 +/- 43.2 %\n",
            "Recall: 19.65 +/- 16.8 %\n",
            "Batch: 29   Training loss: 0.15559512376785278\n",
            "Precision: 55.95 +/- 44.6 %\n",
            "Recall: 15.55 +/- 14.4 %\n",
            "Batch: 30   Training loss: 0.15788783133029938\n",
            "Precision: 67.62 +/- 34.6 %\n",
            "Recall: 29.87 +/- 21.5 %\n",
            "Batch: 31   Training loss: 0.16582311689853668\n",
            "Precision: 57.02 +/- 40.8 %\n",
            "Recall: 19.46 +/- 15.2 %\n",
            "Batch: 32   Training loss: 0.16770219802856445\n",
            "Precision: 72.20 +/- 35.0 %\n",
            "Recall: 24.97 +/- 14.3 %\n",
            "Batch: 33   Training loss: 0.15651819109916687\n",
            "Precision: 68.93 +/- 40.5 %\n",
            "Recall: 22.09 +/- 17.0 %\n",
            "Batch: 34   Training loss: 0.16791461408138275\n",
            "Precision: 77.74 +/- 37.5 %\n",
            "Recall: 26.70 +/- 16.5 %\n",
            "Batch: 35   Training loss: 0.17173713445663452\n",
            "Precision: 64.64 +/- 37.1 %\n",
            "Recall: 22.90 +/- 15.9 %\n",
            "Batch: 36   Training loss: 0.1879565417766571\n",
            "Precision: 69.11 +/- 40.5 %\n",
            "Recall: 20.11 +/- 14.5 %\n",
            "Batch: 37   Training loss: 0.17425711452960968\n",
            "Precision: 71.85 +/- 38.6 %\n",
            "Recall: 23.25 +/- 16.4 %\n",
            "Batch: 38   Training loss: 0.1718318611383438\n",
            "Precision: 58.69 +/- 45.8 %\n",
            "Recall: 16.79 +/- 16.3 %\n",
            "Batch: 39   Training loss: 0.14849324524402618\n",
            "Precision: 66.25 +/- 39.4 %\n",
            "Recall: 21.85 +/- 14.9 %\n",
            "Batch: 40   Training loss: 0.18303309381008148\n",
            "Precision: 70.24 +/- 35.0 %\n",
            "Recall: 22.39 +/- 16.5 %\n",
            "Batch: 41   Training loss: 0.16865718364715576\n",
            "Precision: 65.77 +/- 36.8 %\n",
            "Recall: 23.08 +/- 16.0 %\n",
            "Batch: 42   Training loss: 0.1561727523803711\n",
            "Precision: 54.17 +/- 38.7 %\n",
            "Recall: 20.84 +/- 16.3 %\n",
            "Batch: 43   Training loss: 0.17637021839618683\n",
            "Precision: 76.67 +/- 35.4 %\n",
            "Recall: 24.11 +/- 15.4 %\n",
            "Batch: 44   Training loss: 0.16106641292572021\n",
            "Precision: 61.79 +/- 35.7 %\n",
            "Recall: 26.58 +/- 18.4 %\n",
            "Batch: 45   Training loss: 0.16668905317783356\n",
            "Precision: 63.75 +/- 38.3 %\n",
            "Recall: 25.49 +/- 17.2 %\n",
            "Batch: 46   Training loss: 0.173438161611557\n",
            "Precision: 65.36 +/- 38.7 %\n",
            "Recall: 24.19 +/- 17.0 %\n",
            "Batch: 47   Training loss: 0.17540545761585236\n",
            "Precision: 62.50 +/- 40.2 %\n",
            "Recall: 24.78 +/- 18.0 %\n",
            "Batch: 48   Training loss: 0.17244869470596313\n",
            "Precision: 53.21 +/- 41.3 %\n",
            "Recall: 18.66 +/- 15.8 %\n",
            "[6,    50] loss: 0.154\n",
            "Batch: 49   Training loss: 0.1537138968706131\n",
            "Precision: 65.42 +/- 39.6 %\n",
            "Recall: 27.32 +/- 19.9 %\n",
            "Batch: 50   Training loss: 0.17015868425369263\n",
            "Precision: 72.08 +/- 35.6 %\n",
            "Recall: 25.27 +/- 14.1 %\n",
            "Batch: 51   Training loss: 0.16668939590454102\n",
            "Precision: 62.44 +/- 37.2 %\n",
            "Recall: 29.82 +/- 21.2 %\n",
            "Batch: 52   Training loss: 0.15797875821590424\n",
            "Precision: 63.95 +/- 36.6 %\n",
            "Recall: 28.28 +/- 16.5 %\n",
            "Batch: 53   Training loss: 0.15793432295322418\n",
            "Precision: 57.92 +/- 39.4 %\n",
            "Recall: 23.29 +/- 18.8 %\n",
            "Batch: 54   Training loss: 0.18615394830703735\n",
            "Precision: 55.48 +/- 42.4 %\n",
            "Recall: 19.13 +/- 17.7 %\n",
            "Batch: 55   Training loss: 0.17040635645389557\n",
            "Precision: 73.69 +/- 37.9 %\n",
            "Recall: 20.64 +/- 13.6 %\n",
            "Batch: 56   Training loss: 0.1700240522623062\n",
            "Precision: 66.96 +/- 38.0 %\n",
            "Recall: 23.74 +/- 16.8 %\n",
            "Batch: 57   Training loss: 0.16706104576587677\n",
            "Precision: 55.71 +/- 42.9 %\n",
            "Recall: 16.54 +/- 14.6 %\n",
            "Batch: 58   Training loss: 0.16355131566524506\n",
            "Precision: 63.99 +/- 41.1 %\n",
            "Recall: 23.29 +/- 17.4 %\n",
            "Batch: 59   Training loss: 0.1764659285545349\n",
            "Precision: 56.01 +/- 42.7 %\n",
            "Recall: 18.91 +/- 16.3 %\n",
            "Batch: 60   Training loss: 0.16662293672561646\n",
            "Precision: 65.06 +/- 42.3 %\n",
            "Recall: 21.87 +/- 15.6 %\n",
            "Batch: 61   Training loss: 0.1857011467218399\n",
            "Precision: 70.54 +/- 39.9 %\n",
            "Recall: 18.71 +/- 12.9 %\n",
            "Batch: 62   Training loss: 0.1737658828496933\n",
            "Precision: 65.48 +/- 43.3 %\n",
            "Recall: 17.73 +/- 15.8 %\n",
            "Batch: 63   Training loss: 0.17996244132518768\n",
            "Precision: 77.98 +/- 37.7 %\n",
            "Recall: 17.92 +/- 13.4 %\n",
            "Batch: 64   Training loss: 0.18639729917049408\n",
            "Precision: 70.24 +/- 42.1 %\n",
            "Recall: 16.49 +/- 14.1 %\n",
            "Batch: 65   Training loss: 0.17592333257198334\n",
            "Precision: 52.62 +/- 46.8 %\n",
            "Recall: 13.42 +/- 14.1 %\n",
            "Batch: 66   Training loss: 0.16133563220500946\n",
            "Precision: 67.14 +/- 44.6 %\n",
            "Recall: 18.91 +/- 15.6 %\n",
            "Batch: 67   Training loss: 0.16902582347393036\n",
            "Precision: 76.19 +/- 36.6 %\n",
            "Recall: 27.04 +/- 15.1 %\n",
            "Batch: 68   Training loss: 0.1655014455318451\n",
            "Precision: 67.02 +/- 41.5 %\n",
            "Recall: 25.00 +/- 17.4 %\n",
            "Batch: 69   Training loss: 0.1722055822610855\n",
            "Precision: 73.87 +/- 33.1 %\n",
            "Recall: 25.72 +/- 15.4 %\n",
            "Batch: 70   Training loss: 0.1693553328514099\n",
            "Precision: 74.88 +/- 38.2 %\n",
            "Recall: 22.18 +/- 14.0 %\n",
            "Batch: 71   Training loss: 0.18404078483581543\n",
            "Precision: 68.21 +/- 40.0 %\n",
            "Recall: 19.97 +/- 15.2 %\n",
            "Batch: 72   Training loss: 0.17488490045070648\n",
            "Precision: 60.06 +/- 43.8 %\n",
            "Recall: 16.63 +/- 14.4 %\n",
            "Batch: 73   Training loss: 0.1792651116847992\n",
            "Precision: 79.17 +/- 32.6 %\n",
            "Recall: 21.96 +/- 12.8 %\n",
            "Batch: 74   Training loss: 0.15704227983951569\n",
            "Precision: 62.50 +/- 40.2 %\n",
            "Recall: 17.18 +/- 13.7 %\n",
            "Batch: 75   Training loss: 0.16412222385406494\n",
            "Precision: 68.15 +/- 40.1 %\n",
            "Recall: 23.06 +/- 16.5 %\n",
            "Batch: 76   Training loss: 0.1781797856092453\n",
            "Precision: 67.74 +/- 39.2 %\n",
            "Recall: 21.09 +/- 14.0 %\n",
            "Batch: 77   Training loss: 0.17038199305534363\n",
            "Precision: 58.04 +/- 40.7 %\n",
            "Recall: 23.30 +/- 17.0 %\n",
            "Batch: 78   Training loss: 0.17788301408290863\n",
            "Precision: 47.62 +/- 37.5 %\n",
            "Recall: 16.03 +/- 13.3 %\n",
            "Batch: 79   Training loss: 0.17362138628959656\n",
            "Precision: 58.99 +/- 36.1 %\n",
            "Recall: 23.28 +/- 15.8 %\n",
            "Batch: 80   Training loss: 0.16342635452747345\n",
            "Precision: 49.29 +/- 37.4 %\n",
            "Recall: 21.52 +/- 16.8 %\n",
            "Batch: 81   Training loss: 0.1730337142944336\n",
            "Precision: 60.89 +/- 40.3 %\n",
            "Recall: 20.51 +/- 14.3 %\n",
            "Batch: 82   Training loss: 0.1653095781803131\n",
            "Precision: 56.61 +/- 40.2 %\n",
            "Recall: 23.10 +/- 19.3 %\n",
            "Batch: 83   Training loss: 0.17644457519054413\n",
            "Precision: 66.25 +/- 41.7 %\n",
            "Recall: 20.56 +/- 15.8 %\n",
            "Batch: 84   Training loss: 0.1715545356273651\n",
            "Precision: 62.02 +/- 36.8 %\n",
            "Recall: 23.87 +/- 15.2 %\n",
            "Batch: 85   Training loss: 0.16806833446025848\n",
            "Precision: 46.79 +/- 40.8 %\n",
            "Recall: 17.84 +/- 17.8 %\n",
            "Batch: 86   Training loss: 0.17915195226669312\n",
            "Precision: 66.73 +/- 37.1 %\n",
            "Recall: 22.23 +/- 14.0 %\n",
            "Batch: 87   Training loss: 0.17403993010520935\n",
            "Precision: 54.35 +/- 40.8 %\n",
            "Recall: 18.79 +/- 16.7 %\n",
            "Batch: 88   Training loss: 0.1687059849500656\n",
            "Precision: 70.25 +/- 41.1 %\n",
            "Recall: 19.43 +/- 14.6 %\n",
            "Batch: 89   Training loss: 0.17698344588279724\n",
            "Precision: 67.86 +/- 42.2 %\n",
            "Recall: 20.05 +/- 14.7 %\n",
            "Batch: 90   Training loss: 0.18200045824050903\n",
            "Precision: 62.81 +/- 40.6 %\n",
            "Recall: 15.94 +/- 11.7 %\n",
            "Batch: 91   Training loss: 0.17830465734004974\n",
            "Precision: 60.17 +/- 38.4 %\n",
            "Recall: 21.89 +/- 17.0 %\n",
            "Batch: 92   Training loss: 0.14948680996894836\n",
            "Precision: 52.90 +/- 44.1 %\n",
            "Recall: 18.36 +/- 16.5 %\n",
            "Epoch:  6\n",
            "Batch: 0   Training loss: 0.14893169701099396\n",
            "Precision: 48.02 +/- 41.3 %\n",
            "Recall: 19.11 +/- 19.0 %\n",
            "Batch: 1   Training loss: 0.15887854993343353\n",
            "Precision: 66.07 +/- 39.7 %\n",
            "Recall: 22.32 +/- 16.2 %\n",
            "Batch: 2   Training loss: 0.15326668322086334\n",
            "Precision: 69.52 +/- 39.6 %\n",
            "Recall: 24.02 +/- 16.4 %\n",
            "Batch: 3   Training loss: 0.16518208384513855\n",
            "Precision: 73.81 +/- 40.4 %\n",
            "Recall: 22.15 +/- 16.8 %\n",
            "Batch: 4   Training loss: 0.16138780117034912\n",
            "Precision: 58.33 +/- 46.0 %\n",
            "Recall: 16.73 +/- 16.8 %\n",
            "Batch: 5   Training loss: 0.16603611409664154\n",
            "Precision: 78.33 +/- 38.0 %\n",
            "Recall: 24.76 +/- 16.3 %\n",
            "Batch: 6   Training loss: 0.17718538641929626\n",
            "Precision: 65.60 +/- 41.6 %\n",
            "Recall: 19.24 +/- 15.0 %\n",
            "Batch: 7   Training loss: 0.17044225335121155\n",
            "Precision: 62.62 +/- 40.8 %\n",
            "Recall: 17.23 +/- 13.6 %\n",
            "Batch: 8   Training loss: 0.16758376359939575\n",
            "Precision: 50.24 +/- 45.2 %\n",
            "Recall: 12.38 +/- 11.6 %\n",
            "Batch: 9   Training loss: 0.18206152319908142\n",
            "Precision: 79.01 +/- 38.6 %\n",
            "Recall: 16.45 +/- 10.7 %\n",
            "Batch: 10   Training loss: 0.17776359617710114\n",
            "Precision: 71.43 +/- 41.5 %\n",
            "Recall: 17.77 +/- 13.2 %\n",
            "Batch: 11   Training loss: 0.17905767261981964\n",
            "Precision: 65.18 +/- 40.5 %\n",
            "Recall: 18.91 +/- 14.0 %\n",
            "Batch: 12   Training loss: 0.16395923495292664\n",
            "Precision: 61.01 +/- 41.0 %\n",
            "Recall: 22.89 +/- 17.4 %\n",
            "Batch: 13   Training loss: 0.19111645221710205\n",
            "Precision: 66.13 +/- 32.8 %\n",
            "Recall: 20.04 +/- 11.3 %\n",
            "Batch: 14   Training loss: 0.15471768379211426\n",
            "Precision: 63.21 +/- 40.9 %\n",
            "Recall: 22.33 +/- 17.1 %\n",
            "Batch: 15   Training loss: 0.17482012510299683\n",
            "Precision: 66.07 +/- 40.4 %\n",
            "Recall: 23.69 +/- 20.5 %\n",
            "Batch: 16   Training loss: 0.14969876408576965\n",
            "Precision: 58.69 +/- 40.5 %\n",
            "Recall: 23.99 +/- 18.5 %\n",
            "Batch: 17   Training loss: 0.18681517243385315\n",
            "Precision: 67.32 +/- 35.0 %\n",
            "Recall: 23.43 +/- 15.3 %\n",
            "Batch: 18   Training loss: 0.17121562361717224\n",
            "Precision: 61.37 +/- 36.1 %\n",
            "Recall: 22.86 +/- 16.8 %\n",
            "Batch: 19   Training loss: 0.16679680347442627\n",
            "Precision: 61.96 +/- 37.8 %\n",
            "Recall: 28.37 +/- 19.8 %\n",
            "Batch: 20   Training loss: 0.16925184428691864\n",
            "Precision: 58.75 +/- 39.1 %\n",
            "Recall: 21.98 +/- 15.2 %\n",
            "Batch: 21   Training loss: 0.1559247225522995\n",
            "Precision: 55.60 +/- 39.4 %\n",
            "Recall: 23.80 +/- 18.5 %\n",
            "Batch: 22   Training loss: 0.1655421257019043\n",
            "Precision: 68.27 +/- 35.8 %\n",
            "Recall: 25.22 +/- 14.1 %\n",
            "Batch: 23   Training loss: 0.17381805181503296\n",
            "Precision: 71.96 +/- 35.0 %\n",
            "Recall: 28.22 +/- 19.1 %\n",
            "Batch: 24   Training loss: 0.16733711957931519\n",
            "Precision: 54.52 +/- 34.5 %\n",
            "Recall: 22.17 +/- 14.8 %\n",
            "Batch: 25   Training loss: 0.17472240328788757\n",
            "Precision: 66.31 +/- 31.1 %\n",
            "Recall: 26.51 +/- 15.8 %\n",
            "Batch: 26   Training loss: 0.1689234972000122\n",
            "Precision: 63.75 +/- 35.7 %\n",
            "Recall: 27.82 +/- 21.3 %\n",
            "Batch: 27   Training loss: 0.16954676806926727\n",
            "Precision: 76.37 +/- 30.5 %\n",
            "Recall: 29.53 +/- 14.3 %\n",
            "Batch: 28   Training loss: 0.16245092451572418\n",
            "Precision: 57.50 +/- 37.3 %\n",
            "Recall: 22.00 +/- 16.7 %\n",
            "Batch: 29   Training loss: 0.18775473535060883\n",
            "Precision: 56.55 +/- 38.7 %\n",
            "Recall: 20.69 +/- 17.1 %\n",
            "Batch: 30   Training loss: 0.15906354784965515\n",
            "Precision: 69.40 +/- 36.5 %\n",
            "Recall: 25.02 +/- 16.4 %\n",
            "Batch: 31   Training loss: 0.16925249993801117\n",
            "Precision: 52.38 +/- 42.8 %\n",
            "Recall: 19.39 +/- 18.8 %\n",
            "Batch: 32   Training loss: 0.15742316842079163\n",
            "Precision: 71.73 +/- 37.5 %\n",
            "Recall: 25.61 +/- 16.2 %\n",
            "Batch: 33   Training loss: 0.1720113307237625\n",
            "Precision: 67.62 +/- 39.0 %\n",
            "Recall: 23.74 +/- 20.1 %\n",
            "Batch: 34   Training loss: 0.17034178972244263\n",
            "Precision: 63.45 +/- 37.3 %\n",
            "Recall: 23.34 +/- 14.8 %\n",
            "Batch: 35   Training loss: 0.17641301453113556\n",
            "Precision: 73.21 +/- 36.0 %\n",
            "Recall: 23.22 +/- 16.2 %\n",
            "Batch: 36   Training loss: 0.17420455813407898\n",
            "Precision: 62.20 +/- 42.8 %\n",
            "Recall: 18.63 +/- 15.4 %\n",
            "Batch: 37   Training loss: 0.1733585000038147\n",
            "Precision: 70.42 +/- 40.4 %\n",
            "Recall: 22.84 +/- 16.3 %\n",
            "Batch: 38   Training loss: 0.16451308131217957\n",
            "Precision: 60.26 +/- 41.5 %\n",
            "Recall: 23.51 +/- 18.8 %\n",
            "Batch: 39   Training loss: 0.16827769577503204\n",
            "Precision: 66.43 +/- 39.8 %\n",
            "Recall: 20.77 +/- 15.3 %\n",
            "Batch: 40   Training loss: 0.15953536331653595\n",
            "Precision: 42.14 +/- 41.8 %\n",
            "Recall: 15.15 +/- 16.3 %\n",
            "Batch: 41   Training loss: 0.160384863615036\n",
            "Precision: 64.94 +/- 43.0 %\n",
            "Recall: 19.18 +/- 16.0 %\n",
            "Batch: 42   Training loss: 0.1682431399822235\n",
            "Precision: 66.01 +/- 44.2 %\n",
            "Recall: 16.29 +/- 12.4 %\n",
            "Batch: 43   Training loss: 0.17057059705257416\n",
            "Precision: 66.01 +/- 40.8 %\n",
            "Recall: 24.30 +/- 21.5 %\n",
            "Batch: 44   Training loss: 0.1648634672164917\n",
            "Precision: 66.13 +/- 38.1 %\n",
            "Recall: 21.45 +/- 14.3 %\n",
            "Batch: 45   Training loss: 0.1622891128063202\n",
            "Precision: 64.66 +/- 37.9 %\n",
            "Recall: 28.17 +/- 18.6 %\n",
            "Batch: 46   Training loss: 0.16606658697128296\n",
            "Precision: 57.50 +/- 36.1 %\n",
            "Recall: 23.81 +/- 17.4 %\n",
            "Batch: 47   Training loss: 0.1779693216085434\n",
            "Precision: 67.50 +/- 36.6 %\n",
            "Recall: 24.23 +/- 15.3 %\n",
            "Batch: 48   Training loss: 0.17824417352676392\n",
            "Precision: 64.96 +/- 40.3 %\n",
            "Recall: 23.53 +/- 16.9 %\n",
            "[7,    50] loss: 0.155\n",
            "Batch: 49   Training loss: 0.1554601937532425\n",
            "Precision: 49.04 +/- 42.1 %\n",
            "Recall: 25.59 +/- 25.7 %\n",
            "Batch: 50   Training loss: 0.17216689884662628\n",
            "Precision: 65.12 +/- 37.7 %\n",
            "Recall: 22.99 +/- 17.1 %\n",
            "Batch: 51   Training loss: 0.15067991614341736\n",
            "Precision: 46.43 +/- 43.7 %\n",
            "Recall: 18.02 +/- 18.3 %\n",
            "Batch: 52   Training loss: 0.1640196293592453\n",
            "Precision: 64.88 +/- 39.9 %\n",
            "Recall: 22.97 +/- 16.4 %\n",
            "Batch: 53   Training loss: 0.1798819750547409\n",
            "Precision: 50.89 +/- 41.3 %\n",
            "Recall: 17.36 +/- 15.3 %\n",
            "Batch: 54   Training loss: 0.18491514027118683\n",
            "Precision: 72.14 +/- 30.1 %\n",
            "Recall: 23.62 +/- 12.4 %\n",
            "Batch: 55   Training loss: 0.16937103867530823\n",
            "Precision: 68.63 +/- 36.7 %\n",
            "Recall: 24.60 +/- 15.0 %\n",
            "Batch: 56   Training loss: 0.16538988053798676\n",
            "Precision: 60.48 +/- 37.7 %\n",
            "Recall: 27.18 +/- 19.9 %\n",
            "Batch: 57   Training loss: 0.17671380937099457\n",
            "Precision: 65.83 +/- 30.8 %\n",
            "Recall: 26.38 +/- 16.0 %\n",
            "Batch: 58   Training loss: 0.18554556369781494\n",
            "Precision: 49.58 +/- 38.9 %\n",
            "Recall: 17.84 +/- 15.0 %\n",
            "Batch: 59   Training loss: 0.17212146520614624\n",
            "Precision: 60.77 +/- 41.9 %\n",
            "Recall: 18.46 +/- 16.6 %\n",
            "Batch: 60   Training loss: 0.16235023736953735\n",
            "Precision: 74.05 +/- 35.0 %\n",
            "Recall: 27.12 +/- 17.1 %\n",
            "Batch: 61   Training loss: 0.17416736483573914\n",
            "Precision: 63.10 +/- 45.7 %\n",
            "Recall: 17.80 +/- 15.6 %\n",
            "Batch: 62   Training loss: 0.17249150574207306\n",
            "Precision: 62.20 +/- 41.6 %\n",
            "Recall: 16.64 +/- 12.4 %\n",
            "Batch: 63   Training loss: 0.1664881408214569\n",
            "Precision: 57.44 +/- 44.5 %\n",
            "Recall: 14.89 +/- 13.5 %\n",
            "Batch: 64   Training loss: 0.17023421823978424\n",
            "Precision: 59.11 +/- 39.5 %\n",
            "Recall: 19.35 +/- 14.9 %\n",
            "Batch: 65   Training loss: 0.17558473348617554\n",
            "Precision: 75.30 +/- 37.4 %\n",
            "Recall: 21.60 +/- 14.2 %\n",
            "Batch: 66   Training loss: 0.17556436359882355\n",
            "Precision: 65.77 +/- 43.2 %\n",
            "Recall: 18.60 +/- 13.2 %\n",
            "Batch: 67   Training loss: 0.1616385281085968\n",
            "Precision: 65.80 +/- 36.5 %\n",
            "Recall: 22.53 +/- 15.1 %\n",
            "Batch: 68   Training loss: 0.16385670006275177\n",
            "Precision: 59.23 +/- 41.3 %\n",
            "Recall: 19.41 +/- 14.9 %\n",
            "Batch: 69   Training loss: 0.15854120254516602\n",
            "Precision: 73.39 +/- 40.9 %\n",
            "Recall: 22.43 +/- 15.7 %\n",
            "Batch: 70   Training loss: 0.1545051783323288\n",
            "Precision: 77.02 +/- 27.4 %\n",
            "Recall: 29.41 +/- 14.2 %\n",
            "Batch: 71   Training loss: 0.15643033385276794\n",
            "Precision: 64.76 +/- 39.9 %\n",
            "Recall: 22.21 +/- 14.5 %\n",
            "Batch: 72   Training loss: 0.16618990898132324\n",
            "Precision: 72.08 +/- 36.2 %\n",
            "Recall: 25.85 +/- 15.8 %\n",
            "Batch: 73   Training loss: 0.17789454758167267\n",
            "Precision: 58.93 +/- 41.7 %\n",
            "Recall: 19.08 +/- 16.5 %\n",
            "Batch: 74   Training loss: 0.17495454847812653\n",
            "Precision: 55.71 +/- 37.2 %\n",
            "Recall: 23.52 +/- 18.9 %\n",
            "Batch: 75   Training loss: 0.18633441627025604\n",
            "Precision: 74.46 +/- 31.3 %\n",
            "Recall: 26.79 +/- 13.1 %\n",
            "Batch: 76   Training loss: 0.15654024481773376\n",
            "Precision: 50.36 +/- 36.1 %\n",
            "Recall: 19.42 +/- 16.2 %\n",
            "Batch: 77   Training loss: 0.17767302691936493\n",
            "Precision: 56.31 +/- 38.4 %\n",
            "Recall: 19.65 +/- 14.7 %\n",
            "Batch: 78   Training loss: 0.1704672873020172\n",
            "Precision: 60.30 +/- 38.6 %\n",
            "Recall: 24.15 +/- 17.3 %\n",
            "Batch: 79   Training loss: 0.17263583838939667\n",
            "Precision: 45.12 +/- 39.7 %\n",
            "Recall: 15.07 +/- 17.2 %\n",
            "Batch: 80   Training loss: 0.1689203977584839\n",
            "Precision: 70.30 +/- 37.8 %\n",
            "Recall: 22.35 +/- 15.2 %\n",
            "Batch: 81   Training loss: 0.16752462089061737\n",
            "Precision: 62.44 +/- 32.9 %\n",
            "Recall: 23.29 +/- 15.4 %\n",
            "Batch: 82   Training loss: 0.18754540383815765\n",
            "Precision: 55.24 +/- 42.6 %\n",
            "Recall: 18.75 +/- 16.3 %\n",
            "Batch: 83   Training loss: 0.17391330003738403\n",
            "Precision: 69.82 +/- 36.1 %\n",
            "Recall: 28.69 +/- 17.4 %\n",
            "Batch: 84   Training loss: 0.17047901451587677\n",
            "Precision: 62.74 +/- 36.9 %\n",
            "Recall: 23.98 +/- 15.9 %\n",
            "Batch: 85   Training loss: 0.17653602361679077\n",
            "Precision: 48.27 +/- 36.6 %\n",
            "Recall: 21.31 +/- 18.1 %\n",
            "Batch: 86   Training loss: 0.17118576169013977\n",
            "Precision: 59.82 +/- 40.2 %\n",
            "Recall: 24.60 +/- 19.2 %\n",
            "Batch: 87   Training loss: 0.16305743157863617\n",
            "Precision: 72.68 +/- 32.8 %\n",
            "Recall: 30.78 +/- 17.1 %\n",
            "Batch: 88   Training loss: 0.17258258163928986\n",
            "Precision: 62.68 +/- 36.0 %\n",
            "Recall: 25.11 +/- 16.0 %\n",
            "Batch: 89   Training loss: 0.15654441714286804\n",
            "Precision: 66.67 +/- 37.5 %\n",
            "Recall: 25.04 +/- 16.7 %\n",
            "Batch: 90   Training loss: 0.18072324991226196\n",
            "Precision: 67.02 +/- 40.6 %\n",
            "Recall: 20.13 +/- 15.1 %\n",
            "Batch: 91   Training loss: 0.17686299979686737\n",
            "Precision: 66.67 +/- 40.6 %\n",
            "Recall: 20.23 +/- 16.4 %\n",
            "Batch: 92   Training loss: 0.19573991000652313\n",
            "Precision: 73.62 +/- 39.8 %\n",
            "Recall: 18.05 +/- 12.7 %\n",
            "Epoch:  7\n",
            "Batch: 0   Training loss: 0.15159179270267487\n",
            "Precision: 55.54 +/- 48.4 %\n",
            "Recall: 16.38 +/- 16.8 %\n",
            "Batch: 1   Training loss: 0.16668154299259186\n",
            "Precision: 60.83 +/- 44.4 %\n",
            "Recall: 19.76 +/- 17.0 %\n",
            "Batch: 2   Training loss: 0.1716609001159668\n",
            "Precision: 61.85 +/- 44.5 %\n",
            "Recall: 18.35 +/- 14.6 %\n",
            "Batch: 3   Training loss: 0.158067524433136\n",
            "Precision: 51.19 +/- 41.1 %\n",
            "Recall: 20.02 +/- 19.0 %\n",
            "Batch: 4   Training loss: 0.1635618656873703\n",
            "Precision: 61.96 +/- 40.2 %\n",
            "Recall: 20.37 +/- 16.6 %\n",
            "Batch: 5   Training loss: 0.1612376868724823\n",
            "Precision: 73.81 +/- 41.2 %\n",
            "Recall: 23.23 +/- 16.5 %\n",
            "Batch: 6   Training loss: 0.16460996866226196\n",
            "Precision: 66.55 +/- 40.3 %\n",
            "Recall: 24.73 +/- 18.5 %\n",
            "Batch: 7   Training loss: 0.1735260933637619\n",
            "Precision: 63.27 +/- 40.2 %\n",
            "Recall: 23.32 +/- 18.3 %\n",
            "Batch: 8   Training loss: 0.1608380228281021\n",
            "Precision: 64.76 +/- 38.5 %\n",
            "Recall: 26.99 +/- 20.4 %\n",
            "Batch: 9   Training loss: 0.1668451726436615\n",
            "Precision: 62.10 +/- 41.9 %\n",
            "Recall: 25.74 +/- 19.2 %\n",
            "Batch: 10   Training loss: 0.19087567925453186\n",
            "Precision: 64.52 +/- 34.8 %\n",
            "Recall: 24.95 +/- 15.9 %\n",
            "Batch: 11   Training loss: 0.17635011672973633\n",
            "Precision: 72.68 +/- 31.3 %\n",
            "Recall: 28.76 +/- 14.6 %\n",
            "Batch: 12   Training loss: 0.1767071783542633\n",
            "Precision: 46.01 +/- 45.4 %\n",
            "Recall: 16.11 +/- 16.8 %\n",
            "Batch: 13   Training loss: 0.18257515132427216\n",
            "Precision: 65.36 +/- 39.8 %\n",
            "Recall: 25.21 +/- 20.4 %\n",
            "Batch: 14   Training loss: 0.17020085453987122\n",
            "Precision: 55.06 +/- 43.9 %\n",
            "Recall: 17.52 +/- 15.8 %\n",
            "Batch: 15   Training loss: 0.1623777449131012\n",
            "Precision: 63.58 +/- 39.9 %\n",
            "Recall: 24.81 +/- 22.0 %\n",
            "Batch: 16   Training loss: 0.17378656566143036\n",
            "Precision: 69.23 +/- 36.4 %\n",
            "Recall: 22.12 +/- 12.3 %\n",
            "Batch: 17   Training loss: 0.17537084221839905\n",
            "Precision: 70.12 +/- 33.7 %\n",
            "Recall: 26.18 +/- 16.5 %\n",
            "Batch: 18   Training loss: 0.1669352650642395\n",
            "Precision: 73.27 +/- 32.6 %\n",
            "Recall: 33.00 +/- 16.3 %\n",
            "Batch: 19   Training loss: 0.173360675573349\n",
            "Precision: 66.49 +/- 28.7 %\n",
            "Recall: 32.17 +/- 19.1 %\n",
            "Batch: 20   Training loss: 0.17903444170951843\n",
            "Precision: 50.83 +/- 37.0 %\n",
            "Recall: 22.37 +/- 16.6 %\n",
            "Batch: 21   Training loss: 0.17271636426448822\n",
            "Precision: 62.76 +/- 36.2 %\n",
            "Recall: 22.98 +/- 16.9 %\n",
            "Batch: 22   Training loss: 0.19539596140384674\n",
            "Precision: 56.68 +/- 40.3 %\n",
            "Recall: 17.37 +/- 14.3 %\n",
            "Batch: 23   Training loss: 0.17095287144184113\n",
            "Precision: 75.48 +/- 31.7 %\n",
            "Recall: 29.68 +/- 17.1 %\n",
            "Batch: 24   Training loss: 0.17484433948993683\n",
            "Precision: 55.42 +/- 43.2 %\n",
            "Recall: 17.91 +/- 15.6 %\n",
            "Batch: 25   Training loss: 0.15548107028007507\n",
            "Precision: 54.82 +/- 39.1 %\n",
            "Recall: 20.33 +/- 15.9 %\n",
            "Batch: 26   Training loss: 0.18507780134677887\n",
            "Precision: 64.88 +/- 39.4 %\n",
            "Recall: 18.30 +/- 13.6 %\n",
            "Batch: 27   Training loss: 0.1750892847776413\n",
            "Precision: 71.31 +/- 30.1 %\n",
            "Recall: 27.14 +/- 15.9 %\n",
            "Batch: 28   Training loss: 0.16736797988414764\n",
            "Precision: 62.32 +/- 36.6 %\n",
            "Recall: 26.73 +/- 15.9 %\n",
            "Batch: 29   Training loss: 0.17055325210094452\n",
            "Precision: 62.20 +/- 34.9 %\n",
            "Recall: 29.40 +/- 17.8 %\n",
            "Batch: 30   Training loss: 0.18657761812210083\n",
            "Precision: 60.83 +/- 35.3 %\n",
            "Recall: 24.29 +/- 17.2 %\n",
            "Batch: 31   Training loss: 0.15719491243362427\n",
            "Precision: 53.99 +/- 42.1 %\n",
            "Recall: 17.76 +/- 16.5 %\n",
            "Batch: 32   Training loss: 0.15942834317684174\n",
            "Precision: 62.56 +/- 40.6 %\n",
            "Recall: 23.86 +/- 17.8 %\n",
            "Batch: 33   Training loss: 0.1603754460811615\n",
            "Precision: 62.50 +/- 40.4 %\n",
            "Recall: 20.45 +/- 16.5 %\n",
            "Batch: 34   Training loss: 0.16239888966083527\n",
            "Precision: 72.86 +/- 36.9 %\n",
            "Recall: 23.52 +/- 14.5 %\n",
            "Batch: 35   Training loss: 0.1827119141817093\n",
            "Precision: 73.75 +/- 35.2 %\n",
            "Recall: 24.45 +/- 16.1 %\n",
            "Batch: 36   Training loss: 0.16298168897628784\n",
            "Precision: 65.30 +/- 38.3 %\n",
            "Recall: 24.28 +/- 17.1 %\n",
            "Batch: 37   Training loss: 0.15851901471614838\n",
            "Precision: 60.00 +/- 36.5 %\n",
            "Recall: 27.77 +/- 19.0 %\n",
            "Batch: 38   Training loss: 0.15724442899227142\n",
            "Precision: 72.38 +/- 37.2 %\n",
            "Recall: 24.10 +/- 15.3 %\n",
            "Batch: 39   Training loss: 0.1680147349834442\n",
            "Precision: 61.55 +/- 41.8 %\n",
            "Recall: 20.48 +/- 16.2 %\n",
            "Batch: 40   Training loss: 0.16747331619262695\n",
            "Precision: 51.79 +/- 45.5 %\n",
            "Recall: 15.19 +/- 14.9 %\n",
            "Batch: 41   Training loss: 0.16148428618907928\n",
            "Precision: 83.93 +/- 32.5 %\n",
            "Recall: 22.47 +/- 13.0 %\n",
            "Batch: 42   Training loss: 0.16383185982704163\n",
            "Precision: 70.30 +/- 42.7 %\n",
            "Recall: 19.26 +/- 14.9 %\n",
            "Batch: 43   Training loss: 0.17491990327835083\n",
            "Precision: 69.70 +/- 39.9 %\n",
            "Recall: 21.85 +/- 16.7 %\n",
            "Batch: 44   Training loss: 0.1783541887998581\n",
            "Precision: 65.12 +/- 41.0 %\n",
            "Recall: 18.92 +/- 14.9 %\n",
            "Batch: 45   Training loss: 0.1731845587491989\n",
            "Precision: 64.64 +/- 35.5 %\n",
            "Recall: 25.56 +/- 17.5 %\n",
            "Batch: 46   Training loss: 0.18053926527500153\n",
            "Precision: 65.18 +/- 36.2 %\n",
            "Recall: 25.00 +/- 17.3 %\n",
            "Batch: 47   Training loss: 0.1676703244447708\n",
            "Precision: 60.77 +/- 31.1 %\n",
            "Recall: 28.16 +/- 16.2 %\n",
            "Batch: 48   Training loss: 0.1594426929950714\n",
            "Precision: 55.18 +/- 38.6 %\n",
            "Recall: 24.32 +/- 19.6 %\n",
            "[8,    50] loss: 0.164\n",
            "Batch: 49   Training loss: 0.16393613815307617\n",
            "Precision: 59.11 +/- 41.4 %\n",
            "Recall: 21.47 +/- 17.7 %\n",
            "Batch: 50   Training loss: 0.17881906032562256\n",
            "Precision: 67.56 +/- 39.5 %\n",
            "Recall: 21.83 +/- 15.4 %\n",
            "Batch: 51   Training loss: 0.1522330492734909\n",
            "Precision: 54.17 +/- 41.9 %\n",
            "Recall: 22.76 +/- 20.3 %\n",
            "Batch: 52   Training loss: 0.19608952105045319\n",
            "Precision: 77.86 +/- 32.0 %\n",
            "Recall: 22.49 +/- 12.7 %\n",
            "Batch: 53   Training loss: 0.17263831198215485\n",
            "Precision: 59.40 +/- 36.3 %\n",
            "Recall: 25.30 +/- 21.8 %\n",
            "Batch: 54   Training loss: 0.15619879961013794\n",
            "Precision: 58.57 +/- 41.9 %\n",
            "Recall: 23.28 +/- 19.1 %\n",
            "Batch: 55   Training loss: 0.18329963088035583\n",
            "Precision: 63.45 +/- 36.4 %\n",
            "Recall: 26.37 +/- 18.1 %\n",
            "Batch: 56   Training loss: 0.1564921736717224\n",
            "Precision: 67.09 +/- 33.8 %\n",
            "Recall: 30.38 +/- 17.9 %\n",
            "Batch: 57   Training loss: 0.16516277194023132\n",
            "Precision: 65.24 +/- 41.7 %\n",
            "Recall: 25.99 +/- 18.6 %\n",
            "Batch: 58   Training loss: 0.17175979912281036\n",
            "Precision: 56.28 +/- 39.0 %\n",
            "Recall: 26.55 +/- 19.5 %\n",
            "Batch: 59   Training loss: 0.17048223316669464\n",
            "Precision: 60.54 +/- 36.7 %\n",
            "Recall: 21.65 +/- 16.0 %\n",
            "Batch: 60   Training loss: 0.15620940923690796\n",
            "Precision: 62.08 +/- 38.5 %\n",
            "Recall: 23.90 +/- 17.2 %\n",
            "Batch: 61   Training loss: 0.1742069274187088\n",
            "Precision: 67.50 +/- 40.1 %\n",
            "Recall: 22.16 +/- 17.8 %\n",
            "Batch: 62   Training loss: 0.15989962220191956\n",
            "Precision: 77.14 +/- 33.1 %\n",
            "Recall: 30.05 +/- 16.8 %\n",
            "Batch: 63   Training loss: 0.15424062311649323\n",
            "Precision: 44.58 +/- 37.0 %\n",
            "Recall: 21.35 +/- 19.9 %\n",
            "Batch: 64   Training loss: 0.1611875742673874\n",
            "Precision: 59.76 +/- 40.9 %\n",
            "Recall: 20.84 +/- 17.2 %\n",
            "Batch: 65   Training loss: 0.15961337089538574\n",
            "Precision: 62.38 +/- 42.9 %\n",
            "Recall: 24.47 +/- 18.5 %\n",
            "Batch: 66   Training loss: 0.16066941618919373\n",
            "Precision: 52.79 +/- 43.4 %\n",
            "Recall: 18.21 +/- 16.6 %\n",
            "Batch: 67   Training loss: 0.17394763231277466\n",
            "Precision: 58.15 +/- 42.5 %\n",
            "Recall: 19.67 +/- 16.8 %\n",
            "Batch: 68   Training loss: 0.16769146919250488\n",
            "Precision: 67.62 +/- 38.1 %\n",
            "Recall: 23.05 +/- 15.1 %\n",
            "Batch: 69   Training loss: 0.17638608813285828\n",
            "Precision: 62.86 +/- 37.2 %\n",
            "Recall: 22.27 +/- 16.8 %\n",
            "Batch: 70   Training loss: 0.18033763766288757\n",
            "Precision: 56.01 +/- 39.3 %\n",
            "Recall: 23.37 +/- 18.0 %\n",
            "Batch: 71   Training loss: 0.17603223025798798\n",
            "Precision: 62.62 +/- 35.4 %\n",
            "Recall: 24.37 +/- 14.8 %\n",
            "Batch: 72   Training loss: 0.17142513394355774\n",
            "Precision: 65.71 +/- 31.2 %\n",
            "Recall: 24.26 +/- 13.9 %\n",
            "Batch: 73   Training loss: 0.16816098988056183\n",
            "Precision: 59.52 +/- 42.9 %\n",
            "Recall: 21.50 +/- 18.1 %\n",
            "Batch: 74   Training loss: 0.17506039142608643\n",
            "Precision: 56.61 +/- 44.6 %\n",
            "Recall: 19.63 +/- 18.3 %\n",
            "Batch: 75   Training loss: 0.17057178914546967\n",
            "Precision: 64.76 +/- 37.1 %\n",
            "Recall: 27.38 +/- 17.9 %\n",
            "Batch: 76   Training loss: 0.15194517374038696\n",
            "Precision: 60.30 +/- 46.2 %\n",
            "Recall: 20.18 +/- 17.7 %\n",
            "Batch: 77   Training loss: 0.16218692064285278\n",
            "Precision: 69.40 +/- 41.4 %\n",
            "Recall: 21.27 +/- 15.9 %\n",
            "Batch: 78   Training loss: 0.16575439274311066\n",
            "Precision: 60.65 +/- 43.6 %\n",
            "Recall: 22.00 +/- 18.9 %\n",
            "Batch: 79   Training loss: 0.16287954151630402\n",
            "Precision: 54.64 +/- 45.3 %\n",
            "Recall: 19.38 +/- 18.8 %\n",
            "Batch: 80   Training loss: 0.15784840285778046\n",
            "Precision: 63.89 +/- 45.9 %\n",
            "Recall: 17.16 +/- 15.1 %\n",
            "Batch: 81   Training loss: 0.15869393944740295\n",
            "Precision: 51.79 +/- 46.1 %\n",
            "Recall: 15.48 +/- 16.9 %\n",
            "Batch: 82   Training loss: 0.1587314009666443\n",
            "Precision: 72.02 +/- 40.6 %\n",
            "Recall: 20.05 +/- 14.8 %\n",
            "Batch: 83   Training loss: 0.16900888085365295\n",
            "Precision: 60.42 +/- 44.2 %\n",
            "Recall: 22.93 +/- 21.0 %\n",
            "Batch: 84   Training loss: 0.16074979305267334\n",
            "Precision: 80.00 +/- 31.2 %\n",
            "Recall: 26.49 +/- 13.7 %\n",
            "Batch: 85   Training loss: 0.1625300496816635\n",
            "Precision: 60.89 +/- 38.0 %\n",
            "Recall: 24.93 +/- 16.5 %\n",
            "Batch: 86   Training loss: 0.17316649854183197\n",
            "Precision: 58.99 +/- 37.3 %\n",
            "Recall: 24.97 +/- 17.1 %\n",
            "Batch: 87   Training loss: 0.17852559685707092\n",
            "Precision: 57.76 +/- 40.2 %\n",
            "Recall: 21.15 +/- 16.3 %\n",
            "Batch: 88   Training loss: 0.173948273062706\n",
            "Precision: 65.65 +/- 40.8 %\n",
            "Recall: 22.37 +/- 16.5 %\n",
            "Batch: 89   Training loss: 0.1623048037290573\n",
            "Precision: 64.64 +/- 38.9 %\n",
            "Recall: 21.53 +/- 14.4 %\n",
            "Batch: 90   Training loss: 0.16277754306793213\n",
            "Precision: 52.08 +/- 43.6 %\n",
            "Recall: 18.14 +/- 17.7 %\n",
            "Batch: 91   Training loss: 0.18157605826854706\n",
            "Precision: 57.02 +/- 43.7 %\n",
            "Recall: 15.02 +/- 13.2 %\n",
            "Batch: 92   Training loss: 0.15043406188488007\n",
            "Precision: 58.55 +/- 45.2 %\n",
            "Recall: 18.06 +/- 16.1 %\n",
            "Epoch:  8\n",
            "Batch: 0   Training loss: 0.1805717498064041\n",
            "Precision: 69.64 +/- 41.3 %\n",
            "Recall: 20.23 +/- 15.3 %\n",
            "Batch: 1   Training loss: 0.1407173126935959\n",
            "Precision: 59.58 +/- 39.8 %\n",
            "Recall: 24.06 +/- 18.5 %\n",
            "Batch: 2   Training loss: 0.16354383528232574\n",
            "Precision: 63.87 +/- 37.6 %\n",
            "Recall: 25.99 +/- 16.7 %\n",
            "Batch: 3   Training loss: 0.19079138338565826\n",
            "Precision: 71.43 +/- 33.2 %\n",
            "Recall: 25.51 +/- 15.5 %\n",
            "Batch: 4   Training loss: 0.17523527145385742\n",
            "Precision: 53.87 +/- 41.2 %\n",
            "Recall: 19.60 +/- 15.0 %\n",
            "Batch: 5   Training loss: 0.16695722937583923\n",
            "Precision: 53.45 +/- 38.8 %\n",
            "Recall: 22.84 +/- 17.6 %\n",
            "Batch: 6   Training loss: 0.17397010326385498\n",
            "Precision: 58.04 +/- 41.8 %\n",
            "Recall: 19.36 +/- 15.6 %\n",
            "Batch: 7   Training loss: 0.17539134621620178\n",
            "Precision: 67.32 +/- 39.6 %\n",
            "Recall: 20.86 +/- 15.5 %\n",
            "Batch: 8   Training loss: 0.16653814911842346\n",
            "Precision: 61.96 +/- 37.3 %\n",
            "Recall: 22.68 +/- 15.6 %\n",
            "Batch: 9   Training loss: 0.17811043560504913\n",
            "Precision: 67.08 +/- 38.1 %\n",
            "Recall: 19.97 +/- 14.4 %\n",
            "Batch: 10   Training loss: 0.16867239773273468\n",
            "Precision: 55.60 +/- 41.0 %\n",
            "Recall: 18.41 +/- 15.6 %\n",
            "Batch: 11   Training loss: 0.1624821275472641\n",
            "Precision: 66.85 +/- 41.5 %\n",
            "Recall: 20.86 +/- 16.8 %\n",
            "Batch: 12   Training loss: 0.14933137595653534\n",
            "Precision: 56.07 +/- 44.3 %\n",
            "Recall: 20.81 +/- 19.6 %\n",
            "Batch: 13   Training loss: 0.16108930110931396\n",
            "Precision: 63.39 +/- 43.1 %\n",
            "Recall: 22.14 +/- 17.4 %\n",
            "Batch: 14   Training loss: 0.15702413022518158\n",
            "Precision: 63.21 +/- 40.2 %\n",
            "Recall: 23.59 +/- 17.2 %\n",
            "Batch: 15   Training loss: 0.15413135290145874\n",
            "Precision: 64.29 +/- 38.7 %\n",
            "Recall: 29.50 +/- 22.7 %\n",
            "Batch: 16   Training loss: 0.1738930493593216\n",
            "Precision: 78.87 +/- 31.8 %\n",
            "Recall: 29.41 +/- 13.8 %\n",
            "Batch: 17   Training loss: 0.15864887833595276\n",
            "Precision: 61.49 +/- 32.5 %\n",
            "Recall: 30.84 +/- 18.7 %\n",
            "Batch: 18   Training loss: 0.1761748045682907\n",
            "Precision: 66.13 +/- 32.5 %\n",
            "Recall: 27.49 +/- 15.3 %\n",
            "Batch: 19   Training loss: 0.17308259010314941\n",
            "Precision: 57.08 +/- 36.7 %\n",
            "Recall: 22.02 +/- 15.6 %\n",
            "Batch: 20   Training loss: 0.181174635887146\n",
            "Precision: 62.80 +/- 35.8 %\n",
            "Recall: 24.41 +/- 15.8 %\n",
            "Batch: 21   Training loss: 0.17417970299720764\n",
            "Precision: 62.02 +/- 39.3 %\n",
            "Recall: 20.55 +/- 15.2 %\n",
            "Batch: 22   Training loss: 0.15537817776203156\n",
            "Precision: 65.83 +/- 43.8 %\n",
            "Recall: 20.80 +/- 16.9 %\n",
            "Batch: 23   Training loss: 0.15589578449726105\n",
            "Precision: 53.63 +/- 44.9 %\n",
            "Recall: 14.30 +/- 13.6 %\n",
            "Batch: 24   Training loss: 0.1725214272737503\n",
            "Precision: 80.65 +/- 32.0 %\n",
            "Recall: 26.58 +/- 16.8 %\n",
            "Batch: 25   Training loss: 0.16069692373275757\n",
            "Precision: 53.57 +/- 43.7 %\n",
            "Recall: 19.80 +/- 17.3 %\n",
            "Batch: 26   Training loss: 0.16299715638160706\n",
            "Precision: 56.31 +/- 40.5 %\n",
            "Recall: 22.96 +/- 19.1 %\n",
            "Batch: 27   Training loss: 0.1767071634531021\n",
            "Precision: 68.57 +/- 35.2 %\n",
            "Recall: 24.00 +/- 14.4 %\n",
            "Batch: 28   Training loss: 0.1630004346370697\n",
            "Precision: 65.18 +/- 34.6 %\n",
            "Recall: 31.32 +/- 20.6 %\n",
            "Batch: 29   Training loss: 0.1731548011302948\n",
            "Precision: 65.24 +/- 37.1 %\n",
            "Recall: 24.58 +/- 16.0 %\n",
            "Batch: 30   Training loss: 0.17292720079421997\n",
            "Precision: 60.42 +/- 40.6 %\n",
            "Recall: 28.32 +/- 21.5 %\n",
            "Batch: 31   Training loss: 0.17463834583759308\n",
            "Precision: 58.69 +/- 39.1 %\n",
            "Recall: 21.02 +/- 18.3 %\n",
            "Batch: 32   Training loss: 0.1638469398021698\n",
            "Precision: 70.77 +/- 34.7 %\n",
            "Recall: 26.86 +/- 16.8 %\n",
            "Batch: 33   Training loss: 0.16690586507320404\n",
            "Precision: 60.00 +/- 37.9 %\n",
            "Recall: 24.33 +/- 19.1 %\n",
            "Batch: 34   Training loss: 0.1513088047504425\n",
            "Precision: 68.10 +/- 38.2 %\n",
            "Recall: 24.76 +/- 16.8 %\n",
            "Batch: 35   Training loss: 0.15956895053386688\n",
            "Precision: 66.96 +/- 35.5 %\n",
            "Recall: 25.29 +/- 15.6 %\n",
            "Batch: 36   Training loss: 0.15489451587200165\n",
            "Precision: 56.31 +/- 41.8 %\n",
            "Recall: 21.76 +/- 19.0 %\n",
            "Batch: 37   Training loss: 0.16270314157009125\n",
            "Precision: 70.71 +/- 30.1 %\n",
            "Recall: 30.02 +/- 14.0 %\n",
            "Batch: 38   Training loss: 0.17578959465026855\n",
            "Precision: 59.05 +/- 36.5 %\n",
            "Recall: 24.89 +/- 16.8 %\n",
            "Batch: 39   Training loss: 0.18122981488704681\n",
            "Precision: 66.61 +/- 31.8 %\n",
            "Recall: 26.12 +/- 14.8 %\n",
            "Batch: 40   Training loss: 0.20008404552936554\n",
            "Precision: 59.29 +/- 34.1 %\n",
            "Recall: 20.28 +/- 12.8 %\n",
            "Batch: 41   Training loss: 0.15183213353157043\n",
            "Precision: 58.27 +/- 37.4 %\n",
            "Recall: 27.16 +/- 19.3 %\n",
            "Batch: 42   Training loss: 0.1719517856836319\n",
            "Precision: 70.42 +/- 35.4 %\n",
            "Recall: 27.18 +/- 16.6 %\n",
            "Batch: 43   Training loss: 0.17033596336841583\n",
            "Precision: 63.57 +/- 39.6 %\n",
            "Recall: 20.48 +/- 13.6 %\n",
            "Batch: 44   Training loss: 0.17324094474315643\n",
            "Precision: 62.08 +/- 38.3 %\n",
            "Recall: 22.68 +/- 19.5 %\n",
            "Batch: 45   Training loss: 0.16206355392932892\n",
            "Precision: 66.13 +/- 36.5 %\n",
            "Recall: 26.61 +/- 17.9 %\n",
            "Batch: 46   Training loss: 0.18279309570789337\n",
            "Precision: 57.44 +/- 45.2 %\n",
            "Recall: 20.51 +/- 18.3 %\n",
            "Batch: 47   Training loss: 0.16419604420661926\n",
            "Precision: 64.46 +/- 33.7 %\n",
            "Recall: 29.90 +/- 18.3 %\n",
            "Batch: 48   Training loss: 0.17304286360740662\n",
            "Precision: 52.03 +/- 36.0 %\n",
            "Recall: 22.25 +/- 16.4 %\n",
            "[9,    50] loss: 0.178\n",
            "Batch: 49   Training loss: 0.17829225957393646\n",
            "Precision: 64.52 +/- 34.8 %\n",
            "Recall: 26.91 +/- 18.0 %\n",
            "Batch: 50   Training loss: 0.16437742114067078\n",
            "Precision: 49.48 +/- 37.8 %\n",
            "Recall: 23.63 +/- 18.2 %\n",
            "Batch: 51   Training loss: 0.1669101119041443\n",
            "Precision: 57.35 +/- 41.0 %\n",
            "Recall: 20.10 +/- 15.4 %\n",
            "Batch: 52   Training loss: 0.15711142122745514\n",
            "Precision: 66.25 +/- 37.8 %\n",
            "Recall: 25.71 +/- 17.7 %\n",
            "Batch: 53   Training loss: 0.16865547001361847\n",
            "Precision: 68.21 +/- 36.7 %\n",
            "Recall: 21.46 +/- 13.5 %\n",
            "Batch: 54   Training loss: 0.1645604521036148\n",
            "Precision: 69.70 +/- 38.0 %\n",
            "Recall: 25.59 +/- 16.9 %\n",
            "Batch: 55   Training loss: 0.16388987004756927\n",
            "Precision: 65.30 +/- 38.4 %\n",
            "Recall: 24.04 +/- 15.7 %\n",
            "Batch: 56   Training loss: 0.15502884984016418\n",
            "Precision: 60.36 +/- 36.1 %\n",
            "Recall: 24.72 +/- 15.8 %\n",
            "Batch: 57   Training loss: 0.18203744292259216\n",
            "Precision: 62.56 +/- 41.3 %\n",
            "Recall: 25.19 +/- 18.4 %\n",
            "Batch: 58   Training loss: 0.14888796210289001\n",
            "Precision: 54.05 +/- 38.7 %\n",
            "Recall: 26.80 +/- 21.6 %\n",
            "Batch: 59   Training loss: 0.15600940585136414\n",
            "Precision: 51.96 +/- 37.5 %\n",
            "Recall: 23.44 +/- 18.5 %\n",
            "Batch: 60   Training loss: 0.181452676653862\n",
            "Precision: 61.19 +/- 39.8 %\n",
            "Recall: 25.37 +/- 17.7 %\n",
            "Batch: 61   Training loss: 0.16119666397571564\n",
            "Precision: 68.26 +/- 33.8 %\n",
            "Recall: 29.87 +/- 17.1 %\n",
            "Batch: 62   Training loss: 0.16517552733421326\n",
            "Precision: 60.43 +/- 36.3 %\n",
            "Recall: 27.50 +/- 19.3 %\n",
            "Batch: 63   Training loss: 0.17747363448143005\n",
            "Precision: 59.29 +/- 41.7 %\n",
            "Recall: 19.70 +/- 15.4 %\n",
            "Batch: 64   Training loss: 0.16022071242332458\n",
            "Precision: 59.86 +/- 41.7 %\n",
            "Recall: 22.15 +/- 18.2 %\n",
            "Batch: 65   Training loss: 0.17413002252578735\n",
            "Precision: 67.86 +/- 36.9 %\n",
            "Recall: 21.39 +/- 16.5 %\n",
            "Batch: 66   Training loss: 0.16150155663490295\n",
            "Precision: 58.63 +/- 42.7 %\n",
            "Recall: 20.96 +/- 17.1 %\n",
            "Batch: 67   Training loss: 0.18035179376602173\n",
            "Precision: 58.81 +/- 38.0 %\n",
            "Recall: 18.54 +/- 14.1 %\n",
            "Batch: 68   Training loss: 0.15844561159610748\n",
            "Precision: 48.99 +/- 39.9 %\n",
            "Recall: 16.99 +/- 15.7 %\n",
            "Batch: 69   Training loss: 0.15087471902370453\n",
            "Precision: 60.12 +/- 36.3 %\n",
            "Recall: 25.41 +/- 16.8 %\n",
            "Batch: 70   Training loss: 0.16301029920578003\n",
            "Precision: 48.87 +/- 41.6 %\n",
            "Recall: 18.52 +/- 18.7 %\n",
            "Batch: 71   Training loss: 0.1724669486284256\n",
            "Precision: 61.61 +/- 41.4 %\n",
            "Recall: 21.72 +/- 16.7 %\n",
            "Batch: 72   Training loss: 0.17182952165603638\n",
            "Precision: 68.26 +/- 36.4 %\n",
            "Recall: 27.25 +/- 17.8 %\n",
            "Batch: 73   Training loss: 0.1580267697572708\n",
            "Precision: 59.82 +/- 34.1 %\n",
            "Recall: 27.14 +/- 18.6 %\n",
            "Batch: 74   Training loss: 0.16768309473991394\n",
            "Precision: 65.00 +/- 34.2 %\n",
            "Recall: 28.51 +/- 16.8 %\n",
            "Batch: 75   Training loss: 0.17459771037101746\n",
            "Precision: 47.23 +/- 37.5 %\n",
            "Recall: 20.94 +/- 18.0 %\n",
            "Batch: 76   Training loss: 0.17931589484214783\n",
            "Precision: 61.19 +/- 41.7 %\n",
            "Recall: 22.42 +/- 17.6 %\n",
            "Batch: 77   Training loss: 0.1677273064851761\n",
            "Batch: 78   Training loss: 0.17318086326122284\n",
            "Batch: 79   Training loss: 0.16592289507389069\n",
            "Batch: 80   Training loss: 0.16532571613788605\n",
            "Batch: 81   Training loss: 0.15531659126281738\n",
            "Precision: 72.32 +/- 37.0 %\n",
            "Recall: 30.37 +/- 17.6 %\n",
            "Batch: 82   Training loss: 0.1684054136276245\n",
            "Precision: 72.80 +/- 34.8 %\n",
            "Recall: 26.87 +/- 16.3 %\n",
            "Batch: 83   Training loss: 0.17238639295101166\n",
            "Precision: 63.10 +/- 42.6 %\n",
            "Recall: 24.96 +/- 19.0 %\n",
            "Batch: 84   Training loss: 0.17514322698116302\n",
            "Precision: 56.96 +/- 41.3 %\n",
            "Recall: 21.35 +/- 17.2 %\n",
            "Batch: 85   Training loss: 0.14938205480575562\n",
            "Precision: 63.33 +/- 41.4 %\n",
            "Recall: 22.99 +/- 17.3 %\n",
            "Batch: 86   Training loss: 0.18318773806095123\n",
            "Precision: 73.87 +/- 32.0 %\n",
            "Recall: 24.56 +/- 15.4 %\n",
            "Batch: 87   Training loss: 0.18785516917705536\n",
            "Precision: 63.10 +/- 39.9 %\n",
            "Recall: 17.45 +/- 12.9 %\n",
            "Batch: 88   Training loss: 0.15830236673355103\n",
            "Precision: 60.30 +/- 45.9 %\n",
            "Recall: 16.80 +/- 14.9 %\n",
            "Batch: 89   Training loss: 0.17616884410381317\n",
            "Precision: 71.90 +/- 39.2 %\n",
            "Recall: 19.95 +/- 15.0 %\n",
            "Batch: 90   Training loss: 0.16458095610141754\n",
            "Precision: 68.99 +/- 42.0 %\n",
            "Recall: 19.61 +/- 15.1 %\n",
            "Batch: 91   Training loss: 0.17015278339385986\n",
            "Precision: 70.18 +/- 38.7 %\n",
            "Recall: 24.55 +/- 15.4 %\n",
            "Batch: 92   Training loss: 0.1617814004421234\n",
            "Precision: 61.81 +/- 42.4 %\n",
            "Recall: 18.20 +/- 14.3 %\n",
            "Epoch:  9\n",
            "Batch: 0   Training loss: 0.1815280169248581\n",
            "Precision: 59.05 +/- 42.5 %\n",
            "Recall: 18.90 +/- 15.7 %\n",
            "Batch: 1   Training loss: 0.16684825718402863\n",
            "Precision: 61.19 +/- 41.7 %\n",
            "Recall: 24.92 +/- 19.6 %\n",
            "Batch: 2   Training loss: 0.1684921383857727\n",
            "Precision: 55.48 +/- 41.8 %\n",
            "Recall: 22.28 +/- 18.3 %\n",
            "Batch: 3   Training loss: 0.16095557808876038\n",
            "Precision: 67.02 +/- 43.4 %\n",
            "Recall: 24.12 +/- 18.5 %\n",
            "Batch: 4   Training loss: 0.15293605625629425\n",
            "Precision: 68.33 +/- 38.5 %\n",
            "Recall: 26.98 +/- 18.6 %\n",
            "Batch: 5   Training loss: 0.16942553222179413\n",
            "Precision: 74.82 +/- 32.7 %\n",
            "Recall: 26.89 +/- 15.1 %\n",
            "Batch: 6   Training loss: 0.1567889004945755\n",
            "Precision: 61.73 +/- 40.0 %\n",
            "Recall: 25.17 +/- 19.2 %\n",
            "Batch: 7   Training loss: 0.16529037058353424\n",
            "Precision: 76.13 +/- 32.0 %\n",
            "Recall: 28.43 +/- 15.9 %\n",
            "Batch: 8   Training loss: 0.15874451398849487\n",
            "Precision: 52.14 +/- 40.0 %\n",
            "Recall: 21.17 +/- 17.6 %\n",
            "Batch: 9   Training loss: 0.1539631336927414\n",
            "Precision: 61.01 +/- 35.4 %\n",
            "Recall: 22.56 +/- 15.1 %\n",
            "Batch: 10   Training loss: 0.15863656997680664\n",
            "Precision: 55.77 +/- 41.8 %\n",
            "Recall: 19.48 +/- 16.8 %\n",
            "Batch: 11   Training loss: 0.17980480194091797\n",
            "Precision: 70.06 +/- 33.7 %\n",
            "Recall: 26.11 +/- 14.2 %\n",
            "Batch: 12   Training loss: 0.1738179326057434\n",
            "Precision: 73.51 +/- 25.9 %\n",
            "Recall: 33.34 +/- 13.3 %\n",
            "Batch: 13   Training loss: 0.15245595574378967\n",
            "Precision: 62.74 +/- 37.1 %\n",
            "Recall: 30.88 +/- 18.5 %\n",
            "Batch: 14   Training loss: 0.18015195429325104\n",
            "Precision: 65.00 +/- 35.3 %\n",
            "Recall: 28.54 +/- 18.2 %\n",
            "Batch: 15   Training loss: 0.1630353182554245\n",
            "Precision: 58.93 +/- 34.4 %\n",
            "Recall: 28.91 +/- 17.9 %\n",
            "Batch: 16   Training loss: 0.16450950503349304\n",
            "Precision: 56.31 +/- 34.9 %\n",
            "Recall: 28.51 +/- 21.9 %\n",
            "Batch: 17   Training loss: 0.1584872305393219\n",
            "Precision: 55.79 +/- 37.9 %\n",
            "Recall: 24.92 +/- 18.3 %\n",
            "Batch: 18   Training loss: 0.179067462682724\n",
            "Precision: 78.81 +/- 29.7 %\n",
            "Recall: 24.93 +/- 14.9 %\n",
            "Batch: 19   Training loss: 0.17393797636032104\n",
            "Precision: 65.54 +/- 36.7 %\n",
            "Recall: 20.42 +/- 13.1 %\n",
            "Batch: 20   Training loss: 0.17253649234771729\n",
            "Precision: 71.25 +/- 36.0 %\n",
            "Recall: 23.83 +/- 14.1 %\n",
            "Batch: 21   Training loss: 0.16792362928390503\n",
            "Precision: 57.20 +/- 40.9 %\n",
            "Recall: 20.95 +/- 16.7 %\n",
            "Batch: 22   Training loss: 0.17946039140224457\n",
            "Precision: 53.63 +/- 38.4 %\n",
            "Recall: 20.40 +/- 16.2 %\n",
            "Batch: 23   Training loss: 0.1511194109916687\n",
            "Precision: 66.13 +/- 33.2 %\n",
            "Recall: 27.95 +/- 17.0 %\n",
            "Batch: 24   Training loss: 0.17944692075252533\n",
            "Precision: 59.17 +/- 35.6 %\n",
            "Recall: 25.03 +/- 16.5 %\n",
            "Batch: 25   Training loss: 0.16714023053646088\n",
            "Precision: 55.36 +/- 41.1 %\n",
            "Recall: 23.73 +/- 19.6 %\n",
            "Batch: 26   Training loss: 0.1664401739835739\n",
            "Precision: 69.52 +/- 31.5 %\n",
            "Recall: 32.00 +/- 21.6 %\n",
            "Batch: 27   Training loss: 0.1730998158454895\n",
            "Precision: 57.62 +/- 42.4 %\n",
            "Recall: 18.62 +/- 16.1 %\n",
            "Batch: 28   Training loss: 0.15131470561027527\n",
            "Precision: 52.35 +/- 41.9 %\n",
            "Recall: 21.23 +/- 20.9 %\n",
            "Batch: 29   Training loss: 0.16319499909877777\n",
            "Precision: 59.10 +/- 45.6 %\n",
            "Recall: 20.12 +/- 19.3 %\n",
            "Batch: 30   Training loss: 0.15400639176368713\n",
            "Precision: 67.02 +/- 39.8 %\n",
            "Recall: 27.08 +/- 19.4 %\n",
            "Batch: 31   Training loss: 0.15381698310375214\n",
            "Precision: 54.05 +/- 39.4 %\n",
            "Recall: 24.47 +/- 21.1 %\n",
            "Batch: 32   Training loss: 0.16351863741874695\n",
            "Precision: 69.29 +/- 34.8 %\n",
            "Recall: 34.17 +/- 17.9 %\n",
            "Batch: 33   Training loss: 0.1595044732093811\n",
            "Precision: 52.38 +/- 43.5 %\n",
            "Recall: 23.87 +/- 21.6 %\n",
            "Batch: 34   Training loss: 0.1694622039794922\n",
            "Precision: 54.24 +/- 42.2 %\n",
            "Recall: 21.04 +/- 18.9 %\n",
            "Batch: 35   Training loss: 0.1622333824634552\n",
            "Batch: 36   Training loss: 0.17411302030086517\n",
            "Batch: 37   Training loss: 0.18611055612564087\n",
            "Batch: 38   Training loss: 0.15454423427581787\n",
            "Batch: 39   Training loss: 0.16407544910907745\n",
            "Batch: 40   Training loss: 0.15196478366851807\n",
            "Batch: 41   Training loss: 0.17086103558540344\n",
            "Batch: 42   Training loss: 0.16406862437725067\n",
            "Precision: 66.61 +/- 35.1 %\n",
            "Recall: 26.90 +/- 17.0 %\n",
            "Batch: 43   Training loss: 0.16727380454540253\n",
            "Precision: 63.27 +/- 37.3 %\n",
            "Recall: 23.58 +/- 16.5 %\n",
            "Batch: 44   Training loss: 0.16304895281791687\n",
            "Precision: 72.92 +/- 30.4 %\n",
            "Recall: 29.71 +/- 14.2 %\n",
            "Batch: 45   Training loss: 0.17660237848758698\n",
            "Batch: 46   Training loss: 0.15872161090373993\n",
            "Batch: 47   Training loss: 0.15860456228256226\n",
            "Batch: 48   Training loss: 0.13957150280475616\n",
            "[10,    50] loss: 0.170\n",
            "Batch: 49   Training loss: 0.17043457925319672\n",
            "Batch: 50   Training loss: 0.17380332946777344\n",
            "Batch: 51   Training loss: 0.1537577211856842\n",
            "Batch: 52   Training loss: 0.17673030495643616\n",
            "Batch: 53   Training loss: 0.168812558054924\n",
            "Batch: 54   Training loss: 0.15879443287849426\n",
            "Precision: 67.38 +/- 35.2 %\n",
            "Recall: 27.07 +/- 14.8 %\n",
            "Batch: 55   Training loss: 0.16340063512325287\n",
            "Precision: 71.51 +/- 35.0 %\n",
            "Recall: 29.79 +/- 18.2 %\n",
            "Batch: 56   Training loss: 0.15437106788158417\n",
            "Precision: 57.62 +/- 39.8 %\n",
            "Recall: 25.14 +/- 17.8 %\n",
            "Batch: 57   Training loss: 0.17325125634670258\n",
            "Batch: 58   Training loss: 0.17858748137950897\n",
            "Precision: 74.52 +/- 37.2 %\n",
            "Recall: 21.47 +/- 13.8 %\n",
            "Batch: 59   Training loss: 0.17391739785671234\n",
            "Batch: 60   Training loss: 0.15698477625846863\n",
            "Batch: 61   Training loss: 0.172721266746521\n",
            "Batch: 62   Training loss: 0.17896680533885956\n",
            "Precision: 76.13 +/- 34.8 %\n",
            "Recall: 24.34 +/- 15.2 %\n",
            "Batch: 63   Training loss: 0.18004533648490906\n",
            "Batch: 64   Training loss: 0.1705387830734253\n",
            "Batch: 65   Training loss: 0.17062030732631683\n",
            "Batch: 66   Training loss: 0.1667470782995224\n",
            "Batch: 67   Training loss: 0.15067817270755768\n",
            "Precision: 64.76 +/- 45.2 %\n",
            "Recall: 23.14 +/- 18.9 %\n",
            "Batch: 68   Training loss: 0.17979185283184052\n",
            "Batch: 69   Training loss: 0.1711379885673523\n",
            "Batch: 70   Training loss: 0.1662786453962326\n",
            "Precision: 58.21 +/- 37.1 %\n",
            "Recall: 28.25 +/- 18.6 %\n",
            "Batch: 71   Training loss: 0.17629669606685638\n",
            "Precision: 48.81 +/- 40.6 %\n",
            "Recall: 19.82 +/- 18.3 %\n",
            "Batch: 72   Training loss: 0.17082646489143372\n",
            "Batch: 73   Training loss: 0.16806086897850037\n",
            "Precision: 56.09 +/- 39.0 %\n",
            "Recall: 20.61 +/- 17.1 %\n",
            "Batch: 74   Training loss: 0.16949325799942017\n",
            "Precision: 67.86 +/- 33.5 %\n",
            "Recall: 22.64 +/- 16.2 %\n",
            "Batch: 75   Training loss: 0.1645430028438568\n",
            "Precision: 54.52 +/- 41.1 %\n",
            "Recall: 20.54 +/- 18.1 %\n",
            "Batch: 76   Training loss: 0.17740599811077118\n",
            "Precision: 62.68 +/- 39.6 %\n",
            "Recall: 19.55 +/- 14.3 %\n",
            "Batch: 77   Training loss: 0.16819265484809875\n",
            "Precision: 60.12 +/- 41.4 %\n",
            "Recall: 21.78 +/- 17.1 %\n",
            "Batch: 78   Training loss: 0.1554974913597107\n",
            "Precision: 54.52 +/- 38.2 %\n",
            "Recall: 25.27 +/- 19.8 %\n",
            "Batch: 79   Training loss: 0.1659855991601944\n",
            "Precision: 62.65 +/- 38.4 %\n",
            "Recall: 25.22 +/- 16.6 %\n",
            "Batch: 80   Training loss: 0.15442031621932983\n",
            "Precision: 60.36 +/- 33.9 %\n",
            "Recall: 28.69 +/- 18.7 %\n",
            "Batch: 81   Training loss: 0.16607840359210968\n",
            "Precision: 61.01 +/- 35.1 %\n",
            "Recall: 27.97 +/- 17.8 %\n",
            "Batch: 82   Training loss: 0.1588243991136551\n",
            "Precision: 61.79 +/- 36.6 %\n",
            "Recall: 27.59 +/- 17.3 %\n",
            "Batch: 83   Training loss: 0.17409799993038177\n",
            "Precision: 59.94 +/- 38.0 %\n",
            "Recall: 27.99 +/- 21.7 %\n",
            "Batch: 84   Training loss: 0.17093618214130402\n",
            "Precision: 69.52 +/- 35.6 %\n",
            "Recall: 25.00 +/- 16.9 %\n",
            "Batch: 85   Training loss: 0.16755126416683197\n",
            "Precision: 64.88 +/- 41.1 %\n",
            "Recall: 22.25 +/- 17.1 %\n",
            "Batch: 86   Training loss: 0.18332859873771667\n",
            "Precision: 57.20 +/- 39.5 %\n",
            "Recall: 20.69 +/- 18.5 %\n",
            "Batch: 87   Training loss: 0.15780670940876007\n",
            "Precision: 59.61 +/- 37.9 %\n",
            "Recall: 27.04 +/- 20.6 %\n",
            "Batch: 88   Training loss: 0.17930011451244354\n",
            "Precision: 67.14 +/- 38.4 %\n",
            "Recall: 23.99 +/- 14.9 %\n",
            "Batch: 89   Training loss: 0.15036872029304504\n",
            "Precision: 74.40 +/- 35.5 %\n",
            "Recall: 29.62 +/- 18.6 %\n",
            "Batch: 90   Training loss: 0.16343358159065247\n",
            "Precision: 55.36 +/- 37.3 %\n",
            "Recall: 23.77 +/- 18.2 %\n",
            "Batch: 91   Training loss: 0.15744604170322418\n",
            "Precision: 66.49 +/- 35.6 %\n",
            "Recall: 28.85 +/- 18.8 %\n",
            "Batch: 92   Training loss: 0.16790153086185455\n",
            "Precision: 50.80 +/- 43.1 %\n",
            "Recall: 17.25 +/- 16.0 %\n",
            "Accuracy: \n",
            "0.696 +/- 0.13\n",
            "\n",
            "Fold:  9\n",
            "Epoch:  0\n",
            "Batch: 0   Training loss: 0.6946783065795898\n",
            "Precision: 6.73 +/- 4.5 %\n",
            "Recall: 39.28 +/- 20.1 %\n",
            "Batch: 1   Training loss: 1.9345862865447998\n",
            "Precision: 8.16 +/- 4.5 %\n",
            "Recall: 36.71 +/- 16.4 %\n",
            "Batch: 2   Training loss: 0.7626453042030334\n",
            "Precision: 6.53 +/- 3.2 %\n",
            "Recall: 43.46 +/- 18.2 %\n",
            "Batch: 3   Training loss: 0.7338929772377014\n",
            "Precision: 6.65 +/- 4.4 %\n",
            "Recall: 33.99 +/- 20.3 %\n",
            "Batch: 4   Training loss: 0.6729466319084167\n",
            "Precision: 8.71 +/- 4.2 %\n",
            "Recall: 47.13 +/- 17.3 %\n",
            "Batch: 5   Training loss: 0.6782655715942383\n",
            "Precision: 7.51 +/- 5.8 %\n",
            "Recall: 36.22 +/- 21.4 %\n",
            "Batch: 6   Training loss: 0.6681400537490845\n",
            "Precision: 7.78 +/- 4.7 %\n",
            "Recall: 33.66 +/- 18.1 %\n",
            "Batch: 7   Training loss: 0.6690595149993896\n",
            "Precision: 7.65 +/- 4.6 %\n",
            "Recall: 40.34 +/- 19.3 %\n",
            "Batch: 8   Training loss: 0.6612097024917603\n",
            "Precision: 7.83 +/- 5.5 %\n",
            "Recall: 34.04 +/- 19.1 %\n",
            "Batch: 9   Training loss: 0.6542990207672119\n",
            "Precision: 9.03 +/- 5.6 %\n",
            "Recall: 44.33 +/- 19.3 %\n",
            "Batch: 10   Training loss: 0.6535767912864685\n",
            "Precision: 6.88 +/- 4.5 %\n",
            "Recall: 34.88 +/- 18.9 %\n",
            "Batch: 11   Training loss: 0.6484490036964417\n",
            "Precision: 5.50 +/- 4.8 %\n",
            "Recall: 24.19 +/- 18.7 %\n",
            "Batch: 12   Training loss: 0.6471549272537231\n",
            "Precision: 6.16 +/- 4.3 %\n",
            "Recall: 32.64 +/- 23.7 %\n",
            "Batch: 13   Training loss: 0.6412875652313232\n",
            "Precision: 9.45 +/- 6.6 %\n",
            "Recall: 34.19 +/- 19.1 %\n",
            "Batch: 14   Training loss: 0.6423766613006592\n",
            "Precision: 8.00 +/- 5.4 %\n",
            "Recall: 29.65 +/- 20.9 %\n",
            "Batch: 15   Training loss: 0.6322510242462158\n",
            "Precision: 6.53 +/- 4.4 %\n",
            "Recall: 29.82 +/- 20.6 %\n",
            "Batch: 16   Training loss: 0.6279090046882629\n",
            "Precision: 6.76 +/- 4.0 %\n",
            "Recall: 30.66 +/- 17.6 %\n",
            "Batch: 17   Training loss: 0.6246113777160645\n",
            "Precision: 7.27 +/- 4.5 %\n",
            "Recall: 28.65 +/- 14.3 %\n",
            "Batch: 18   Training loss: 0.6194073557853699\n",
            "Precision: 7.97 +/- 4.9 %\n",
            "Recall: 30.01 +/- 15.1 %\n",
            "Batch: 19   Training loss: 0.6171415448188782\n",
            "Precision: 8.16 +/- 4.8 %\n",
            "Recall: 28.12 +/- 13.3 %\n",
            "Batch: 20   Training loss: 0.6104568839073181\n",
            "Precision: 7.64 +/- 6.0 %\n",
            "Recall: 29.03 +/- 20.5 %\n",
            "Batch: 21   Training loss: 0.6032267808914185\n",
            "Precision: 7.06 +/- 5.3 %\n",
            "Recall: 27.84 +/- 20.3 %\n",
            "Batch: 22   Training loss: 0.6027147173881531\n",
            "Precision: 7.03 +/- 4.6 %\n",
            "Recall: 30.16 +/- 20.2 %\n",
            "Batch: 23   Training loss: 0.5952737927436829\n",
            "Precision: 5.87 +/- 4.9 %\n",
            "Recall: 21.31 +/- 16.6 %\n",
            "Batch: 24   Training loss: 0.5893341898918152\n",
            "Precision: 6.00 +/- 5.5 %\n",
            "Recall: 18.08 +/- 13.5 %\n",
            "Batch: 25   Training loss: 0.5795753002166748\n",
            "Precision: 6.96 +/- 4.4 %\n",
            "Recall: 25.38 +/- 17.9 %\n",
            "Batch: 26   Training loss: 0.5869300961494446\n",
            "Precision: 8.34 +/- 5.4 %\n",
            "Recall: 25.85 +/- 15.0 %\n",
            "Batch: 27   Training loss: 0.5672840476036072\n",
            "Precision: 5.11 +/- 4.1 %\n",
            "Recall: 24.89 +/- 24.2 %\n",
            "Batch: 28   Training loss: 0.5742707848548889\n",
            "Precision: 6.71 +/- 4.0 %\n",
            "Recall: 21.03 +/- 11.6 %\n",
            "Batch: 29   Training loss: 0.5678033828735352\n",
            "Precision: 8.57 +/- 5.5 %\n",
            "Recall: 24.25 +/- 18.0 %\n",
            "Batch: 30   Training loss: 0.5641043782234192\n",
            "Precision: 7.62 +/- 5.5 %\n",
            "Recall: 18.53 +/- 10.1 %\n",
            "Batch: 31   Training loss: 0.5509685277938843\n",
            "Precision: 7.95 +/- 6.0 %\n",
            "Recall: 21.09 +/- 11.4 %\n",
            "Batch: 32   Training loss: 0.5411157608032227\n",
            "Precision: 7.27 +/- 5.5 %\n",
            "Recall: 20.05 +/- 11.3 %\n",
            "Batch: 33   Training loss: 0.542052686214447\n",
            "Precision: 9.10 +/- 6.2 %\n",
            "Recall: 27.55 +/- 22.9 %\n",
            "Batch: 34   Training loss: 0.5443625450134277\n",
            "Precision: 7.73 +/- 4.2 %\n",
            "Recall: 20.40 +/- 10.5 %\n",
            "Batch: 35   Training loss: 0.5192920565605164\n",
            "Precision: 8.53 +/- 7.0 %\n",
            "Recall: 21.13 +/- 15.3 %\n",
            "Batch: 36   Training loss: 0.5204852819442749\n",
            "Precision: 8.49 +/- 6.4 %\n",
            "Recall: 22.79 +/- 20.1 %\n",
            "Batch: 37   Training loss: 0.5226560235023499\n",
            "Precision: 9.06 +/- 6.0 %\n",
            "Recall: 21.15 +/- 18.2 %\n",
            "Batch: 38   Training loss: 0.5050156712532043\n",
            "Precision: 6.14 +/- 5.4 %\n",
            "Recall: 18.12 +/- 19.6 %\n",
            "Batch: 39   Training loss: 0.5059816837310791\n",
            "Precision: 7.85 +/- 5.4 %\n",
            "Recall: 20.47 +/- 18.3 %\n",
            "Batch: 40   Training loss: 0.4939139187335968\n",
            "Precision: 5.86 +/- 3.9 %\n",
            "Recall: 14.93 +/- 12.0 %\n",
            "Batch: 41   Training loss: 0.48400771617889404\n",
            "Precision: 7.37 +/- 5.7 %\n",
            "Recall: 14.17 +/- 10.0 %\n",
            "Batch: 42   Training loss: 0.4779416024684906\n",
            "Precision: 6.40 +/- 5.9 %\n",
            "Recall: 11.12 +/- 9.8 %\n",
            "Batch: 43   Training loss: 0.46518513560295105\n",
            "Precision: 7.29 +/- 6.5 %\n",
            "Recall: 13.97 +/- 12.2 %\n",
            "Batch: 44   Training loss: 0.464800089597702\n",
            "Precision: 5.97 +/- 5.9 %\n",
            "Recall: 15.41 +/- 20.5 %\n",
            "Batch: 45   Training loss: 0.459016352891922\n",
            "Precision: 6.23 +/- 4.9 %\n",
            "Recall: 14.14 +/- 9.9 %\n",
            "Batch: 46   Training loss: 0.44969022274017334\n",
            "Precision: 6.18 +/- 4.8 %\n",
            "Recall: 11.46 +/- 7.8 %\n",
            "Batch: 47   Training loss: 0.4403376877307892\n",
            "Precision: 7.87 +/- 5.7 %\n",
            "Recall: 13.30 +/- 9.2 %\n",
            "Batch: 48   Training loss: 0.4347948431968689\n",
            "Precision: 6.06 +/- 5.3 %\n",
            "Recall: 10.21 +/- 9.2 %\n",
            "[1,    50] loss: 0.427\n",
            "Batch: 49   Training loss: 0.4267277717590332\n",
            "Precision: 7.34 +/- 5.4 %\n",
            "Recall: 12.50 +/- 9.4 %\n",
            "Batch: 50   Training loss: 0.4219900667667389\n",
            "Precision: 10.06 +/- 6.9 %\n",
            "Recall: 19.30 +/- 15.2 %\n",
            "Batch: 51   Training loss: 0.4119543731212616\n",
            "Precision: 6.45 +/- 6.1 %\n",
            "Recall: 12.09 +/- 18.9 %\n",
            "Batch: 52   Training loss: 0.3991577923297882\n",
            "Precision: 7.64 +/- 6.7 %\n",
            "Recall: 10.70 +/- 8.7 %\n",
            "Batch: 53   Training loss: 0.4044373035430908\n",
            "Precision: 11.56 +/- 6.4 %\n",
            "Recall: 20.41 +/- 18.9 %\n",
            "Batch: 54   Training loss: 0.4005124270915985\n",
            "Precision: 11.64 +/- 6.5 %\n",
            "Recall: 17.38 +/- 9.4 %\n",
            "Batch: 55   Training loss: 0.39439189434051514\n",
            "Precision: 12.08 +/- 7.9 %\n",
            "Recall: 20.49 +/- 18.0 %\n",
            "Batch: 56   Training loss: 0.38033926486968994\n",
            "Precision: 11.26 +/- 8.4 %\n",
            "Recall: 19.78 +/- 19.4 %\n",
            "Batch: 57   Training loss: 0.3885801434516907\n",
            "Precision: 12.86 +/- 7.2 %\n",
            "Recall: 17.86 +/- 8.7 %\n",
            "Batch: 58   Training loss: 0.37931254506111145\n",
            "Precision: 13.01 +/- 6.6 %\n",
            "Recall: 16.97 +/- 7.9 %\n",
            "Batch: 59   Training loss: 0.3765445649623871\n",
            "Precision: 18.52 +/- 9.3 %\n",
            "Recall: 20.19 +/- 7.5 %\n",
            "Batch: 60   Training loss: 0.36809930205345154\n",
            "Precision: 17.90 +/- 10.9 %\n",
            "Recall: 21.26 +/- 11.2 %\n",
            "Batch: 61   Training loss: 0.361564040184021\n",
            "Precision: 19.76 +/- 10.8 %\n",
            "Recall: 21.94 +/- 11.7 %\n",
            "Batch: 62   Training loss: 0.335626482963562\n",
            "Precision: 15.70 +/- 10.4 %\n",
            "Recall: 21.59 +/- 11.9 %\n",
            "Batch: 63   Training loss: 0.32500600814819336\n",
            "Precision: 16.82 +/- 11.4 %\n",
            "Recall: 28.73 +/- 23.6 %\n",
            "Batch: 64   Training loss: 0.32685744762420654\n",
            "Precision: 19.18 +/- 9.2 %\n",
            "Recall: 21.80 +/- 11.3 %\n",
            "Batch: 65   Training loss: 0.3148230016231537\n",
            "Precision: 14.94 +/- 9.8 %\n",
            "Recall: 16.83 +/- 11.6 %\n",
            "Batch: 66   Training loss: 0.32815662026405334\n",
            "Precision: 17.32 +/- 11.6 %\n",
            "Recall: 18.24 +/- 11.6 %\n",
            "Batch: 67   Training loss: 0.30636832118034363\n",
            "Precision: 13.64 +/- 11.0 %\n",
            "Recall: 15.43 +/- 10.6 %\n",
            "Batch: 68   Training loss: 0.30179327726364136\n",
            "Precision: 18.24 +/- 11.6 %\n",
            "Recall: 21.94 +/- 11.4 %\n",
            "Batch: 69   Training loss: 0.30123603343963623\n",
            "Precision: 18.86 +/- 11.3 %\n",
            "Recall: 26.11 +/- 19.1 %\n",
            "Batch: 70   Training loss: 0.2915526330471039\n",
            "Precision: 17.31 +/- 11.9 %\n",
            "Recall: 17.80 +/- 12.1 %\n",
            "Batch: 71   Training loss: 0.289335697889328\n",
            "Precision: 19.77 +/- 10.3 %\n",
            "Recall: 15.55 +/- 6.9 %\n",
            "Batch: 72   Training loss: 0.29291895031929016\n",
            "Precision: 18.88 +/- 12.7 %\n",
            "Recall: 13.61 +/- 9.8 %\n",
            "Batch: 73   Training loss: 0.29641231894493103\n",
            "Precision: 25.34 +/- 11.0 %\n",
            "Recall: 15.13 +/- 5.1 %\n",
            "Batch: 74   Training loss: 0.26798635721206665\n",
            "Precision: 20.46 +/- 12.5 %\n",
            "Recall: 19.74 +/- 18.0 %\n",
            "Batch: 75   Training loss: 0.2653455138206482\n",
            "Precision: 19.90 +/- 12.3 %\n",
            "Recall: 15.95 +/- 7.7 %\n",
            "Batch: 76   Training loss: 0.27926427125930786\n",
            "Precision: 19.30 +/- 13.3 %\n",
            "Recall: 12.43 +/- 7.9 %\n",
            "Batch: 77   Training loss: 0.2719830274581909\n",
            "Precision: 24.57 +/- 17.7 %\n",
            "Recall: 13.16 +/- 8.8 %\n",
            "Batch: 78   Training loss: 0.25032249093055725\n",
            "Precision: 29.19 +/- 18.2 %\n",
            "Recall: 22.02 +/- 18.7 %\n",
            "Batch: 79   Training loss: 0.2620678246021271\n",
            "Precision: 30.12 +/- 13.2 %\n",
            "Recall: 21.68 +/- 11.2 %\n",
            "Batch: 80   Training loss: 0.2788042426109314\n",
            "Precision: 26.96 +/- 14.1 %\n",
            "Recall: 14.82 +/- 7.2 %\n",
            "Batch: 81   Training loss: 0.25679752230644226\n",
            "Precision: 20.60 +/- 14.7 %\n",
            "Recall: 11.44 +/- 7.6 %\n",
            "Batch: 82   Training loss: 0.2440028190612793\n",
            "Precision: 26.19 +/- 18.7 %\n",
            "Recall: 13.04 +/- 8.5 %\n",
            "Batch: 83   Training loss: 0.24946652352809906\n",
            "Precision: 28.93 +/- 17.6 %\n",
            "Recall: 15.47 +/- 8.8 %\n",
            "Batch: 84   Training loss: 0.23448768258094788\n",
            "Precision: 30.24 +/- 19.5 %\n",
            "Recall: 14.74 +/- 9.2 %\n",
            "Batch: 85   Training loss: 0.24019789695739746\n",
            "Precision: 35.83 +/- 25.0 %\n",
            "Recall: 12.51 +/- 8.5 %\n",
            "Batch: 86   Training loss: 0.23887582123279572\n",
            "Precision: 30.95 +/- 21.2 %\n",
            "Recall: 8.85 +/- 6.3 %\n",
            "Batch: 87   Training loss: 0.22086140513420105\n",
            "Precision: 27.38 +/- 20.0 %\n",
            "Recall: 9.27 +/- 7.3 %\n",
            "Batch: 88   Training loss: 0.23144839704036713\n",
            "Precision: 30.95 +/- 25.1 %\n",
            "Recall: 7.75 +/- 5.9 %\n",
            "Batch: 89   Training loss: 0.2191552072763443\n",
            "Precision: 41.73 +/- 19.3 %\n",
            "Recall: 14.30 +/- 6.7 %\n",
            "Batch: 90   Training loss: 0.2042837291955948\n",
            "Precision: 35.71 +/- 16.7 %\n",
            "Recall: 18.38 +/- 6.9 %\n",
            "Batch: 91   Training loss: 0.2109961360692978\n",
            "Precision: 36.07 +/- 22.5 %\n",
            "Recall: 15.54 +/- 9.0 %\n",
            "Batch: 92   Training loss: 0.22223412990570068\n",
            "Precision: 37.32 +/- 22.6 %\n",
            "Recall: 11.96 +/- 7.4 %\n",
            "Epoch:  1\n",
            "Batch: 0   Training loss: 0.2155754119157791\n",
            "Precision: 33.04 +/- 23.2 %\n",
            "Recall: 10.11 +/- 7.4 %\n",
            "Batch: 1   Training loss: 0.21528169512748718\n",
            "Precision: 57.14 +/- 37.1 %\n",
            "Recall: 11.68 +/- 7.4 %\n",
            "Batch: 2   Training loss: 0.23019801080226898\n",
            "Precision: 46.43 +/- 35.8 %\n",
            "Recall: 9.74 +/- 8.1 %\n",
            "Batch: 3   Training loss: 0.20453417301177979\n",
            "Precision: 47.02 +/- 28.2 %\n",
            "Recall: 12.50 +/- 10.3 %\n",
            "Batch: 4   Training loss: 0.21550720930099487\n",
            "Precision: 53.51 +/- 35.2 %\n",
            "Recall: 20.49 +/- 16.3 %\n",
            "Batch: 5   Training loss: 0.2092219591140747\n",
            "Precision: 62.80 +/- 28.7 %\n",
            "Recall: 23.44 +/- 12.4 %\n",
            "Batch: 6   Training loss: 0.18745848536491394\n",
            "Precision: 64.29 +/- 33.5 %\n",
            "Recall: 21.81 +/- 14.5 %\n",
            "Batch: 7   Training loss: 0.20603615045547485\n",
            "Precision: 57.74 +/- 35.2 %\n",
            "Recall: 17.30 +/- 12.6 %\n",
            "Batch: 8   Training loss: 0.1978643238544464\n",
            "Precision: 67.26 +/- 32.9 %\n",
            "Recall: 19.79 +/- 11.1 %\n",
            "Batch: 9   Training loss: 0.215484619140625\n",
            "Precision: 62.20 +/- 41.6 %\n",
            "Recall: 16.55 +/- 13.6 %\n",
            "Batch: 10   Training loss: 0.19347476959228516\n",
            "Precision: 53.57 +/- 34.9 %\n",
            "Recall: 18.43 +/- 15.5 %\n",
            "Batch: 11   Training loss: 0.17348125576972961\n",
            "Precision: 53.87 +/- 43.2 %\n",
            "Recall: 13.95 +/- 13.3 %\n",
            "Batch: 12   Training loss: 0.19261659681797028\n",
            "Precision: 68.75 +/- 39.1 %\n",
            "Recall: 19.71 +/- 13.6 %\n",
            "Batch: 13   Training loss: 0.1876102089881897\n",
            "Precision: 69.35 +/- 27.4 %\n",
            "Recall: 30.25 +/- 13.4 %\n",
            "Batch: 14   Training loss: 0.19160877168178558\n",
            "Precision: 63.21 +/- 29.7 %\n",
            "Recall: 30.94 +/- 15.1 %\n",
            "Batch: 15   Training loss: 0.19202843308448792\n",
            "Precision: 68.40 +/- 34.2 %\n",
            "Recall: 27.07 +/- 14.4 %\n",
            "Batch: 16   Training loss: 0.20093666017055511\n",
            "Precision: 77.26 +/- 26.0 %\n",
            "Recall: 25.31 +/- 10.3 %\n",
            "Batch: 17   Training loss: 0.18455536663532257\n",
            "Precision: 66.67 +/- 37.8 %\n",
            "Recall: 20.95 +/- 13.9 %\n",
            "Batch: 18   Training loss: 0.17666193842887878\n",
            "Precision: 64.29 +/- 45.4 %\n",
            "Recall: 18.62 +/- 16.5 %\n",
            "Batch: 19   Training loss: 0.17438578605651855\n",
            "Precision: 53.57 +/- 49.9 %\n",
            "Recall: 14.87 +/- 17.2 %\n",
            "Batch: 20   Training loss: 0.20226077735424042\n",
            "Precision: 75.00 +/- 43.3 %\n",
            "Recall: 13.54 +/- 8.9 %\n",
            "Batch: 21   Training loss: 0.19929726421833038\n",
            "Precision: 83.33 +/- 35.1 %\n",
            "Recall: 18.05 +/- 9.7 %\n",
            "Batch: 22   Training loss: 0.18357329070568085\n",
            "Precision: 61.43 +/- 35.2 %\n",
            "Recall: 22.23 +/- 14.6 %\n",
            "Batch: 23   Training loss: 0.1827644258737564\n",
            "Precision: 53.51 +/- 32.9 %\n",
            "Recall: 27.29 +/- 17.3 %\n",
            "Batch: 24   Training loss: 0.183933287858963\n",
            "Precision: 56.31 +/- 34.0 %\n",
            "Recall: 31.33 +/- 18.1 %\n",
            "Batch: 25   Training loss: 0.17690278589725494\n",
            "Precision: 47.62 +/- 42.2 %\n",
            "Recall: 17.31 +/- 17.5 %\n",
            "Batch: 26   Training loss: 0.19953396916389465\n",
            "Precision: 61.90 +/- 47.7 %\n",
            "Recall: 10.95 +/- 8.7 %\n",
            "Batch: 27   Training loss: 0.17271080613136292\n",
            "Precision: 73.81 +/- 40.2 %\n",
            "Recall: 22.17 +/- 15.9 %\n",
            "Batch: 28   Training loss: 0.1850932091474533\n",
            "Precision: 66.67 +/- 34.5 %\n",
            "Recall: 20.34 +/- 13.3 %\n",
            "Batch: 29   Training loss: 0.1728929579257965\n",
            "Precision: 66.67 +/- 32.1 %\n",
            "Recall: 23.95 +/- 16.0 %\n",
            "Batch: 30   Training loss: 0.1857077032327652\n",
            "Precision: 59.52 +/- 36.2 %\n",
            "Recall: 20.82 +/- 16.1 %\n",
            "Batch: 31   Training loss: 0.1844460666179657\n",
            "Precision: 74.94 +/- 34.1 %\n",
            "Recall: 26.03 +/- 11.9 %\n",
            "Batch: 32   Training loss: 0.1893722265958786\n",
            "Precision: 74.94 +/- 27.6 %\n",
            "Recall: 25.71 +/- 13.4 %\n",
            "Batch: 33   Training loss: 0.18039216101169586\n",
            "Precision: 75.00 +/- 38.4 %\n",
            "Recall: 17.59 +/- 10.3 %\n",
            "Batch: 34   Training loss: 0.1714751124382019\n",
            "Precision: 77.38 +/- 40.9 %\n",
            "Recall: 21.24 +/- 16.0 %\n",
            "Batch: 35   Training loss: 0.18454080820083618\n",
            "Precision: 77.38 +/- 37.9 %\n",
            "Recall: 17.88 +/- 12.1 %\n",
            "Batch: 36   Training loss: 0.1907305121421814\n",
            "Precision: 66.67 +/- 40.6 %\n",
            "Recall: 17.88 +/- 11.6 %\n",
            "Batch: 37   Training loss: 0.1674572378396988\n",
            "Precision: 64.23 +/- 38.9 %\n",
            "Recall: 26.09 +/- 17.4 %\n",
            "Batch: 38   Training loss: 0.21397171914577484\n",
            "Precision: 60.05 +/- 38.6 %\n",
            "Recall: 19.30 +/- 12.7 %\n",
            "Batch: 39   Training loss: 0.17847366631031036\n",
            "Precision: 73.33 +/- 34.7 %\n",
            "Recall: 24.07 +/- 12.9 %\n",
            "Batch: 40   Training loss: 0.16860412061214447\n",
            "Precision: 87.08 +/- 30.8 %\n",
            "Recall: 26.76 +/- 15.2 %\n",
            "Batch: 41   Training loss: 0.18986113369464874\n",
            "Precision: 62.50 +/- 35.0 %\n",
            "Recall: 18.33 +/- 11.9 %\n",
            "Batch: 42   Training loss: 0.16681845486164093\n",
            "Precision: 76.19 +/- 28.0 %\n",
            "Recall: 25.21 +/- 15.3 %\n",
            "Batch: 43   Training loss: 0.1726200133562088\n",
            "Precision: 63.69 +/- 40.3 %\n",
            "Recall: 17.12 +/- 12.7 %\n",
            "Batch: 44   Training loss: 0.1704646199941635\n",
            "Precision: 61.50 +/- 41.7 %\n",
            "Recall: 26.10 +/- 18.7 %\n",
            "Batch: 45   Training loss: 0.17016901075839996\n",
            "Precision: 58.69 +/- 36.2 %\n",
            "Recall: 29.15 +/- 18.0 %\n",
            "Batch: 46   Training loss: 0.18178166449069977\n",
            "Precision: 53.33 +/- 40.0 %\n",
            "Recall: 23.94 +/- 19.4 %\n",
            "Batch: 47   Training loss: 0.16676735877990723\n",
            "Precision: 57.02 +/- 38.5 %\n",
            "Recall: 20.43 +/- 13.8 %\n",
            "Batch: 48   Training loss: 0.17972667515277863\n",
            "Precision: 72.92 +/- 37.7 %\n",
            "Recall: 22.35 +/- 14.0 %\n",
            "[2,    50] loss: 0.183\n",
            "Batch: 49   Training loss: 0.18279527127742767\n",
            "Precision: 65.48 +/- 45.8 %\n",
            "Recall: 17.05 +/- 14.4 %\n",
            "Batch: 50   Training loss: 0.1661236435174942\n",
            "Precision: 67.86 +/- 44.1 %\n",
            "Recall: 20.79 +/- 16.8 %\n",
            "Batch: 51   Training loss: 0.19297575950622559\n",
            "Precision: 78.57 +/- 38.0 %\n",
            "Recall: 18.07 +/- 11.7 %\n",
            "Batch: 52   Training loss: 0.1796473264694214\n",
            "Precision: 58.33 +/- 47.7 %\n",
            "Recall: 11.67 +/- 11.5 %\n",
            "Batch: 53   Training loss: 0.1828729510307312\n",
            "Precision: 68.27 +/- 40.1 %\n",
            "Recall: 21.56 +/- 15.3 %\n",
            "Batch: 54   Training loss: 0.1907486766576767\n",
            "Precision: 68.50 +/- 33.7 %\n",
            "Recall: 27.16 +/- 13.3 %\n",
            "Batch: 55   Training loss: 0.19075839221477509\n",
            "Precision: 64.17 +/- 33.9 %\n",
            "Recall: 27.79 +/- 14.6 %\n",
            "Batch: 56   Training loss: 0.19183968007564545\n",
            "Precision: 56.07 +/- 39.5 %\n",
            "Recall: 20.97 +/- 15.7 %\n",
            "Batch: 57   Training loss: 0.16602247953414917\n",
            "Precision: 74.82 +/- 40.6 %\n",
            "Recall: 21.62 +/- 15.0 %\n",
            "Batch: 58   Training loss: 0.1707451194524765\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 13.03 +/- 9.7 %\n",
            "Batch: 59   Training loss: 0.1935066133737564\n",
            "Precision: 88.10 +/- 31.1 %\n",
            "Recall: 15.72 +/- 6.9 %\n",
            "Batch: 60   Training loss: 0.18019038438796997\n",
            "Precision: 61.90 +/- 47.7 %\n",
            "Recall: 15.22 +/- 15.4 %\n",
            "Batch: 61   Training loss: 0.17270620167255402\n",
            "Precision: 69.05 +/- 38.8 %\n",
            "Recall: 18.66 +/- 13.6 %\n",
            "Batch: 62   Training loss: 0.18689289689064026\n",
            "Precision: 72.62 +/- 40.9 %\n",
            "Recall: 19.07 +/- 13.4 %\n",
            "Batch: 63   Training loss: 0.16568338871002197\n",
            "Precision: 73.81 +/- 43.1 %\n",
            "Recall: 20.95 +/- 15.0 %\n",
            "Batch: 64   Training loss: 0.16956819593906403\n",
            "Precision: 62.08 +/- 44.5 %\n",
            "Recall: 20.87 +/- 16.8 %\n",
            "Batch: 65   Training loss: 0.1912209838628769\n",
            "Precision: 57.68 +/- 37.8 %\n",
            "Recall: 20.38 +/- 15.6 %\n",
            "Batch: 66   Training loss: 0.1821468025445938\n",
            "Precision: 73.51 +/- 33.6 %\n",
            "Recall: 22.06 +/- 12.3 %\n",
            "Batch: 67   Training loss: 0.19676509499549866\n",
            "Precision: 66.07 +/- 40.3 %\n",
            "Recall: 17.97 +/- 12.5 %\n",
            "Batch: 68   Training loss: 0.1727706640958786\n",
            "Precision: 45.24 +/- 42.9 %\n",
            "Recall: 13.20 +/- 13.7 %\n",
            "Batch: 69   Training loss: 0.174934983253479\n",
            "Precision: 74.40 +/- 38.1 %\n",
            "Recall: 18.46 +/- 11.6 %\n",
            "Batch: 70   Training loss: 0.16897214949131012\n",
            "Precision: 63.10 +/- 44.0 %\n",
            "Recall: 17.65 +/- 15.3 %\n",
            "Batch: 71   Training loss: 0.1734894961118698\n",
            "Precision: 65.18 +/- 42.5 %\n",
            "Recall: 16.50 +/- 14.0 %\n",
            "Batch: 72   Training loss: 0.17260736227035522\n",
            "Precision: 57.44 +/- 42.8 %\n",
            "Recall: 18.25 +/- 14.9 %\n",
            "Batch: 73   Training loss: 0.171243816614151\n",
            "Precision: 57.74 +/- 44.7 %\n",
            "Recall: 16.61 +/- 14.1 %\n",
            "Batch: 74   Training loss: 0.15706345438957214\n",
            "Precision: 79.17 +/- 34.8 %\n",
            "Recall: 27.12 +/- 15.9 %\n",
            "Batch: 75   Training loss: 0.1679406613111496\n",
            "Precision: 62.50 +/- 42.2 %\n",
            "Recall: 20.27 +/- 17.2 %\n",
            "Batch: 76   Training loss: 0.17040924727916718\n",
            "Precision: 61.90 +/- 42.5 %\n",
            "Recall: 17.85 +/- 13.1 %\n",
            "Batch: 77   Training loss: 0.1653081178665161\n",
            "Precision: 50.60 +/- 44.0 %\n",
            "Recall: 16.76 +/- 17.1 %\n",
            "Batch: 78   Training loss: 0.18620385229587555\n",
            "Precision: 65.18 +/- 41.8 %\n",
            "Recall: 16.76 +/- 12.1 %\n",
            "Batch: 79   Training loss: 0.17413102090358734\n",
            "Precision: 69.35 +/- 40.8 %\n",
            "Recall: 19.76 +/- 13.6 %\n",
            "Batch: 80   Training loss: 0.1707322597503662\n",
            "Precision: 60.03 +/- 33.0 %\n",
            "Recall: 20.13 +/- 15.3 %\n",
            "Batch: 81   Training loss: 0.1614905446767807\n",
            "Precision: 55.40 +/- 29.5 %\n",
            "Recall: 19.02 +/- 12.9 %\n",
            "Batch: 82   Training loss: 0.15396994352340698\n",
            "Precision: 54.70 +/- 39.3 %\n",
            "Recall: 21.95 +/- 17.1 %\n",
            "Batch: 83   Training loss: 0.1752212643623352\n",
            "Precision: 70.83 +/- 40.2 %\n",
            "Recall: 20.01 +/- 13.7 %\n",
            "Batch: 84   Training loss: 0.17130786180496216\n",
            "Precision: 65.48 +/- 45.0 %\n",
            "Recall: 15.56 +/- 13.3 %\n",
            "Batch: 85   Training loss: 0.17252157628536224\n",
            "Precision: 80.56 +/- 38.7 %\n",
            "Recall: 20.32 +/- 13.7 %\n",
            "Batch: 86   Training loss: 0.17993836104869843\n",
            "Precision: 73.81 +/- 38.2 %\n",
            "Recall: 21.33 +/- 13.8 %\n",
            "Batch: 87   Training loss: 0.17833669483661652\n",
            "Precision: 64.58 +/- 37.2 %\n",
            "Recall: 22.27 +/- 14.7 %\n",
            "Batch: 88   Training loss: 0.17734384536743164\n",
            "Precision: 71.25 +/- 36.8 %\n",
            "Recall: 25.07 +/- 15.6 %\n",
            "Batch: 89   Training loss: 0.16902214288711548\n",
            "Precision: 71.37 +/- 35.7 %\n",
            "Recall: 23.81 +/- 13.9 %\n",
            "Batch: 90   Training loss: 0.16088314354419708\n",
            "Precision: 50.19 +/- 39.3 %\n",
            "Recall: 14.78 +/- 13.2 %\n",
            "Batch: 91   Training loss: 0.15467755496501923\n",
            "Precision: 58.33 +/- 44.9 %\n",
            "Recall: 17.69 +/- 16.1 %\n",
            "Batch: 92   Training loss: 0.16600793600082397\n",
            "Precision: 68.84 +/- 40.6 %\n",
            "Recall: 22.01 +/- 16.2 %\n",
            "Epoch:  2\n",
            "Batch: 0   Training loss: 0.18772101402282715\n",
            "Precision: 72.20 +/- 39.6 %\n",
            "Recall: 22.04 +/- 16.4 %\n",
            "Batch: 1   Training loss: 0.1745646595954895\n",
            "Precision: 62.80 +/- 36.3 %\n",
            "Recall: 18.03 +/- 11.9 %\n",
            "Batch: 2   Training loss: 0.16927380859851837\n",
            "Precision: 69.05 +/- 42.7 %\n",
            "Recall: 20.66 +/- 15.3 %\n",
            "Batch: 3   Training loss: 0.17995582520961761\n",
            "Precision: 76.19 +/- 37.6 %\n",
            "Recall: 22.00 +/- 15.1 %\n",
            "Batch: 4   Training loss: 0.1580127626657486\n",
            "Precision: 65.60 +/- 42.1 %\n",
            "Recall: 22.70 +/- 15.7 %\n",
            "Batch: 5   Training loss: 0.17174752056598663\n",
            "Precision: 66.67 +/- 34.0 %\n",
            "Recall: 24.18 +/- 13.9 %\n",
            "Batch: 6   Training loss: 0.16516607999801636\n",
            "Precision: 73.15 +/- 35.1 %\n",
            "Recall: 27.87 +/- 16.9 %\n",
            "Batch: 7   Training loss: 0.17388151586055756\n",
            "Precision: 58.15 +/- 38.8 %\n",
            "Recall: 22.10 +/- 15.8 %\n",
            "Batch: 8   Training loss: 0.16660700738430023\n",
            "Precision: 66.19 +/- 36.2 %\n",
            "Recall: 25.27 +/- 15.8 %\n",
            "Batch: 9   Training loss: 0.16222378611564636\n",
            "Precision: 66.45 +/- 31.4 %\n",
            "Recall: 30.57 +/- 14.9 %\n",
            "Batch: 10   Training loss: 0.18013569712638855\n",
            "Precision: 69.35 +/- 39.8 %\n",
            "Recall: 18.44 +/- 12.8 %\n",
            "Batch: 11   Training loss: 0.16498930752277374\n",
            "Precision: 55.86 +/- 47.2 %\n",
            "Recall: 14.39 +/- 14.9 %\n",
            "Batch: 12   Training loss: 0.17038393020629883\n",
            "Precision: 65.82 +/- 46.5 %\n",
            "Recall: 15.71 +/- 14.5 %\n",
            "Batch: 13   Training loss: 0.1712988018989563\n",
            "Precision: 78.57 +/- 35.9 %\n",
            "Recall: 20.33 +/- 12.5 %\n",
            "Batch: 14   Training loss: 0.18586322665214539\n",
            "Precision: 73.81 +/- 43.1 %\n",
            "Recall: 16.17 +/- 13.1 %\n",
            "Batch: 15   Training loss: 0.16972315311431885\n",
            "Precision: 64.76 +/- 44.6 %\n",
            "Recall: 17.75 +/- 13.7 %\n",
            "Batch: 16   Training loss: 0.18796169757843018\n",
            "Precision: 70.12 +/- 34.3 %\n",
            "Recall: 22.61 +/- 13.6 %\n",
            "Batch: 17   Training loss: 0.1721610724925995\n",
            "Precision: 66.84 +/- 39.3 %\n",
            "Recall: 22.35 +/- 15.0 %\n",
            "Batch: 18   Training loss: 0.17825059592723846\n",
            "Precision: 88.99 +/- 27.3 %\n",
            "Recall: 25.48 +/- 13.0 %\n",
            "Batch: 19   Training loss: 0.1849943846464157\n",
            "Precision: 70.95 +/- 40.0 %\n",
            "Recall: 19.95 +/- 13.8 %\n",
            "Batch: 20   Training loss: 0.17867252230644226\n",
            "Precision: 75.77 +/- 33.1 %\n",
            "Recall: 25.19 +/- 13.7 %\n",
            "Batch: 21   Training loss: 0.17418745160102844\n",
            "Precision: 66.61 +/- 38.9 %\n",
            "Recall: 23.75 +/- 17.0 %\n",
            "Batch: 22   Training loss: 0.1667599231004715\n",
            "Precision: 63.15 +/- 43.4 %\n",
            "Recall: 20.35 +/- 15.7 %\n",
            "Batch: 23   Training loss: 0.17728982865810394\n",
            "Precision: 63.21 +/- 37.4 %\n",
            "Recall: 22.36 +/- 15.3 %\n",
            "Batch: 24   Training loss: 0.15682126581668854\n",
            "Precision: 61.31 +/- 32.5 %\n",
            "Recall: 28.49 +/- 16.3 %\n",
            "Batch: 25   Training loss: 0.16803643107414246\n",
            "Precision: 59.52 +/- 35.9 %\n",
            "Recall: 23.47 +/- 15.2 %\n",
            "Batch: 26   Training loss: 0.1746756136417389\n",
            "Precision: 63.75 +/- 38.4 %\n",
            "Recall: 19.69 +/- 13.9 %\n",
            "Batch: 27   Training loss: 0.1762775331735611\n",
            "Precision: 75.55 +/- 33.6 %\n",
            "Recall: 22.76 +/- 11.8 %\n",
            "Batch: 28   Training loss: 0.17027057707309723\n",
            "Precision: 47.01 +/- 37.4 %\n",
            "Recall: 17.98 +/- 14.6 %\n",
            "Batch: 29   Training loss: 0.1844959706068039\n",
            "Precision: 59.11 +/- 40.7 %\n",
            "Recall: 21.99 +/- 15.9 %\n",
            "Batch: 30   Training loss: 0.18002073466777802\n",
            "Precision: 60.12 +/- 45.0 %\n",
            "Recall: 17.41 +/- 16.0 %\n",
            "Batch: 31   Training loss: 0.1697400063276291\n",
            "Precision: 62.68 +/- 42.6 %\n",
            "Recall: 18.50 +/- 12.9 %\n",
            "Batch: 32   Training loss: 0.17289398610591888\n",
            "Precision: 58.93 +/- 40.7 %\n",
            "Recall: 19.73 +/- 16.1 %\n",
            "Batch: 33   Training loss: 0.1728067845106125\n",
            "Precision: 58.93 +/- 40.7 %\n",
            "Recall: 15.72 +/- 11.6 %\n",
            "Batch: 34   Training loss: 0.16836129128932953\n",
            "Precision: 68.15 +/- 37.5 %\n",
            "Recall: 22.81 +/- 15.1 %\n",
            "Batch: 35   Training loss: 0.15906329452991486\n",
            "Precision: 70.42 +/- 33.4 %\n",
            "Recall: 27.43 +/- 13.4 %\n",
            "Batch: 36   Training loss: 0.16715283691883087\n",
            "Precision: 66.07 +/- 30.0 %\n",
            "Recall: 27.35 +/- 19.3 %\n",
            "Batch: 37   Training loss: 0.17600660026073456\n",
            "Precision: 65.48 +/- 37.7 %\n",
            "Recall: 22.43 +/- 15.2 %\n",
            "Batch: 38   Training loss: 0.1842336505651474\n",
            "Precision: 80.06 +/- 28.6 %\n",
            "Recall: 25.92 +/- 13.7 %\n",
            "Batch: 39   Training loss: 0.17582225799560547\n",
            "Precision: 76.96 +/- 35.4 %\n",
            "Recall: 23.73 +/- 14.7 %\n",
            "Batch: 40   Training loss: 0.18088886141777039\n",
            "Precision: 61.90 +/- 40.5 %\n",
            "Recall: 15.59 +/- 12.0 %\n",
            "Batch: 41   Training loss: 0.17163878679275513\n",
            "Precision: 68.39 +/- 36.6 %\n",
            "Recall: 24.71 +/- 16.0 %\n",
            "Batch: 42   Training loss: 0.16710442304611206\n",
            "Precision: 61.43 +/- 33.5 %\n",
            "Recall: 28.60 +/- 15.4 %\n",
            "Batch: 43   Training loss: 0.18629509210586548\n",
            "Precision: 62.74 +/- 33.9 %\n",
            "Recall: 26.18 +/- 14.0 %\n",
            "Batch: 44   Training loss: 0.17674346268177032\n",
            "Precision: 58.75 +/- 33.1 %\n",
            "Recall: 25.75 +/- 15.2 %\n",
            "Batch: 45   Training loss: 0.1762067973613739\n",
            "Precision: 65.95 +/- 40.4 %\n",
            "Recall: 19.61 +/- 14.7 %\n",
            "Batch: 46   Training loss: 0.16222162544727325\n",
            "Precision: 62.20 +/- 44.3 %\n",
            "Recall: 17.68 +/- 15.6 %\n",
            "Batch: 47   Training loss: 0.15881676971912384\n",
            "Precision: 78.57 +/- 41.0 %\n",
            "Recall: 20.94 +/- 15.8 %\n",
            "Batch: 48   Training loss: 0.1845572292804718\n",
            "Precision: 72.62 +/- 43.7 %\n",
            "Recall: 13.89 +/- 9.8 %\n",
            "[3,    50] loss: 0.171\n",
            "Batch: 49   Training loss: 0.17124326527118683\n",
            "Precision: 64.58 +/- 37.7 %\n",
            "Recall: 21.74 +/- 14.8 %\n",
            "Batch: 50   Training loss: 0.1701699048280716\n",
            "Precision: 57.26 +/- 40.2 %\n",
            "Recall: 17.32 +/- 13.6 %\n",
            "Batch: 51   Training loss: 0.17056158185005188\n",
            "Precision: 56.31 +/- 44.3 %\n",
            "Recall: 19.73 +/- 17.9 %\n",
            "Batch: 52   Training loss: 0.17256219685077667\n",
            "Precision: 73.87 +/- 36.3 %\n",
            "Recall: 24.44 +/- 14.6 %\n",
            "Batch: 53   Training loss: 0.16959618031978607\n",
            "Precision: 57.50 +/- 43.8 %\n",
            "Recall: 20.02 +/- 17.9 %\n",
            "Batch: 54   Training loss: 0.17653504014015198\n",
            "Precision: 43.45 +/- 46.1 %\n",
            "Recall: 9.46 +/- 9.8 %\n",
            "Batch: 55   Training loss: 0.17843028903007507\n",
            "Precision: 62.10 +/- 39.9 %\n",
            "Recall: 19.62 +/- 15.3 %\n",
            "Batch: 56   Training loss: 0.18078817427158356\n",
            "Precision: 63.10 +/- 40.4 %\n",
            "Recall: 21.51 +/- 16.7 %\n",
            "Batch: 57   Training loss: 0.16629090905189514\n",
            "Precision: 63.15 +/- 39.9 %\n",
            "Recall: 21.44 +/- 16.2 %\n",
            "Batch: 58   Training loss: 0.16242407262325287\n",
            "Precision: 80.95 +/- 28.8 %\n",
            "Recall: 26.57 +/- 12.3 %\n",
            "Batch: 59   Training loss: 0.16762855648994446\n",
            "Precision: 56.79 +/- 41.0 %\n",
            "Recall: 20.28 +/- 16.5 %\n",
            "Batch: 60   Training loss: 0.16801594197750092\n",
            "Precision: 59.11 +/- 32.8 %\n",
            "Recall: 24.24 +/- 14.4 %\n",
            "Batch: 61   Training loss: 0.158394917845726\n",
            "Precision: 56.37 +/- 35.5 %\n",
            "Recall: 25.64 +/- 20.7 %\n",
            "Batch: 62   Training loss: 0.15759573876857758\n",
            "Precision: 60.05 +/- 32.0 %\n",
            "Recall: 24.59 +/- 16.1 %\n",
            "Batch: 63   Training loss: 0.18096038699150085\n",
            "Precision: 72.86 +/- 31.9 %\n",
            "Recall: 22.60 +/- 14.7 %\n",
            "Batch: 64   Training loss: 0.1845996379852295\n",
            "Precision: 74.70 +/- 38.8 %\n",
            "Recall: 21.52 +/- 15.1 %\n",
            "Batch: 65   Training loss: 0.16690845787525177\n",
            "Precision: 62.50 +/- 38.7 %\n",
            "Recall: 20.45 +/- 14.1 %\n",
            "Batch: 66   Training loss: 0.1633789986371994\n",
            "Precision: 64.29 +/- 35.6 %\n",
            "Recall: 24.25 +/- 15.1 %\n",
            "Batch: 67   Training loss: 0.1598249226808548\n",
            "Precision: 51.90 +/- 39.2 %\n",
            "Recall: 19.55 +/- 16.3 %\n",
            "Batch: 68   Training loss: 0.1628219485282898\n",
            "Precision: 70.60 +/- 34.4 %\n",
            "Recall: 28.93 +/- 16.2 %\n",
            "Batch: 69   Training loss: 0.19884517788887024\n",
            "Precision: 77.68 +/- 33.9 %\n",
            "Recall: 21.43 +/- 13.1 %\n",
            "Batch: 70   Training loss: 0.16700948774814606\n",
            "Precision: 82.74 +/- 32.9 %\n",
            "Recall: 23.58 +/- 14.8 %\n",
            "Batch: 71   Training loss: 0.1618761420249939\n",
            "Precision: 73.21 +/- 43.3 %\n",
            "Recall: 18.23 +/- 13.7 %\n",
            "Batch: 72   Training loss: 0.177291139960289\n",
            "Precision: 63.39 +/- 45.8 %\n",
            "Recall: 15.05 +/- 14.4 %\n",
            "Batch: 73   Training loss: 0.16834098100662231\n",
            "Precision: 53.57 +/- 43.7 %\n",
            "Recall: 18.47 +/- 16.5 %\n",
            "Batch: 74   Training loss: 0.16345180571079254\n",
            "Precision: 54.17 +/- 38.0 %\n",
            "Recall: 24.22 +/- 17.7 %\n",
            "Batch: 75   Training loss: 0.17544132471084595\n",
            "Precision: 54.23 +/- 32.8 %\n",
            "Recall: 25.41 +/- 16.5 %\n",
            "Batch: 76   Training loss: 0.17744100093841553\n",
            "Precision: 58.99 +/- 37.1 %\n",
            "Recall: 25.18 +/- 17.1 %\n",
            "Batch: 77   Training loss: 0.16519106924533844\n",
            "Precision: 58.21 +/- 46.1 %\n",
            "Recall: 16.82 +/- 16.6 %\n",
            "Batch: 78   Training loss: 0.16633962094783783\n",
            "Precision: 82.14 +/- 38.3 %\n",
            "Recall: 19.49 +/- 13.4 %\n",
            "Batch: 79   Training loss: 0.16900768876075745\n",
            "Precision: 61.31 +/- 44.8 %\n",
            "Recall: 15.45 +/- 14.4 %\n",
            "Batch: 80   Training loss: 0.16799914836883545\n",
            "Precision: 67.86 +/- 41.8 %\n",
            "Recall: 19.17 +/- 14.7 %\n",
            "Batch: 81   Training loss: 0.1763097494840622\n",
            "Precision: 64.58 +/- 42.7 %\n",
            "Recall: 19.30 +/- 17.2 %\n",
            "Batch: 82   Training loss: 0.17319747805595398\n",
            "Precision: 56.31 +/- 44.6 %\n",
            "Recall: 18.68 +/- 17.6 %\n",
            "Batch: 83   Training loss: 0.1689295917749405\n",
            "Precision: 68.45 +/- 43.9 %\n",
            "Recall: 20.82 +/- 17.5 %\n",
            "Batch: 84   Training loss: 0.1652248054742813\n",
            "Precision: 71.43 +/- 43.4 %\n",
            "Recall: 18.20 +/- 15.2 %\n",
            "Batch: 85   Training loss: 0.15838149189949036\n",
            "Precision: 59.52 +/- 46.6 %\n",
            "Recall: 16.85 +/- 15.8 %\n",
            "Batch: 86   Training loss: 0.1575305312871933\n",
            "Precision: 66.67 +/- 46.3 %\n",
            "Recall: 20.13 +/- 17.3 %\n",
            "Batch: 87   Training loss: 0.16267819702625275\n",
            "Precision: 55.83 +/- 42.2 %\n",
            "Recall: 18.59 +/- 15.6 %\n",
            "Batch: 88   Training loss: 0.18085822463035583\n",
            "Precision: 78.81 +/- 34.0 %\n",
            "Recall: 24.07 +/- 13.5 %\n",
            "Batch: 89   Training loss: 0.15876853466033936\n",
            "Precision: 69.48 +/- 32.7 %\n",
            "Recall: 31.05 +/- 17.0 %\n",
            "Batch: 90   Training loss: 0.17734329402446747\n",
            "Precision: 73.36 +/- 30.7 %\n",
            "Recall: 27.00 +/- 14.6 %\n",
            "Batch: 91   Training loss: 0.16790083050727844\n",
            "Precision: 65.12 +/- 41.6 %\n",
            "Recall: 20.38 +/- 15.8 %\n",
            "Batch: 92   Training loss: 0.1724162995815277\n",
            "Precision: 73.04 +/- 40.1 %\n",
            "Recall: 18.83 +/- 12.8 %\n",
            "Epoch:  3\n",
            "Batch: 0   Training loss: 0.17090819776058197\n",
            "Precision: 55.95 +/- 47.2 %\n",
            "Recall: 13.38 +/- 12.9 %\n",
            "Batch: 1   Training loss: 0.1645885854959488\n",
            "Precision: 67.14 +/- 43.7 %\n",
            "Recall: 18.79 +/- 16.8 %\n",
            "Batch: 2   Training loss: 0.1740504652261734\n",
            "Precision: 66.31 +/- 38.5 %\n",
            "Recall: 19.33 +/- 14.7 %\n",
            "Batch: 3   Training loss: 0.16698025166988373\n",
            "Precision: 72.98 +/- 31.2 %\n",
            "Recall: 30.12 +/- 14.7 %\n",
            "Batch: 4   Training loss: 0.15710239112377167\n",
            "Precision: 58.24 +/- 35.9 %\n",
            "Recall: 25.67 +/- 15.5 %\n",
            "Batch: 5   Training loss: 0.15812532603740692\n",
            "Precision: 55.94 +/- 37.2 %\n",
            "Recall: 24.76 +/- 18.1 %\n",
            "Batch: 6   Training loss: 0.1727016717195511\n",
            "Precision: 75.36 +/- 35.2 %\n",
            "Recall: 26.12 +/- 15.1 %\n",
            "Batch: 7   Training loss: 0.1768273562192917\n",
            "Precision: 57.80 +/- 45.6 %\n",
            "Recall: 15.92 +/- 15.1 %\n",
            "Batch: 8   Training loss: 0.16307948529720306\n",
            "Precision: 66.67 +/- 40.8 %\n",
            "Recall: 22.36 +/- 18.0 %\n",
            "Batch: 9   Training loss: 0.18669825792312622\n",
            "Precision: 73.81 +/- 41.2 %\n",
            "Recall: 16.17 +/- 11.3 %\n",
            "Batch: 10   Training loss: 0.17934487760066986\n",
            "Precision: 64.58 +/- 41.6 %\n",
            "Recall: 18.86 +/- 15.6 %\n",
            "Batch: 11   Training loss: 0.18667683005332947\n",
            "Precision: 70.00 +/- 34.3 %\n",
            "Recall: 23.93 +/- 14.9 %\n",
            "Batch: 12   Training loss: 0.16908681392669678\n",
            "Precision: 54.47 +/- 36.6 %\n",
            "Recall: 24.99 +/- 17.8 %\n",
            "Batch: 13   Training loss: 0.18703944981098175\n",
            "Precision: 52.38 +/- 37.3 %\n",
            "Recall: 22.17 +/- 16.2 %\n",
            "Batch: 14   Training loss: 0.17600107192993164\n",
            "Precision: 72.64 +/- 29.2 %\n",
            "Recall: 32.18 +/- 15.3 %\n",
            "Batch: 15   Training loss: 0.17565231025218964\n",
            "Precision: 69.70 +/- 37.0 %\n",
            "Recall: 22.83 +/- 13.9 %\n",
            "Batch: 16   Training loss: 0.18099148571491241\n",
            "Precision: 69.64 +/- 43.2 %\n",
            "Recall: 18.97 +/- 13.8 %\n",
            "Batch: 17   Training loss: 0.19450868666172028\n",
            "Precision: 68.75 +/- 35.7 %\n",
            "Recall: 22.93 +/- 13.7 %\n",
            "Batch: 18   Training loss: 0.1731564998626709\n",
            "Precision: 61.93 +/- 35.0 %\n",
            "Recall: 27.33 +/- 17.0 %\n",
            "Batch: 19   Training loss: 0.1758175790309906\n",
            "Precision: 53.21 +/- 40.1 %\n",
            "Recall: 23.51 +/- 17.9 %\n",
            "Batch: 20   Training loss: 0.1743507832288742\n",
            "Precision: 64.04 +/- 32.9 %\n",
            "Recall: 30.90 +/- 17.8 %\n",
            "Batch: 21   Training loss: 0.15800926089286804\n",
            "Precision: 62.44 +/- 38.6 %\n",
            "Recall: 27.21 +/- 18.4 %\n",
            "Batch: 22   Training loss: 0.1648712158203125\n",
            "Precision: 61.79 +/- 44.9 %\n",
            "Recall: 17.75 +/- 16.8 %\n",
            "Batch: 23   Training loss: 0.1598898321390152\n",
            "Precision: 60.71 +/- 48.8 %\n",
            "Recall: 15.54 +/- 16.2 %\n",
            "Batch: 24   Training loss: 0.1619119495153427\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 21.24 +/- 18.3 %\n",
            "Batch: 25   Training loss: 0.17403961718082428\n",
            "Precision: 71.43 +/- 45.2 %\n",
            "Recall: 17.91 +/- 15.1 %\n",
            "Batch: 26   Training loss: 0.1748850792646408\n",
            "Precision: 70.30 +/- 43.1 %\n",
            "Recall: 17.95 +/- 13.9 %\n",
            "Batch: 27   Training loss: 0.1678987592458725\n",
            "Precision: 80.42 +/- 28.2 %\n",
            "Recall: 33.41 +/- 12.4 %\n",
            "Batch: 28   Training loss: 0.16402263939380646\n",
            "Precision: 68.69 +/- 28.4 %\n",
            "Recall: 34.01 +/- 12.6 %\n",
            "Batch: 29   Training loss: 0.16245147585868835\n",
            "Precision: 55.89 +/- 41.0 %\n",
            "Recall: 24.07 +/- 18.0 %\n",
            "Batch: 30   Training loss: 0.16882936656475067\n",
            "Precision: 65.06 +/- 42.4 %\n",
            "Recall: 21.72 +/- 15.6 %\n",
            "Batch: 31   Training loss: 0.15445539355278015\n",
            "Precision: 53.57 +/- 40.9 %\n",
            "Recall: 17.21 +/- 15.5 %\n",
            "Batch: 32   Training loss: 0.18455569446086884\n",
            "Precision: 81.55 +/- 30.0 %\n",
            "Recall: 19.31 +/- 11.5 %\n",
            "Batch: 33   Training loss: 0.17210055887699127\n",
            "Precision: 59.46 +/- 39.7 %\n",
            "Recall: 19.46 +/- 15.1 %\n",
            "Batch: 34   Training loss: 0.1930178999900818\n",
            "Precision: 55.65 +/- 42.7 %\n",
            "Recall: 18.98 +/- 15.4 %\n",
            "Batch: 35   Training loss: 0.15892811119556427\n",
            "Precision: 65.60 +/- 39.3 %\n",
            "Recall: 26.59 +/- 17.1 %\n",
            "Batch: 36   Training loss: 0.1696057915687561\n",
            "Precision: 65.36 +/- 32.8 %\n",
            "Recall: 28.43 +/- 15.3 %\n",
            "Batch: 37   Training loss: 0.18020866811275482\n",
            "Precision: 54.88 +/- 29.8 %\n",
            "Recall: 27.67 +/- 15.5 %\n",
            "Batch: 38   Training loss: 0.16604503989219666\n",
            "Precision: 65.59 +/- 32.3 %\n",
            "Recall: 31.97 +/- 14.8 %\n",
            "Batch: 39   Training loss: 0.16115257143974304\n",
            "Precision: 50.95 +/- 39.4 %\n",
            "Recall: 19.97 +/- 16.7 %\n",
            "Batch: 40   Training loss: 0.16146352887153625\n",
            "Precision: 69.05 +/- 35.0 %\n",
            "Recall: 19.62 +/- 13.0 %\n",
            "Batch: 41   Training loss: 0.19328372180461884\n",
            "Precision: 67.26 +/- 44.2 %\n",
            "Recall: 13.18 +/- 10.2 %\n",
            "Batch: 42   Training loss: 0.17061448097229004\n",
            "Precision: 73.15 +/- 38.7 %\n",
            "Recall: 19.81 +/- 13.5 %\n",
            "Batch: 43   Training loss: 0.17167901992797852\n",
            "Precision: 68.45 +/- 40.6 %\n",
            "Recall: 18.50 +/- 14.3 %\n",
            "Batch: 44   Training loss: 0.1595194935798645\n",
            "Precision: 59.29 +/- 41.5 %\n",
            "Recall: 21.38 +/- 15.6 %\n",
            "Batch: 45   Training loss: 0.17034503817558289\n",
            "Precision: 63.45 +/- 32.4 %\n",
            "Recall: 27.84 +/- 17.3 %\n",
            "Batch: 46   Training loss: 0.1733940690755844\n",
            "Precision: 58.66 +/- 34.2 %\n",
            "Recall: 25.47 +/- 15.7 %\n",
            "Batch: 47   Training loss: 0.16549870371818542\n",
            "Precision: 64.67 +/- 39.3 %\n",
            "Recall: 22.25 +/- 15.0 %\n",
            "Batch: 48   Training loss: 0.16472949087619781\n",
            "Precision: 81.79 +/- 30.4 %\n",
            "Recall: 26.81 +/- 13.6 %\n",
            "[4,    50] loss: 0.151\n",
            "Batch: 49   Training loss: 0.15121310949325562\n",
            "Precision: 66.37 +/- 41.9 %\n",
            "Recall: 18.49 +/- 13.2 %\n",
            "Batch: 50   Training loss: 0.1885259747505188\n",
            "Precision: 68.45 +/- 38.3 %\n",
            "Recall: 18.35 +/- 12.9 %\n",
            "Batch: 51   Training loss: 0.17240957915782928\n",
            "Precision: 73.21 +/- 38.9 %\n",
            "Recall: 20.53 +/- 13.7 %\n",
            "Batch: 52   Training loss: 0.17151783406734467\n",
            "Precision: 74.58 +/- 33.9 %\n",
            "Recall: 23.50 +/- 12.9 %\n",
            "Batch: 53   Training loss: 0.17194399237632751\n",
            "Precision: 53.27 +/- 36.6 %\n",
            "Recall: 19.17 +/- 14.5 %\n",
            "Batch: 54   Training loss: 0.1776445358991623\n",
            "Precision: 72.50 +/- 37.2 %\n",
            "Recall: 23.96 +/- 15.7 %\n",
            "Batch: 55   Training loss: 0.17637266218662262\n",
            "Precision: 68.37 +/- 35.6 %\n",
            "Recall: 23.73 +/- 13.3 %\n",
            "Batch: 56   Training loss: 0.1605263203382492\n",
            "Precision: 73.33 +/- 38.2 %\n",
            "Recall: 23.77 +/- 14.7 %\n",
            "Batch: 57   Training loss: 0.1870420277118683\n",
            "Precision: 75.48 +/- 37.1 %\n",
            "Recall: 20.75 +/- 12.8 %\n",
            "Batch: 58   Training loss: 0.1770731657743454\n",
            "Precision: 66.79 +/- 35.7 %\n",
            "Recall: 22.51 +/- 15.7 %\n",
            "Batch: 59   Training loss: 0.16919870674610138\n",
            "Precision: 50.18 +/- 45.4 %\n",
            "Recall: 17.27 +/- 18.4 %\n",
            "Batch: 60   Training loss: 0.17329594492912292\n",
            "Precision: 65.48 +/- 42.4 %\n",
            "Recall: 18.36 +/- 14.5 %\n",
            "Batch: 61   Training loss: 0.16524888575077057\n",
            "Precision: 67.44 +/- 41.9 %\n",
            "Recall: 19.92 +/- 14.8 %\n",
            "Batch: 62   Training loss: 0.17532430589199066\n",
            "Precision: 75.00 +/- 37.2 %\n",
            "Recall: 25.21 +/- 15.3 %\n",
            "Batch: 63   Training loss: 0.18037384748458862\n",
            "Precision: 69.52 +/- 38.9 %\n",
            "Recall: 19.19 +/- 12.3 %\n",
            "Batch: 64   Training loss: 0.16884663701057434\n",
            "Precision: 75.24 +/- 31.9 %\n",
            "Recall: 25.08 +/- 14.1 %\n",
            "Batch: 65   Training loss: 0.16324035823345184\n",
            "Precision: 60.65 +/- 37.6 %\n",
            "Recall: 24.41 +/- 17.8 %\n",
            "Batch: 66   Training loss: 0.16554798185825348\n",
            "Precision: 66.49 +/- 38.4 %\n",
            "Recall: 24.31 +/- 16.2 %\n",
            "Batch: 67   Training loss: 0.18244536221027374\n",
            "Precision: 65.36 +/- 39.9 %\n",
            "Recall: 22.50 +/- 15.1 %\n",
            "Batch: 68   Training loss: 0.15771135687828064\n",
            "Precision: 56.55 +/- 34.8 %\n",
            "Recall: 28.81 +/- 18.6 %\n",
            "Batch: 69   Training loss: 0.17055147886276245\n",
            "Precision: 56.37 +/- 37.9 %\n",
            "Recall: 22.69 +/- 16.1 %\n",
            "Batch: 70   Training loss: 0.1615877002477646\n",
            "Precision: 65.65 +/- 29.7 %\n",
            "Recall: 32.98 +/- 16.3 %\n",
            "Batch: 71   Training loss: 0.1691301465034485\n",
            "Precision: 67.74 +/- 32.2 %\n",
            "Recall: 28.98 +/- 15.4 %\n",
            "Batch: 72   Training loss: 0.15891145169734955\n",
            "Precision: 61.33 +/- 33.7 %\n",
            "Recall: 27.93 +/- 16.9 %\n",
            "Batch: 73   Training loss: 0.16670148074626923\n",
            "Precision: 53.63 +/- 39.1 %\n",
            "Recall: 21.02 +/- 16.8 %\n",
            "Batch: 74   Training loss: 0.16806210577487946\n",
            "Precision: 67.02 +/- 36.2 %\n",
            "Recall: 24.01 +/- 15.6 %\n",
            "Batch: 75   Training loss: 0.17120502889156342\n",
            "Precision: 73.10 +/- 35.4 %\n",
            "Recall: 22.29 +/- 12.6 %\n",
            "Batch: 76   Training loss: 0.158161923289299\n",
            "Precision: 79.82 +/- 32.0 %\n",
            "Recall: 26.74 +/- 13.0 %\n",
            "Batch: 77   Training loss: 0.1718137562274933\n",
            "Precision: 61.67 +/- 41.2 %\n",
            "Recall: 19.64 +/- 16.3 %\n",
            "Batch: 78   Training loss: 0.17936153709888458\n",
            "Precision: 60.60 +/- 42.6 %\n",
            "Recall: 22.33 +/- 22.0 %\n",
            "Batch: 79   Training loss: 0.16451023519039154\n",
            "Precision: 63.27 +/- 44.2 %\n",
            "Recall: 15.99 +/- 12.9 %\n",
            "Batch: 80   Training loss: 0.1719687581062317\n",
            "Precision: 79.52 +/- 36.8 %\n",
            "Recall: 23.24 +/- 14.4 %\n",
            "Batch: 81   Training loss: 0.18120883405208588\n",
            "Precision: 77.26 +/- 30.9 %\n",
            "Recall: 22.89 +/- 12.3 %\n",
            "Batch: 82   Training loss: 0.169524148106575\n",
            "Precision: 61.20 +/- 39.6 %\n",
            "Recall: 26.31 +/- 17.7 %\n",
            "Batch: 83   Training loss: 0.17213307321071625\n",
            "Precision: 62.96 +/- 35.0 %\n",
            "Recall: 26.37 +/- 14.5 %\n",
            "Batch: 84   Training loss: 0.18233872950077057\n",
            "Precision: 69.36 +/- 34.3 %\n",
            "Recall: 29.21 +/- 16.7 %\n",
            "Batch: 85   Training loss: 0.16805627942085266\n",
            "Precision: 68.46 +/- 27.5 %\n",
            "Recall: 32.91 +/- 16.8 %\n",
            "Batch: 86   Training loss: 0.1554996222257614\n",
            "Precision: 52.69 +/- 38.2 %\n",
            "Recall: 26.36 +/- 20.3 %\n",
            "Batch: 87   Training loss: 0.16167449951171875\n",
            "Precision: 72.20 +/- 33.1 %\n",
            "Recall: 27.95 +/- 16.6 %\n",
            "Batch: 88   Training loss: 0.1685141921043396\n",
            "Precision: 61.31 +/- 42.3 %\n",
            "Recall: 21.57 +/- 17.1 %\n",
            "Batch: 89   Training loss: 0.18538370728492737\n",
            "Precision: 67.76 +/- 36.7 %\n",
            "Recall: 22.98 +/- 16.0 %\n",
            "Batch: 90   Training loss: 0.17227427661418915\n",
            "Precision: 53.15 +/- 42.4 %\n",
            "Recall: 17.73 +/- 16.3 %\n",
            "Batch: 91   Training loss: 0.147933229804039\n",
            "Precision: 52.13 +/- 40.5 %\n",
            "Recall: 21.97 +/- 18.2 %\n",
            "Batch: 92   Training loss: 0.1371799260377884\n",
            "Precision: 49.64 +/- 42.6 %\n",
            "Recall: 22.61 +/- 19.8 %\n",
            "Epoch:  4\n",
            "Batch: 0   Training loss: 0.18105189502239227\n",
            "Precision: 53.69 +/- 40.2 %\n",
            "Recall: 17.43 +/- 13.4 %\n",
            "Batch: 1   Training loss: 0.16315312683582306\n",
            "Precision: 52.68 +/- 43.2 %\n",
            "Recall: 17.91 +/- 17.3 %\n",
            "Batch: 2   Training loss: 0.16652190685272217\n",
            "Precision: 59.23 +/- 45.4 %\n",
            "Recall: 16.18 +/- 14.5 %\n",
            "Batch: 3   Training loss: 0.18231502175331116\n",
            "Precision: 67.94 +/- 35.7 %\n",
            "Recall: 18.74 +/- 11.1 %\n",
            "Batch: 4   Training loss: 0.1778886616230011\n",
            "Precision: 73.04 +/- 35.5 %\n",
            "Recall: 26.21 +/- 13.6 %\n",
            "Batch: 5   Training loss: 0.1780984103679657\n",
            "Precision: 73.18 +/- 34.1 %\n",
            "Recall: 28.01 +/- 14.8 %\n",
            "Batch: 6   Training loss: 0.16945314407348633\n",
            "Precision: 62.34 +/- 38.8 %\n",
            "Recall: 26.43 +/- 18.9 %\n",
            "Batch: 7   Training loss: 0.15862904489040375\n",
            "Precision: 68.04 +/- 41.2 %\n",
            "Recall: 24.16 +/- 16.9 %\n",
            "Batch: 8   Training loss: 0.17716354131698608\n",
            "Precision: 65.65 +/- 41.3 %\n",
            "Recall: 20.30 +/- 15.4 %\n",
            "Batch: 9   Training loss: 0.17984110116958618\n",
            "Precision: 76.90 +/- 34.7 %\n",
            "Recall: 23.84 +/- 13.4 %\n",
            "Batch: 10   Training loss: 0.15655043721199036\n",
            "Precision: 62.26 +/- 37.5 %\n",
            "Recall: 26.79 +/- 17.0 %\n",
            "Batch: 11   Training loss: 0.14802022278308868\n",
            "Precision: 65.54 +/- 36.1 %\n",
            "Recall: 29.64 +/- 18.0 %\n",
            "Batch: 12   Training loss: 0.18597450852394104\n",
            "Precision: 64.82 +/- 34.8 %\n",
            "Recall: 24.03 +/- 15.4 %\n",
            "Batch: 13   Training loss: 0.16663330793380737\n",
            "Precision: 69.46 +/- 37.0 %\n",
            "Recall: 23.23 +/- 15.6 %\n",
            "Batch: 14   Training loss: 0.17840692400932312\n",
            "Precision: 75.71 +/- 38.4 %\n",
            "Recall: 20.90 +/- 14.1 %\n",
            "Batch: 15   Training loss: 0.1630210131406784\n",
            "Precision: 54.94 +/- 46.6 %\n",
            "Recall: 15.12 +/- 15.8 %\n",
            "Batch: 16   Training loss: 0.183033287525177\n",
            "Precision: 76.37 +/- 37.8 %\n",
            "Recall: 19.94 +/- 13.9 %\n",
            "Batch: 17   Training loss: 0.15779715776443481\n",
            "Precision: 70.95 +/- 39.9 %\n",
            "Recall: 26.94 +/- 18.5 %\n",
            "Batch: 18   Training loss: 0.17766526341438293\n",
            "Precision: 48.81 +/- 36.6 %\n",
            "Recall: 20.34 +/- 16.2 %\n",
            "Batch: 19   Training loss: 0.17060865461826324\n",
            "Precision: 56.92 +/- 35.2 %\n",
            "Recall: 28.53 +/- 18.7 %\n",
            "Batch: 20   Training loss: 0.17239885032176971\n",
            "Precision: 63.10 +/- 31.9 %\n",
            "Recall: 33.66 +/- 16.2 %\n",
            "Batch: 21   Training loss: 0.17028523981571198\n",
            "Precision: 61.47 +/- 27.1 %\n",
            "Recall: 30.54 +/- 13.6 %\n",
            "Batch: 22   Training loss: 0.1784932017326355\n",
            "Precision: 69.40 +/- 35.8 %\n",
            "Recall: 27.41 +/- 15.0 %\n",
            "Batch: 23   Training loss: 0.16747541725635529\n",
            "Precision: 63.99 +/- 40.2 %\n",
            "Recall: 20.02 +/- 14.1 %\n",
            "Batch: 24   Training loss: 0.15127405524253845\n",
            "Precision: 51.25 +/- 44.8 %\n",
            "Recall: 16.65 +/- 16.0 %\n",
            "Batch: 25   Training loss: 0.19017700850963593\n",
            "Precision: 69.74 +/- 38.5 %\n",
            "Recall: 19.94 +/- 14.0 %\n",
            "Batch: 26   Training loss: 0.14342676103115082\n",
            "Precision: 75.77 +/- 35.1 %\n",
            "Recall: 28.34 +/- 15.8 %\n",
            "Batch: 27   Training loss: 0.17150942981243134\n",
            "Precision: 58.15 +/- 40.6 %\n",
            "Recall: 21.78 +/- 15.9 %\n",
            "Batch: 28   Training loss: 0.172939732670784\n",
            "Precision: 64.17 +/- 37.7 %\n",
            "Recall: 28.95 +/- 17.4 %\n",
            "Batch: 29   Training loss: 0.16991347074508667\n",
            "Precision: 55.71 +/- 32.2 %\n",
            "Recall: 26.22 +/- 15.5 %\n",
            "Batch: 30   Training loss: 0.17601771652698517\n",
            "Precision: 70.14 +/- 32.6 %\n",
            "Recall: 28.75 +/- 14.6 %\n",
            "Batch: 31   Training loss: 0.18121348321437836\n",
            "Precision: 74.06 +/- 28.8 %\n",
            "Recall: 29.70 +/- 14.8 %\n",
            "Batch: 32   Training loss: 0.17197301983833313\n",
            "Precision: 67.69 +/- 45.1 %\n",
            "Recall: 20.34 +/- 15.9 %\n",
            "Batch: 33   Training loss: 0.16437584161758423\n",
            "Precision: 68.81 +/- 41.6 %\n",
            "Recall: 19.22 +/- 14.5 %\n",
            "Batch: 34   Training loss: 0.17558932304382324\n",
            "Precision: 73.77 +/- 40.1 %\n",
            "Recall: 17.38 +/- 12.9 %\n",
            "Batch: 35   Training loss: 0.16339673101902008\n",
            "Precision: 58.27 +/- 41.6 %\n",
            "Recall: 20.61 +/- 17.0 %\n",
            "Batch: 36   Training loss: 0.17081290483474731\n",
            "Precision: 68.21 +/- 37.6 %\n",
            "Recall: 26.31 +/- 18.4 %\n",
            "Batch: 37   Training loss: 0.1850365549325943\n",
            "Precision: 67.31 +/- 33.3 %\n",
            "Recall: 27.50 +/- 15.0 %\n",
            "Batch: 38   Training loss: 0.17466241121292114\n",
            "Precision: 65.48 +/- 34.2 %\n",
            "Recall: 26.41 +/- 16.2 %\n",
            "Batch: 39   Training loss: 0.1631004512310028\n",
            "Precision: 60.30 +/- 38.1 %\n",
            "Recall: 23.31 +/- 16.8 %\n",
            "Batch: 40   Training loss: 0.17214272916316986\n",
            "Precision: 69.26 +/- 41.0 %\n",
            "Recall: 20.23 +/- 13.9 %\n",
            "Batch: 41   Training loss: 0.18363338708877563\n",
            "Precision: 66.07 +/- 45.9 %\n",
            "Recall: 16.36 +/- 15.0 %\n",
            "Batch: 42   Training loss: 0.18213634192943573\n",
            "Precision: 71.13 +/- 36.6 %\n",
            "Recall: 17.89 +/- 11.3 %\n",
            "Batch: 43   Training loss: 0.16234926879405975\n",
            "Precision: 64.29 +/- 40.0 %\n",
            "Recall: 20.10 +/- 14.9 %\n",
            "Batch: 44   Training loss: 0.18105514347553253\n",
            "Precision: 75.80 +/- 24.4 %\n",
            "Recall: 30.18 +/- 12.0 %\n",
            "Batch: 45   Training loss: 0.15011906623840332\n",
            "Precision: 52.48 +/- 36.1 %\n",
            "Recall: 26.13 +/- 18.9 %\n",
            "Batch: 46   Training loss: 0.16663986444473267\n",
            "Precision: 54.75 +/- 36.2 %\n",
            "Recall: 26.11 +/- 18.3 %\n",
            "Batch: 47   Training loss: 0.15279987454414368\n",
            "Precision: 75.90 +/- 29.3 %\n",
            "Recall: 34.78 +/- 13.6 %\n",
            "Batch: 48   Training loss: 0.16732385754585266\n",
            "Precision: 59.82 +/- 35.1 %\n",
            "Recall: 24.85 +/- 16.3 %\n",
            "[5,    50] loss: 0.173\n",
            "Batch: 49   Training loss: 0.17321862280368805\n",
            "Precision: 62.62 +/- 35.5 %\n",
            "Recall: 24.03 +/- 15.7 %\n",
            "Batch: 50   Training loss: 0.14179305732250214\n",
            "Precision: 58.51 +/- 41.5 %\n",
            "Recall: 21.37 +/- 17.2 %\n",
            "Batch: 51   Training loss: 0.16281159222126007\n",
            "Precision: 65.18 +/- 42.1 %\n",
            "Recall: 21.18 +/- 16.2 %\n",
            "Batch: 52   Training loss: 0.16946767270565033\n",
            "Precision: 71.73 +/- 41.5 %\n",
            "Recall: 17.54 +/- 11.9 %\n",
            "Batch: 53   Training loss: 0.16467829048633575\n",
            "Precision: 61.37 +/- 42.8 %\n",
            "Recall: 18.73 +/- 15.4 %\n",
            "Batch: 54   Training loss: 0.16732971370220184\n",
            "Precision: 58.82 +/- 35.1 %\n",
            "Recall: 25.45 +/- 16.6 %\n",
            "Batch: 55   Training loss: 0.16878639161586761\n",
            "Precision: 55.15 +/- 35.5 %\n",
            "Recall: 27.73 +/- 17.7 %\n",
            "Batch: 56   Training loss: 0.15823160111904144\n",
            "Precision: 57.13 +/- 28.8 %\n",
            "Recall: 33.17 +/- 15.4 %\n",
            "Batch: 57   Training loss: 0.17116887867450714\n",
            "Precision: 65.90 +/- 34.8 %\n",
            "Recall: 30.39 +/- 16.4 %\n",
            "Batch: 58   Training loss: 0.17494332790374756\n",
            "Precision: 67.98 +/- 32.4 %\n",
            "Recall: 25.74 +/- 15.3 %\n",
            "Batch: 59   Training loss: 0.17387521266937256\n",
            "Precision: 66.73 +/- 38.2 %\n",
            "Recall: 20.71 +/- 13.4 %\n",
            "Batch: 60   Training loss: 0.1608256697654724\n",
            "Precision: 72.56 +/- 37.8 %\n",
            "Recall: 23.26 +/- 14.8 %\n",
            "Batch: 61   Training loss: 0.16318361461162567\n",
            "Precision: 56.85 +/- 47.1 %\n",
            "Recall: 17.11 +/- 17.2 %\n",
            "Batch: 62   Training loss: 0.19176627695560455\n",
            "Precision: 77.98 +/- 38.8 %\n",
            "Recall: 18.26 +/- 13.5 %\n",
            "Batch: 63   Training loss: 0.18374662101268768\n",
            "Precision: 63.63 +/- 38.2 %\n",
            "Recall: 18.33 +/- 13.9 %\n",
            "Batch: 64   Training loss: 0.16719463467597961\n",
            "Precision: 68.23 +/- 30.4 %\n",
            "Recall: 29.34 +/- 14.5 %\n",
            "Batch: 65   Training loss: 0.17824964225292206\n",
            "Precision: 57.93 +/- 34.2 %\n",
            "Recall: 29.76 +/- 17.8 %\n",
            "Batch: 66   Training loss: 0.17655768990516663\n",
            "Precision: 64.31 +/- 31.0 %\n",
            "Recall: 36.05 +/- 15.8 %\n",
            "Batch: 67   Training loss: 0.15935398638248444\n",
            "Precision: 59.09 +/- 30.8 %\n",
            "Recall: 31.96 +/- 15.7 %\n",
            "Batch: 68   Training loss: 0.158699631690979\n",
            "Precision: 73.21 +/- 34.3 %\n",
            "Recall: 28.84 +/- 16.6 %\n",
            "Batch: 69   Training loss: 0.14106768369674683\n",
            "Precision: 67.98 +/- 42.5 %\n",
            "Recall: 24.95 +/- 18.3 %\n",
            "Batch: 70   Training loss: 0.17268134653568268\n",
            "Precision: 58.33 +/- 46.0 %\n",
            "Recall: 18.64 +/- 18.4 %\n",
            "Batch: 71   Training loss: 0.16876037418842316\n",
            "Precision: 77.86 +/- 40.8 %\n",
            "Recall: 22.00 +/- 15.6 %\n",
            "Batch: 72   Training loss: 0.1738583743572235\n",
            "Precision: 68.04 +/- 41.4 %\n",
            "Recall: 18.62 +/- 13.8 %\n",
            "Batch: 73   Training loss: 0.18337403237819672\n",
            "Precision: 68.39 +/- 34.5 %\n",
            "Recall: 22.72 +/- 13.9 %\n",
            "Batch: 74   Training loss: 0.1800471693277359\n",
            "Precision: 56.84 +/- 37.4 %\n",
            "Recall: 28.25 +/- 19.2 %\n",
            "Batch: 75   Training loss: 0.16003286838531494\n",
            "Precision: 57.32 +/- 35.7 %\n",
            "Recall: 28.78 +/- 17.4 %\n",
            "Batch: 76   Training loss: 0.17192357778549194\n",
            "Precision: 56.51 +/- 35.3 %\n",
            "Recall: 27.28 +/- 16.4 %\n",
            "Batch: 77   Training loss: 0.16081251204013824\n",
            "Precision: 73.15 +/- 34.1 %\n",
            "Recall: 29.92 +/- 15.1 %\n",
            "Batch: 78   Training loss: 0.16513176262378693\n",
            "Precision: 56.31 +/- 38.8 %\n",
            "Recall: 23.74 +/- 18.4 %\n",
            "Batch: 79   Training loss: 0.16588954627513885\n",
            "Precision: 69.29 +/- 39.9 %\n",
            "Recall: 22.04 +/- 15.1 %\n",
            "Batch: 80   Training loss: 0.15724386274814606\n",
            "Precision: 62.13 +/- 39.4 %\n",
            "Recall: 23.98 +/- 16.4 %\n",
            "Batch: 81   Training loss: 0.18239814043045044\n",
            "Precision: 66.07 +/- 35.2 %\n",
            "Recall: 24.58 +/- 15.2 %\n",
            "Batch: 82   Training loss: 0.17304487526416779\n",
            "Precision: 57.08 +/- 40.3 %\n",
            "Recall: 20.12 +/- 14.9 %\n",
            "Batch: 83   Training loss: 0.15761840343475342\n",
            "Precision: 68.18 +/- 38.6 %\n",
            "Recall: 26.37 +/- 18.1 %\n",
            "Batch: 84   Training loss: 0.17521360516548157\n",
            "Precision: 76.67 +/- 36.7 %\n",
            "Recall: 23.93 +/- 15.4 %\n",
            "Batch: 85   Training loss: 0.15634964406490326\n",
            "Precision: 64.05 +/- 42.3 %\n",
            "Recall: 23.06 +/- 17.5 %\n",
            "Batch: 86   Training loss: 0.15843574702739716\n",
            "Precision: 66.96 +/- 46.3 %\n",
            "Recall: 17.91 +/- 15.6 %\n",
            "Batch: 87   Training loss: 0.17042700946331024\n",
            "Precision: 64.76 +/- 44.9 %\n",
            "Recall: 18.53 +/- 16.2 %\n",
            "Batch: 88   Training loss: 0.1616448611021042\n",
            "Precision: 81.85 +/- 27.8 %\n",
            "Recall: 28.62 +/- 14.6 %\n",
            "Batch: 89   Training loss: 0.18021678924560547\n",
            "Precision: 71.13 +/- 33.8 %\n",
            "Recall: 24.97 +/- 14.3 %\n",
            "Batch: 90   Training loss: 0.16064655780792236\n",
            "Precision: 56.00 +/- 35.0 %\n",
            "Recall: 26.38 +/- 18.2 %\n",
            "Batch: 91   Training loss: 0.16677170991897583\n",
            "Precision: 61.63 +/- 37.5 %\n",
            "Recall: 24.63 +/- 16.2 %\n",
            "Batch: 92   Training loss: 0.15680696070194244\n",
            "Precision: 55.86 +/- 33.6 %\n",
            "Recall: 29.36 +/- 19.0 %\n",
            "Epoch:  5\n",
            "Batch: 0   Training loss: 0.17469313740730286\n",
            "Precision: 73.81 +/- 30.7 %\n",
            "Recall: 29.92 +/- 12.5 %\n",
            "Batch: 1   Training loss: 0.1849198192358017\n",
            "Precision: 72.32 +/- 31.7 %\n",
            "Recall: 29.64 +/- 15.8 %\n",
            "Batch: 2   Training loss: 0.15330995619297028\n",
            "Precision: 68.10 +/- 36.0 %\n",
            "Recall: 27.19 +/- 16.4 %\n",
            "Batch: 3   Training loss: 0.16449110209941864\n",
            "Precision: 65.06 +/- 38.0 %\n",
            "Recall: 23.59 +/- 14.5 %\n",
            "Batch: 4   Training loss: 0.16052086651325226\n",
            "Precision: 66.67 +/- 38.6 %\n",
            "Recall: 20.62 +/- 12.4 %\n",
            "Batch: 5   Training loss: 0.16354414820671082\n",
            "Precision: 66.19 +/- 42.2 %\n",
            "Recall: 23.05 +/- 17.0 %\n",
            "Batch: 6   Training loss: 0.1725952923297882\n",
            "Precision: 81.10 +/- 26.7 %\n",
            "Recall: 27.05 +/- 13.7 %\n",
            "Batch: 7   Training loss: 0.17057150602340698\n",
            "Precision: 51.25 +/- 38.0 %\n",
            "Recall: 19.97 +/- 15.6 %\n",
            "Batch: 8   Training loss: 0.17284104228019714\n",
            "Precision: 57.38 +/- 38.9 %\n",
            "Recall: 21.42 +/- 15.9 %\n",
            "Batch: 9   Training loss: 0.15709300339221954\n",
            "Precision: 60.74 +/- 37.3 %\n",
            "Recall: 27.92 +/- 16.7 %\n",
            "Batch: 10   Training loss: 0.17244020104408264\n",
            "Precision: 53.25 +/- 36.3 %\n",
            "Recall: 24.79 +/- 18.4 %\n",
            "Batch: 11   Training loss: 0.1679161787033081\n",
            "Precision: 79.23 +/- 27.3 %\n",
            "Recall: 32.62 +/- 13.4 %\n",
            "Batch: 12   Training loss: 0.16201575100421906\n",
            "Precision: 72.86 +/- 37.0 %\n",
            "Recall: 26.41 +/- 16.2 %\n",
            "Batch: 13   Training loss: 0.1696528196334839\n",
            "Precision: 73.63 +/- 33.2 %\n",
            "Recall: 24.91 +/- 14.5 %\n",
            "Batch: 14   Training loss: 0.16687270998954773\n",
            "Precision: 77.38 +/- 29.2 %\n",
            "Recall: 27.29 +/- 15.3 %\n",
            "Batch: 15   Training loss: 0.1775561273097992\n",
            "Precision: 68.87 +/- 37.2 %\n",
            "Recall: 24.99 +/- 14.8 %\n",
            "Batch: 16   Training loss: 0.16947032511234283\n",
            "Precision: 60.56 +/- 35.6 %\n",
            "Recall: 25.14 +/- 16.9 %\n",
            "Batch: 17   Training loss: 0.16502562165260315\n",
            "Precision: 63.11 +/- 35.8 %\n",
            "Recall: 28.49 +/- 17.2 %\n",
            "Batch: 18   Training loss: 0.1601308435201645\n",
            "Precision: 61.03 +/- 38.0 %\n",
            "Recall: 29.24 +/- 19.0 %\n",
            "Batch: 19   Training loss: 0.15791566669940948\n",
            "Precision: 67.70 +/- 38.4 %\n",
            "Recall: 29.34 +/- 17.5 %\n",
            "Batch: 20   Training loss: 0.1570686548948288\n",
            "Precision: 55.83 +/- 41.8 %\n",
            "Recall: 22.52 +/- 18.6 %\n",
            "Batch: 21   Training loss: 0.1396549791097641\n",
            "Precision: 55.24 +/- 41.3 %\n",
            "Recall: 23.16 +/- 19.5 %\n",
            "Batch: 22   Training loss: 0.1514788568019867\n",
            "Precision: 68.81 +/- 38.2 %\n",
            "Recall: 26.10 +/- 16.7 %\n",
            "Batch: 23   Training loss: 0.17811034619808197\n",
            "Precision: 63.63 +/- 36.3 %\n",
            "Recall: 20.81 +/- 12.4 %\n",
            "Batch: 24   Training loss: 0.15308520197868347\n",
            "Precision: 68.77 +/- 28.9 %\n",
            "Recall: 31.69 +/- 14.7 %\n",
            "Batch: 25   Training loss: 0.16349460184574127\n",
            "Precision: 56.01 +/- 37.5 %\n",
            "Recall: 25.49 +/- 18.6 %\n",
            "Batch: 26   Training loss: 0.17201822996139526\n",
            "Precision: 53.48 +/- 40.1 %\n",
            "Recall: 21.20 +/- 17.2 %\n",
            "Batch: 27   Training loss: 0.19406934082508087\n",
            "Precision: 66.13 +/- 39.7 %\n",
            "Recall: 19.30 +/- 14.4 %\n",
            "Batch: 28   Training loss: 0.15618672966957092\n",
            "Precision: 69.23 +/- 42.9 %\n",
            "Recall: 19.85 +/- 14.9 %\n",
            "Batch: 29   Training loss: 0.16258732974529266\n",
            "Precision: 52.74 +/- 38.2 %\n",
            "Recall: 19.57 +/- 17.4 %\n",
            "Batch: 30   Training loss: 0.17706848680973053\n",
            "Precision: 77.20 +/- 34.1 %\n",
            "Recall: 27.17 +/- 15.0 %\n",
            "Batch: 31   Training loss: 0.1707753986120224\n",
            "Precision: 74.71 +/- 24.4 %\n",
            "Recall: 35.14 +/- 13.0 %\n",
            "Batch: 32   Training loss: 0.18244004249572754\n",
            "Precision: 54.70 +/- 34.9 %\n",
            "Recall: 27.15 +/- 16.7 %\n",
            "Batch: 33   Training loss: 0.16997858881950378\n",
            "Precision: 60.20 +/- 33.5 %\n",
            "Recall: 24.99 +/- 15.1 %\n",
            "Batch: 34   Training loss: 0.15987619757652283\n",
            "Precision: 63.95 +/- 39.6 %\n",
            "Recall: 24.45 +/- 17.5 %\n",
            "Batch: 35   Training loss: 0.16154909133911133\n",
            "Precision: 80.60 +/- 30.9 %\n",
            "Recall: 28.82 +/- 15.9 %\n",
            "Batch: 36   Training loss: 0.17654217779636383\n",
            "Precision: 68.21 +/- 39.0 %\n",
            "Recall: 22.63 +/- 14.9 %\n",
            "Batch: 37   Training loss: 0.17470289766788483\n",
            "Precision: 66.96 +/- 34.8 %\n",
            "Recall: 22.54 +/- 15.0 %\n",
            "Batch: 38   Training loss: 0.15224133431911469\n",
            "Precision: 61.58 +/- 37.1 %\n",
            "Recall: 25.99 +/- 18.7 %\n",
            "Batch: 39   Training loss: 0.18418937921524048\n",
            "Precision: 59.94 +/- 39.3 %\n",
            "Recall: 17.15 +/- 13.8 %\n",
            "Batch: 40   Training loss: 0.15115882456302643\n",
            "Precision: 70.00 +/- 40.1 %\n",
            "Recall: 24.90 +/- 17.3 %\n",
            "Batch: 41   Training loss: 0.15956029295921326\n",
            "Precision: 56.06 +/- 40.9 %\n",
            "Recall: 22.56 +/- 19.2 %\n",
            "Batch: 42   Training loss: 0.15628744661808014\n",
            "Precision: 59.88 +/- 40.4 %\n",
            "Recall: 20.36 +/- 15.6 %\n",
            "Batch: 43   Training loss: 0.17840628325939178\n",
            "Precision: 65.89 +/- 35.2 %\n",
            "Recall: 22.71 +/- 13.7 %\n",
            "Batch: 44   Training loss: 0.18007904291152954\n",
            "Precision: 69.70 +/- 35.6 %\n",
            "Recall: 23.34 +/- 13.6 %\n",
            "Batch: 45   Training loss: 0.17640364170074463\n",
            "Precision: 76.07 +/- 28.4 %\n",
            "Recall: 30.76 +/- 12.5 %\n",
            "Batch: 46   Training loss: 0.1725764274597168\n",
            "Precision: 66.85 +/- 32.9 %\n",
            "Recall: 29.17 +/- 16.6 %\n",
            "Batch: 47   Training loss: 0.18830250203609467\n",
            "Precision: 69.65 +/- 34.6 %\n",
            "Recall: 30.09 +/- 16.0 %\n",
            "Batch: 48   Training loss: 0.15545305609703064\n",
            "Precision: 55.20 +/- 40.9 %\n",
            "Recall: 21.69 +/- 16.5 %\n",
            "[6,    50] loss: 0.183\n",
            "Batch: 49   Training loss: 0.18320471048355103\n",
            "Precision: 65.24 +/- 38.9 %\n",
            "Recall: 24.15 +/- 16.6 %\n",
            "Batch: 50   Training loss: 0.15729470551013947\n",
            "Precision: 57.98 +/- 37.1 %\n",
            "Recall: 26.60 +/- 17.5 %\n",
            "Batch: 51   Training loss: 0.15754859149456024\n",
            "Precision: 55.60 +/- 38.6 %\n",
            "Recall: 25.31 +/- 19.8 %\n",
            "Batch: 52   Training loss: 0.1655726432800293\n",
            "Precision: 53.35 +/- 39.7 %\n",
            "Recall: 23.41 +/- 18.3 %\n",
            "Batch: 53   Training loss: 0.15791639685630798\n",
            "Precision: 56.56 +/- 42.7 %\n",
            "Recall: 24.15 +/- 19.3 %\n",
            "Batch: 54   Training loss: 0.17176862061023712\n",
            "Precision: 71.30 +/- 36.0 %\n",
            "Recall: 26.00 +/- 14.6 %\n",
            "Batch: 55   Training loss: 0.17550310492515564\n",
            "Precision: 58.10 +/- 41.9 %\n",
            "Recall: 21.12 +/- 16.0 %\n",
            "Batch: 56   Training loss: 0.18186677992343903\n",
            "Precision: 71.57 +/- 34.2 %\n",
            "Recall: 24.37 +/- 13.9 %\n",
            "Batch: 57   Training loss: 0.17654170095920563\n",
            "Precision: 56.36 +/- 35.9 %\n",
            "Recall: 26.10 +/- 17.5 %\n",
            "Batch: 58   Training loss: 0.17527826130390167\n",
            "Precision: 67.70 +/- 35.7 %\n",
            "Recall: 27.96 +/- 14.0 %\n",
            "Batch: 59   Training loss: 0.15991678833961487\n",
            "Precision: 55.37 +/- 34.1 %\n",
            "Recall: 26.96 +/- 18.0 %\n",
            "Batch: 60   Training loss: 0.16873639822006226\n",
            "Precision: 55.08 +/- 36.1 %\n",
            "Recall: 25.70 +/- 18.2 %\n",
            "Batch: 61   Training loss: 0.15930548310279846\n",
            "Precision: 57.61 +/- 35.9 %\n",
            "Recall: 23.41 +/- 18.7 %\n",
            "Batch: 62   Training loss: 0.15086986124515533\n",
            "Precision: 74.49 +/- 34.2 %\n",
            "Recall: 28.96 +/- 16.5 %\n",
            "Batch: 63   Training loss: 0.16792131960391998\n",
            "Precision: 77.72 +/- 30.8 %\n",
            "Recall: 28.93 +/- 15.9 %\n",
            "Batch: 64   Training loss: 0.18365436792373657\n",
            "Precision: 50.65 +/- 40.9 %\n",
            "Recall: 18.84 +/- 16.4 %\n",
            "Batch: 65   Training loss: 0.17231373488903046\n",
            "Precision: 66.73 +/- 37.9 %\n",
            "Recall: 24.22 +/- 16.2 %\n",
            "Batch: 66   Training loss: 0.14437538385391235\n",
            "Precision: 63.99 +/- 39.4 %\n",
            "Recall: 23.71 +/- 16.6 %\n",
            "Batch: 67   Training loss: 0.16960398852825165\n",
            "Precision: 74.88 +/- 35.5 %\n",
            "Recall: 23.75 +/- 13.2 %\n",
            "Batch: 68   Training loss: 0.16216062009334564\n",
            "Precision: 65.83 +/- 41.4 %\n",
            "Recall: 22.97 +/- 17.1 %\n",
            "Batch: 69   Training loss: 0.19666054844856262\n",
            "Precision: 67.62 +/- 39.6 %\n",
            "Recall: 19.40 +/- 14.0 %\n",
            "Batch: 70   Training loss: 0.16304214298725128\n",
            "Precision: 79.94 +/- 34.5 %\n",
            "Recall: 27.31 +/- 16.3 %\n",
            "Batch: 71   Training loss: 0.16909392178058624\n",
            "Precision: 68.10 +/- 36.1 %\n",
            "Recall: 27.35 +/- 16.8 %\n",
            "Batch: 72   Training loss: 0.1542683094739914\n",
            "Precision: 64.15 +/- 36.0 %\n",
            "Recall: 27.33 +/- 16.8 %\n",
            "Batch: 73   Training loss: 0.1606491208076477\n",
            "Precision: 46.25 +/- 37.6 %\n",
            "Recall: 19.10 +/- 17.7 %\n",
            "Batch: 74   Training loss: 0.16144567728042603\n",
            "Precision: 70.12 +/- 34.1 %\n",
            "Recall: 27.00 +/- 15.7 %\n",
            "Batch: 75   Training loss: 0.17829541862010956\n",
            "Precision: 71.07 +/- 32.7 %\n",
            "Recall: 27.05 +/- 14.6 %\n",
            "Batch: 76   Training loss: 0.18166877329349518\n",
            "Precision: 61.49 +/- 33.7 %\n",
            "Recall: 24.06 +/- 14.5 %\n",
            "Batch: 77   Training loss: 0.16836871206760406\n",
            "Precision: 59.94 +/- 36.3 %\n",
            "Recall: 24.31 +/- 16.5 %\n",
            "Batch: 78   Training loss: 0.15514856576919556\n",
            "Precision: 62.00 +/- 34.7 %\n",
            "Recall: 29.01 +/- 17.2 %\n",
            "Batch: 79   Training loss: 0.1603032499551773\n",
            "Precision: 65.00 +/- 32.1 %\n",
            "Recall: 28.90 +/- 14.5 %\n",
            "Batch: 80   Training loss: 0.16170227527618408\n",
            "Precision: 55.83 +/- 34.4 %\n",
            "Recall: 24.82 +/- 16.6 %\n",
            "Batch: 81   Training loss: 0.15667368471622467\n",
            "Precision: 77.28 +/- 29.4 %\n",
            "Recall: 32.18 +/- 16.2 %\n",
            "Batch: 82   Training loss: 0.15801706910133362\n",
            "Precision: 66.98 +/- 37.4 %\n",
            "Recall: 25.87 +/- 17.5 %\n",
            "Batch: 83   Training loss: 0.16063976287841797\n",
            "Precision: 70.48 +/- 40.9 %\n",
            "Recall: 23.46 +/- 16.1 %\n",
            "Batch: 84   Training loss: 0.16846154630184174\n",
            "Precision: 72.01 +/- 33.6 %\n",
            "Recall: 27.71 +/- 15.2 %\n",
            "Batch: 85   Training loss: 0.15174038708209991\n",
            "Precision: 54.78 +/- 40.6 %\n",
            "Recall: 24.20 +/- 19.8 %\n",
            "Batch: 86   Training loss: 0.17932459712028503\n",
            "Precision: 59.98 +/- 31.9 %\n",
            "Recall: 25.32 +/- 13.6 %\n",
            "Batch: 87   Training loss: 0.1746213138103485\n",
            "Precision: 59.01 +/- 36.1 %\n",
            "Recall: 29.97 +/- 18.5 %\n",
            "Batch: 88   Training loss: 0.1598140150308609\n",
            "Precision: 71.70 +/- 27.4 %\n",
            "Recall: 39.00 +/- 15.7 %\n",
            "Batch: 89   Training loss: 0.18509510159492493\n",
            "Precision: 55.19 +/- 36.1 %\n",
            "Recall: 27.76 +/- 19.7 %\n",
            "Batch: 90   Training loss: 0.18246771395206451\n",
            "Precision: 71.23 +/- 37.1 %\n",
            "Recall: 21.08 +/- 14.7 %\n",
            "Batch: 91   Training loss: 0.1679336428642273\n",
            "Precision: 58.45 +/- 40.9 %\n",
            "Recall: 21.49 +/- 15.8 %\n",
            "Batch: 92   Training loss: 0.1685553640127182\n",
            "Precision: 61.50 +/- 33.4 %\n",
            "Recall: 28.26 +/- 16.2 %\n",
            "Epoch:  6\n",
            "Batch: 0   Training loss: 0.17455743253231049\n",
            "Precision: 75.35 +/- 28.2 %\n",
            "Recall: 27.73 +/- 11.3 %\n",
            "Batch: 1   Training loss: 0.1635613888502121\n",
            "Precision: 64.93 +/- 37.0 %\n",
            "Recall: 23.39 +/- 15.3 %\n",
            "Batch: 2   Training loss: 0.18137052655220032\n",
            "Precision: 63.78 +/- 36.5 %\n",
            "Recall: 22.82 +/- 14.3 %\n",
            "Batch: 3   Training loss: 0.1600179821252823\n",
            "Precision: 66.92 +/- 35.4 %\n",
            "Recall: 30.73 +/- 18.6 %\n",
            "Batch: 4   Training loss: 0.16872912645339966\n",
            "Precision: 59.22 +/- 37.2 %\n",
            "Recall: 28.30 +/- 16.4 %\n",
            "Batch: 5   Training loss: 0.16390632092952728\n",
            "Precision: 61.08 +/- 36.9 %\n",
            "Recall: 29.02 +/- 18.6 %\n",
            "Batch: 6   Training loss: 0.15273573994636536\n",
            "Precision: 62.22 +/- 35.9 %\n",
            "Recall: 26.25 +/- 14.5 %\n",
            "Batch: 7   Training loss: 0.16045241057872772\n",
            "Precision: 63.56 +/- 41.0 %\n",
            "Recall: 24.00 +/- 17.0 %\n",
            "Batch: 8   Training loss: 0.1680034101009369\n",
            "Precision: 66.31 +/- 37.2 %\n",
            "Recall: 26.94 +/- 16.8 %\n",
            "Batch: 9   Training loss: 0.15021005272865295\n",
            "Precision: 59.58 +/- 39.8 %\n",
            "Recall: 27.66 +/- 19.4 %\n",
            "Batch: 10   Training loss: 0.1589718759059906\n",
            "Precision: 65.60 +/- 36.5 %\n",
            "Recall: 28.38 +/- 16.9 %\n",
            "Batch: 11   Training loss: 0.17765802145004272\n",
            "Precision: 62.89 +/- 40.5 %\n",
            "Recall: 22.42 +/- 17.8 %\n",
            "Batch: 12   Training loss: 0.13966001570224762\n",
            "Precision: 52.47 +/- 38.9 %\n",
            "Recall: 29.46 +/- 21.6 %\n",
            "Batch: 13   Training loss: 0.1546698808670044\n",
            "Precision: 57.47 +/- 34.9 %\n",
            "Recall: 30.82 +/- 22.3 %\n",
            "Batch: 14   Training loss: 0.16847650706768036\n",
            "Precision: 66.20 +/- 30.2 %\n",
            "Recall: 33.94 +/- 18.0 %\n",
            "Batch: 15   Training loss: 0.16834641993045807\n",
            "Precision: 71.70 +/- 32.3 %\n",
            "Recall: 32.87 +/- 16.7 %\n",
            "Batch: 16   Training loss: 0.18845120072364807\n",
            "Precision: 59.02 +/- 36.7 %\n",
            "Recall: 24.46 +/- 16.3 %\n",
            "Batch: 17   Training loss: 0.15290895104408264\n",
            "Precision: 61.45 +/- 35.8 %\n",
            "Recall: 24.23 +/- 15.2 %\n",
            "Batch: 18   Training loss: 0.1489492803812027\n",
            "Precision: 60.35 +/- 39.8 %\n",
            "Recall: 28.32 +/- 19.2 %\n",
            "Batch: 19   Training loss: 0.16928736865520477\n",
            "Precision: 62.98 +/- 37.6 %\n",
            "Recall: 23.55 +/- 15.3 %\n",
            "Batch: 20   Training loss: 0.16269071400165558\n",
            "Precision: 59.73 +/- 38.4 %\n",
            "Recall: 27.20 +/- 18.7 %\n",
            "Batch: 21   Training loss: 0.17644627392292023\n",
            "Precision: 66.74 +/- 32.3 %\n",
            "Recall: 33.23 +/- 16.2 %\n",
            "Batch: 22   Training loss: 0.17694327235221863\n",
            "Precision: 55.80 +/- 33.9 %\n",
            "Recall: 26.97 +/- 17.4 %\n",
            "Batch: 23   Training loss: 0.15994177758693695\n",
            "Precision: 68.76 +/- 33.3 %\n",
            "Recall: 29.71 +/- 15.7 %\n",
            "Batch: 24   Training loss: 0.1688871830701828\n",
            "Precision: 59.54 +/- 35.3 %\n",
            "Recall: 22.94 +/- 15.9 %\n",
            "Batch: 25   Training loss: 0.16413111984729767\n",
            "Precision: 68.95 +/- 31.4 %\n",
            "Recall: 25.84 +/- 15.6 %\n",
            "Batch: 26   Training loss: 0.1437997668981552\n",
            "Precision: 51.70 +/- 42.6 %\n",
            "Recall: 23.62 +/- 20.8 %\n",
            "Batch: 27   Training loss: 0.1558508574962616\n",
            "Precision: 63.90 +/- 38.9 %\n",
            "Recall: 25.77 +/- 18.6 %\n",
            "Batch: 28   Training loss: 0.15669475495815277\n",
            "Precision: 65.37 +/- 36.1 %\n",
            "Recall: 27.93 +/- 17.4 %\n",
            "Batch: 29   Training loss: 0.17785383760929108\n",
            "Precision: 57.08 +/- 33.3 %\n",
            "Recall: 28.03 +/- 16.3 %\n",
            "Batch: 30   Training loss: 0.17732982337474823\n",
            "Precision: 65.81 +/- 35.4 %\n",
            "Recall: 30.06 +/- 17.0 %\n",
            "Batch: 31   Training loss: 0.16429376602172852\n",
            "Precision: 61.45 +/- 40.4 %\n",
            "Recall: 24.51 +/- 17.7 %\n",
            "Batch: 32   Training loss: 0.15490388870239258\n",
            "Precision: 65.73 +/- 38.7 %\n",
            "Recall: 29.64 +/- 17.9 %\n",
            "Batch: 33   Training loss: 0.15645983815193176\n",
            "Precision: 61.19 +/- 39.8 %\n",
            "Recall: 23.13 +/- 17.9 %\n",
            "Batch: 34   Training loss: 0.1678226739168167\n",
            "Precision: 63.57 +/- 44.1 %\n",
            "Recall: 19.41 +/- 17.0 %\n",
            "Batch: 35   Training loss: 0.16863366961479187\n",
            "Precision: 65.83 +/- 41.6 %\n",
            "Recall: 19.25 +/- 15.6 %\n",
            "Batch: 36   Training loss: 0.1806323528289795\n",
            "Precision: 55.95 +/- 41.2 %\n",
            "Recall: 16.09 +/- 14.7 %\n",
            "Batch: 37   Training loss: 0.15495386719703674\n",
            "Precision: 58.89 +/- 40.1 %\n",
            "Recall: 20.87 +/- 16.0 %\n",
            "Batch: 38   Training loss: 0.1546282172203064\n",
            "Precision: 63.23 +/- 39.5 %\n",
            "Recall: 29.14 +/- 20.8 %\n",
            "Batch: 39   Training loss: 0.16080719232559204\n",
            "Precision: 57.68 +/- 29.1 %\n",
            "Recall: 33.46 +/- 18.3 %\n",
            "Batch: 40   Training loss: 0.15695661306381226\n",
            "Precision: 61.84 +/- 34.9 %\n",
            "Recall: 32.87 +/- 18.2 %\n",
            "Batch: 41   Training loss: 0.15864643454551697\n",
            "Precision: 61.95 +/- 35.9 %\n",
            "Recall: 30.79 +/- 17.2 %\n",
            "Batch: 42   Training loss: 0.16335347294807434\n",
            "Precision: 57.14 +/- 41.3 %\n",
            "Recall: 24.02 +/- 19.2 %\n",
            "Batch: 43   Training loss: 0.16858820617198944\n",
            "Precision: 71.13 +/- 39.2 %\n",
            "Recall: 20.54 +/- 14.7 %\n",
            "Batch: 44   Training loss: 0.16802625358104706\n",
            "Precision: 73.81 +/- 43.1 %\n",
            "Recall: 20.41 +/- 16.3 %\n",
            "Batch: 45   Training loss: 0.1843487024307251\n",
            "Precision: 85.00 +/- 34.9 %\n",
            "Recall: 20.51 +/- 12.6 %\n",
            "Batch: 46   Training loss: 0.1731678545475006\n",
            "Precision: 59.11 +/- 41.9 %\n",
            "Recall: 19.16 +/- 14.8 %\n",
            "Batch: 47   Training loss: 0.17508940398693085\n",
            "Precision: 71.96 +/- 32.0 %\n",
            "Recall: 24.98 +/- 13.4 %\n",
            "Batch: 48   Training loss: 0.16174985468387604\n",
            "Precision: 58.61 +/- 34.4 %\n",
            "Recall: 35.15 +/- 19.3 %\n",
            "[7,    50] loss: 0.163\n",
            "Batch: 49   Training loss: 0.1630076915025711\n",
            "Precision: 60.74 +/- 30.5 %\n",
            "Recall: 37.97 +/- 17.8 %\n",
            "Batch: 50   Training loss: 0.1695232391357422\n",
            "Precision: 56.86 +/- 28.3 %\n",
            "Recall: 34.92 +/- 17.5 %\n",
            "Batch: 51   Training loss: 0.18980289995670319\n",
            "Precision: 62.10 +/- 37.5 %\n",
            "Recall: 24.71 +/- 16.6 %\n",
            "Batch: 52   Training loss: 0.17426177859306335\n",
            "Precision: 54.58 +/- 44.0 %\n",
            "Recall: 15.79 +/- 15.2 %\n",
            "Batch: 53   Training loss: 0.16338689625263214\n",
            "Precision: 67.76 +/- 39.3 %\n",
            "Recall: 21.44 +/- 15.2 %\n",
            "Batch: 54   Training loss: 0.1580173522233963\n",
            "Precision: 52.47 +/- 45.4 %\n",
            "Recall: 17.74 +/- 18.3 %\n",
            "Batch: 55   Training loss: 0.15835094451904297\n",
            "Precision: 59.97 +/- 42.4 %\n",
            "Recall: 21.03 +/- 16.5 %\n",
            "Batch: 56   Training loss: 0.16948777437210083\n",
            "Precision: 64.79 +/- 34.4 %\n",
            "Recall: 29.44 +/- 16.5 %\n",
            "Batch: 57   Training loss: 0.16813313961029053\n",
            "Precision: 61.85 +/- 30.1 %\n",
            "Recall: 36.40 +/- 18.4 %\n",
            "Batch: 58   Training loss: 0.17085440456867218\n",
            "Precision: 58.50 +/- 32.2 %\n",
            "Recall: 35.44 +/- 17.8 %\n",
            "Batch: 59   Training loss: 0.17954270541667938\n",
            "Precision: 61.46 +/- 25.2 %\n",
            "Recall: 39.03 +/- 13.1 %\n",
            "Batch: 60   Training loss: 0.17869658768177032\n",
            "Precision: 66.36 +/- 28.3 %\n",
            "Recall: 37.89 +/- 17.4 %\n",
            "Batch: 61   Training loss: 0.16890452802181244\n",
            "Precision: 65.59 +/- 32.6 %\n",
            "Recall: 30.91 +/- 14.7 %\n",
            "Batch: 62   Training loss: 0.1525687426328659\n",
            "Precision: 61.37 +/- 37.9 %\n",
            "Recall: 28.90 +/- 19.2 %\n",
            "Batch: 63   Training loss: 0.16093562543392181\n",
            "Precision: 77.56 +/- 38.6 %\n",
            "Recall: 24.34 +/- 15.8 %\n",
            "Batch: 64   Training loss: 0.16890780627727509\n",
            "Precision: 60.36 +/- 43.9 %\n",
            "Recall: 18.39 +/- 16.3 %\n",
            "Batch: 65   Training loss: 0.17486143112182617\n",
            "Precision: 71.79 +/- 38.8 %\n",
            "Recall: 19.82 +/- 14.4 %\n",
            "Batch: 66   Training loss: 0.18495212495326996\n",
            "Precision: 70.12 +/- 39.5 %\n",
            "Recall: 22.61 +/- 15.3 %\n",
            "Batch: 67   Training loss: 0.1636286824941635\n",
            "Precision: 64.57 +/- 30.3 %\n",
            "Recall: 27.33 +/- 15.1 %\n",
            "Batch: 68   Training loss: 0.1609739363193512\n",
            "Precision: 56.06 +/- 36.1 %\n",
            "Recall: 28.10 +/- 18.8 %\n",
            "Batch: 69   Training loss: 0.16142509877681732\n",
            "Precision: 67.77 +/- 30.4 %\n",
            "Recall: 36.21 +/- 17.4 %\n",
            "Batch: 70   Training loss: 0.1423806995153427\n",
            "Precision: 58.78 +/- 34.4 %\n",
            "Recall: 32.03 +/- 19.0 %\n",
            "Batch: 71   Training loss: 0.17811214923858643\n",
            "Precision: 56.80 +/- 36.4 %\n",
            "Recall: 26.61 +/- 17.4 %\n",
            "Batch: 72   Training loss: 0.16099628806114197\n",
            "Precision: 69.14 +/- 32.7 %\n",
            "Recall: 33.22 +/- 18.4 %\n",
            "Batch: 73   Training loss: 0.17025958001613617\n",
            "Precision: 63.58 +/- 34.2 %\n",
            "Recall: 26.51 +/- 16.9 %\n",
            "Batch: 74   Training loss: 0.1641404628753662\n",
            "Precision: 55.16 +/- 35.3 %\n",
            "Recall: 24.84 +/- 18.5 %\n",
            "Batch: 75   Training loss: 0.1695794314146042\n",
            "Precision: 66.96 +/- 34.8 %\n",
            "Recall: 29.22 +/- 16.3 %\n",
            "Batch: 76   Training loss: 0.1575208455324173\n",
            "Precision: 58.84 +/- 35.9 %\n",
            "Recall: 28.83 +/- 18.8 %\n",
            "Batch: 77   Training loss: 0.16631323099136353\n",
            "Precision: 58.68 +/- 39.6 %\n",
            "Recall: 24.41 +/- 17.5 %\n",
            "Batch: 78   Training loss: 0.15602365136146545\n",
            "Precision: 74.13 +/- 31.9 %\n",
            "Recall: 32.79 +/- 15.4 %\n",
            "Batch: 79   Training loss: 0.1987576186656952\n",
            "Precision: 58.85 +/- 34.6 %\n",
            "Recall: 23.74 +/- 12.8 %\n",
            "Batch: 80   Training loss: 0.16642211377620697\n",
            "Precision: 71.58 +/- 32.0 %\n",
            "Recall: 31.96 +/- 13.6 %\n",
            "Batch: 81   Training loss: 0.16629737615585327\n",
            "Precision: 63.31 +/- 29.4 %\n",
            "Recall: 35.66 +/- 16.4 %\n",
            "Batch: 82   Training loss: 0.1660328060388565\n",
            "Precision: 58.66 +/- 37.4 %\n",
            "Recall: 30.83 +/- 20.3 %\n",
            "Batch: 83   Training loss: 0.16079948842525482\n",
            "Precision: 64.85 +/- 32.1 %\n",
            "Recall: 28.48 +/- 16.0 %\n",
            "Batch: 84   Training loss: 0.16938836872577667\n",
            "Precision: 68.88 +/- 33.0 %\n",
            "Recall: 29.50 +/- 18.5 %\n",
            "Batch: 85   Training loss: 0.16573017835617065\n",
            "Precision: 78.53 +/- 31.0 %\n",
            "Recall: 28.21 +/- 16.0 %\n",
            "Batch: 86   Training loss: 0.15347440540790558\n",
            "Precision: 63.03 +/- 38.8 %\n",
            "Recall: 24.88 +/- 18.0 %\n",
            "Batch: 87   Training loss: 0.17727451026439667\n",
            "Precision: 76.36 +/- 31.5 %\n",
            "Recall: 31.88 +/- 17.2 %\n",
            "Batch: 88   Training loss: 0.17273154854774475\n",
            "Precision: 61.23 +/- 30.3 %\n",
            "Recall: 28.51 +/- 14.1 %\n",
            "Batch: 89   Training loss: 0.1628219336271286\n",
            "Precision: 71.56 +/- 31.1 %\n",
            "Recall: 33.33 +/- 17.4 %\n",
            "Batch: 90   Training loss: 0.15359725058078766\n",
            "Precision: 49.70 +/- 33.3 %\n",
            "Recall: 26.04 +/- 17.4 %\n",
            "Batch: 91   Training loss: 0.1710502952337265\n",
            "Precision: 63.44 +/- 33.6 %\n",
            "Recall: 27.54 +/- 16.5 %\n",
            "Batch: 92   Training loss: 0.16544802486896515\n",
            "Precision: 56.84 +/- 35.6 %\n",
            "Recall: 24.69 +/- 17.0 %\n",
            "Epoch:  7\n",
            "Batch: 0   Training loss: 0.16284124553203583\n",
            "Precision: 70.62 +/- 33.8 %\n",
            "Recall: 28.25 +/- 14.8 %\n",
            "Batch: 1   Training loss: 0.15294407308101654\n",
            "Precision: 64.86 +/- 34.5 %\n",
            "Recall: 30.43 +/- 18.2 %\n",
            "Batch: 2   Training loss: 0.1719484180212021\n",
            "Precision: 65.38 +/- 29.9 %\n",
            "Recall: 29.86 +/- 15.3 %\n",
            "Batch: 3   Training loss: 0.15197916328907013\n",
            "Precision: 54.48 +/- 35.6 %\n",
            "Recall: 27.42 +/- 19.4 %\n",
            "Batch: 4   Training loss: 0.16845983266830444\n",
            "Precision: 64.61 +/- 31.4 %\n",
            "Recall: 37.24 +/- 16.6 %\n",
            "Batch: 5   Training loss: 0.1687440127134323\n",
            "Precision: 70.83 +/- 31.9 %\n",
            "Recall: 29.15 +/- 15.8 %\n",
            "Batch: 6   Training loss: 0.16232648491859436\n",
            "Precision: 55.71 +/- 39.4 %\n",
            "Recall: 25.76 +/- 18.8 %\n",
            "Batch: 7   Training loss: 0.14284881949424744\n",
            "Precision: 59.06 +/- 39.7 %\n",
            "Recall: 24.84 +/- 18.8 %\n",
            "Batch: 8   Training loss: 0.14626677334308624\n",
            "Precision: 58.89 +/- 37.2 %\n",
            "Recall: 26.97 +/- 19.4 %\n",
            "Batch: 9   Training loss: 0.16076698899269104\n",
            "Precision: 61.65 +/- 37.5 %\n",
            "Recall: 30.14 +/- 19.1 %\n",
            "Batch: 10   Training loss: 0.16980382800102234\n",
            "Precision: 64.29 +/- 32.7 %\n",
            "Recall: 31.47 +/- 15.6 %\n",
            "Batch: 11   Training loss: 0.17168028652668\n",
            "Precision: 62.19 +/- 32.5 %\n",
            "Recall: 31.35 +/- 16.8 %\n",
            "Batch: 12   Training loss: 0.15989266335964203\n",
            "Precision: 60.19 +/- 34.7 %\n",
            "Recall: 31.78 +/- 19.8 %\n",
            "Batch: 13   Training loss: 0.17616526782512665\n",
            "Precision: 63.29 +/- 32.4 %\n",
            "Recall: 29.07 +/- 16.1 %\n",
            "Batch: 14   Training loss: 0.17576460540294647\n",
            "Precision: 67.59 +/- 34.7 %\n",
            "Recall: 25.56 +/- 16.0 %\n",
            "Batch: 15   Training loss: 0.1871238648891449\n",
            "Precision: 67.26 +/- 39.6 %\n",
            "Recall: 20.67 +/- 15.2 %\n",
            "Batch: 16   Training loss: 0.18104463815689087\n",
            "Precision: 62.08 +/- 38.9 %\n",
            "Recall: 20.83 +/- 14.9 %\n",
            "Batch: 17   Training loss: 0.1703048199415207\n",
            "Precision: 56.63 +/- 39.9 %\n",
            "Recall: 22.51 +/- 19.2 %\n",
            "Batch: 18   Training loss: 0.1726212352514267\n",
            "Precision: 70.68 +/- 32.8 %\n",
            "Recall: 25.31 +/- 13.7 %\n",
            "Batch: 19   Training loss: 0.15903340280056\n",
            "Precision: 65.39 +/- 34.2 %\n",
            "Recall: 33.12 +/- 16.3 %\n",
            "Batch: 20   Training loss: 0.1754143089056015\n",
            "Precision: 47.72 +/- 33.0 %\n",
            "Recall: 27.72 +/- 18.5 %\n",
            "Batch: 21   Training loss: 0.16577352583408356\n",
            "Precision: 56.80 +/- 30.9 %\n",
            "Recall: 33.19 +/- 16.7 %\n",
            "Batch: 22   Training loss: 0.16668570041656494\n",
            "Precision: 51.02 +/- 37.2 %\n",
            "Recall: 25.80 +/- 18.8 %\n",
            "Batch: 23   Training loss: 0.186506450176239\n",
            "Precision: 67.88 +/- 29.6 %\n",
            "Recall: 29.30 +/- 14.5 %\n",
            "Batch: 24   Training loss: 0.17085812985897064\n",
            "Precision: 56.79 +/- 39.7 %\n",
            "Recall: 21.41 +/- 18.0 %\n",
            "Batch: 25   Training loss: 0.1462419629096985\n",
            "Precision: 67.86 +/- 46.7 %\n",
            "Recall: 17.34 +/- 14.8 %\n",
            "Batch: 26   Training loss: 0.15553432703018188\n",
            "Precision: 68.45 +/- 44.6 %\n",
            "Recall: 21.69 +/- 17.9 %\n",
            "Batch: 27   Training loss: 0.1702718883752823\n",
            "Precision: 73.10 +/- 36.8 %\n",
            "Recall: 20.81 +/- 14.2 %\n",
            "Batch: 28   Training loss: 0.16173699498176575\n",
            "Precision: 66.41 +/- 38.5 %\n",
            "Recall: 25.98 +/- 17.5 %\n",
            "Batch: 29   Training loss: 0.15578000247478485\n",
            "Precision: 56.45 +/- 37.1 %\n",
            "Recall: 28.98 +/- 19.8 %\n",
            "Batch: 30   Training loss: 0.14437825977802277\n",
            "Precision: 65.34 +/- 37.2 %\n",
            "Recall: 33.57 +/- 19.1 %\n",
            "Batch: 31   Training loss: 0.16900691390037537\n",
            "Precision: 67.10 +/- 29.6 %\n",
            "Recall: 35.58 +/- 13.2 %\n",
            "Batch: 32   Training loss: 0.16781318187713623\n",
            "Precision: 55.34 +/- 34.4 %\n",
            "Recall: 31.56 +/- 18.5 %\n",
            "Batch: 33   Training loss: 0.15636670589447021\n",
            "Precision: 57.79 +/- 37.2 %\n",
            "Recall: 29.34 +/- 18.6 %\n",
            "Batch: 34   Training loss: 0.1633426994085312\n",
            "Precision: 66.59 +/- 34.8 %\n",
            "Recall: 28.54 +/- 17.2 %\n",
            "Batch: 35   Training loss: 0.16776621341705322\n",
            "Precision: 73.92 +/- 31.9 %\n",
            "Recall: 32.75 +/- 16.6 %\n",
            "Batch: 36   Training loss: 0.16573484241962433\n",
            "Precision: 75.53 +/- 34.6 %\n",
            "Recall: 29.45 +/- 16.5 %\n",
            "Batch: 37   Training loss: 0.16760803759098053\n",
            "Precision: 48.05 +/- 42.6 %\n",
            "Recall: 19.68 +/- 18.9 %\n",
            "Batch: 38   Training loss: 0.16571252048015594\n",
            "Precision: 75.56 +/- 30.6 %\n",
            "Recall: 32.10 +/- 16.6 %\n",
            "Batch: 39   Training loss: 0.1625172197818756\n",
            "Precision: 64.26 +/- 32.0 %\n",
            "Recall: 32.34 +/- 16.5 %\n",
            "Batch: 40   Training loss: 0.1716783046722412\n",
            "Precision: 54.70 +/- 31.9 %\n",
            "Recall: 28.63 +/- 16.6 %\n",
            "Batch: 41   Training loss: 0.18381208181381226\n",
            "Precision: 57.40 +/- 32.2 %\n",
            "Recall: 32.03 +/- 19.8 %\n",
            "Batch: 42   Training loss: 0.1748570054769516\n",
            "Precision: 56.95 +/- 30.9 %\n",
            "Recall: 32.91 +/- 18.5 %\n",
            "Batch: 43   Training loss: 0.16152262687683105\n",
            "Precision: 68.17 +/- 26.2 %\n",
            "Recall: 38.44 +/- 14.0 %\n",
            "Batch: 44   Training loss: 0.1669888198375702\n",
            "Precision: 58.28 +/- 35.4 %\n",
            "Recall: 31.40 +/- 18.7 %\n",
            "Batch: 45   Training loss: 0.16758863627910614\n",
            "Precision: 55.21 +/- 35.2 %\n",
            "Recall: 25.02 +/- 16.4 %\n",
            "Batch: 46   Training loss: 0.1718575656414032\n",
            "Precision: 57.82 +/- 33.4 %\n",
            "Recall: 25.91 +/- 16.4 %\n",
            "Batch: 47   Training loss: 0.1779271364212036\n",
            "Precision: 66.51 +/- 33.9 %\n",
            "Recall: 26.06 +/- 14.7 %\n",
            "Batch: 48   Training loss: 0.16719883680343628\n",
            "Precision: 80.30 +/- 26.9 %\n",
            "Recall: 30.48 +/- 12.9 %\n",
            "[8,    50] loss: 0.148\n",
            "Batch: 49   Training loss: 0.1475202441215515\n",
            "Precision: 64.38 +/- 37.0 %\n",
            "Recall: 27.33 +/- 16.6 %\n",
            "Batch: 50   Training loss: 0.1686783730983734\n",
            "Precision: 69.94 +/- 33.4 %\n",
            "Recall: 28.05 +/- 14.1 %\n",
            "Batch: 51   Training loss: 0.1692550629377365\n",
            "Precision: 67.25 +/- 34.9 %\n",
            "Recall: 27.97 +/- 16.2 %\n",
            "Batch: 52   Training loss: 0.15529726445674896\n",
            "Precision: 69.48 +/- 35.1 %\n",
            "Recall: 28.74 +/- 17.5 %\n",
            "Batch: 53   Training loss: 0.14926181733608246\n",
            "Precision: 67.62 +/- 38.5 %\n",
            "Recall: 25.54 +/- 17.7 %\n",
            "Batch: 54   Training loss: 0.15412740409374237\n",
            "Precision: 61.87 +/- 36.7 %\n",
            "Recall: 25.58 +/- 16.6 %\n",
            "Batch: 55   Training loss: 0.17894279956817627\n",
            "Precision: 70.68 +/- 30.3 %\n",
            "Recall: 31.24 +/- 15.8 %\n",
            "Batch: 56   Training loss: 0.16540050506591797\n",
            "Precision: 54.96 +/- 35.8 %\n",
            "Recall: 29.76 +/- 19.2 %\n",
            "Batch: 57   Training loss: 0.14112494885921478\n",
            "Precision: 63.62 +/- 27.4 %\n",
            "Recall: 36.57 +/- 16.3 %\n",
            "Batch: 58   Training loss: 0.16786858439445496\n",
            "Precision: 72.41 +/- 31.2 %\n",
            "Recall: 31.14 +/- 14.9 %\n",
            "Batch: 59   Training loss: 0.16956208646297455\n",
            "Precision: 61.58 +/- 32.6 %\n",
            "Recall: 31.87 +/- 18.0 %\n",
            "Batch: 60   Training loss: 0.160944864153862\n",
            "Precision: 64.82 +/- 33.1 %\n",
            "Recall: 34.81 +/- 18.6 %\n",
            "Batch: 61   Training loss: 0.18558365106582642\n",
            "Precision: 62.78 +/- 34.2 %\n",
            "Recall: 28.19 +/- 18.9 %\n",
            "Batch: 62   Training loss: 0.163466677069664\n",
            "Precision: 58.02 +/- 35.9 %\n",
            "Recall: 30.29 +/- 18.9 %\n",
            "Batch: 63   Training loss: 0.1569681614637375\n",
            "Precision: 67.60 +/- 33.4 %\n",
            "Recall: 28.80 +/- 15.6 %\n",
            "Batch: 64   Training loss: 0.14498579502105713\n",
            "Precision: 64.43 +/- 35.3 %\n",
            "Recall: 34.87 +/- 20.3 %\n",
            "Batch: 65   Training loss: 0.15918350219726562\n",
            "Precision: 67.30 +/- 36.8 %\n",
            "Recall: 29.54 +/- 17.3 %\n",
            "Batch: 66   Training loss: 0.1421881914138794\n",
            "Precision: 64.98 +/- 40.5 %\n",
            "Recall: 26.02 +/- 18.3 %\n",
            "Batch: 67   Training loss: 0.20437774062156677\n",
            "Precision: 60.18 +/- 41.3 %\n",
            "Recall: 17.26 +/- 14.3 %\n",
            "Batch: 68   Training loss: 0.16321735084056854\n",
            "Precision: 71.20 +/- 31.4 %\n",
            "Recall: 29.52 +/- 15.6 %\n",
            "Batch: 69   Training loss: 0.16463062167167664\n",
            "Precision: 54.90 +/- 36.0 %\n",
            "Recall: 25.69 +/- 17.7 %\n",
            "Batch: 70   Training loss: 0.14929059147834778\n",
            "Precision: 65.50 +/- 30.0 %\n",
            "Recall: 36.43 +/- 17.0 %\n",
            "Batch: 71   Training loss: 0.16685153543949127\n",
            "Precision: 51.65 +/- 34.7 %\n",
            "Recall: 29.43 +/- 19.3 %\n",
            "Batch: 72   Training loss: 0.1719236671924591\n",
            "Precision: 71.78 +/- 28.2 %\n",
            "Recall: 35.34 +/- 15.0 %\n",
            "Batch: 73   Training loss: 0.14967195689678192\n",
            "Precision: 42.74 +/- 35.2 %\n",
            "Recall: 22.53 +/- 18.9 %\n",
            "Batch: 74   Training loss: 0.17000365257263184\n",
            "Precision: 57.60 +/- 34.7 %\n",
            "Recall: 23.55 +/- 14.6 %\n",
            "Batch: 75   Training loss: 0.1551860272884369\n",
            "Precision: 72.81 +/- 33.5 %\n",
            "Recall: 34.64 +/- 19.0 %\n",
            "Batch: 76   Training loss: 0.16557233035564423\n",
            "Precision: 64.99 +/- 34.2 %\n",
            "Recall: 32.77 +/- 18.3 %\n",
            "Batch: 77   Training loss: 0.15313075482845306\n",
            "Precision: 61.89 +/- 35.0 %\n",
            "Recall: 30.28 +/- 17.6 %\n",
            "Batch: 78   Training loss: 0.1456853449344635\n",
            "Precision: 52.84 +/- 39.2 %\n",
            "Recall: 25.24 +/- 18.7 %\n",
            "Batch: 79   Training loss: 0.18236173689365387\n",
            "Precision: 69.53 +/- 34.3 %\n",
            "Recall: 25.92 +/- 16.3 %\n",
            "Batch: 80   Training loss: 0.16658253967761993\n",
            "Precision: 65.68 +/- 38.6 %\n",
            "Recall: 25.77 +/- 16.2 %\n",
            "Batch: 81   Training loss: 0.16311153769493103\n",
            "Precision: 58.21 +/- 38.0 %\n",
            "Recall: 25.39 +/- 18.5 %\n",
            "Batch: 82   Training loss: 0.15880829095840454\n",
            "Precision: 58.32 +/- 37.1 %\n",
            "Recall: 26.85 +/- 17.2 %\n",
            "Batch: 83   Training loss: 0.16273291409015656\n",
            "Batch: 84   Training loss: 0.16346243023872375\n",
            "Batch: 85   Training loss: 0.15888065099716187\n",
            "Batch: 86   Training loss: 0.15948784351348877\n",
            "Precision: 53.19 +/- 43.0 %\n",
            "Recall: 25.20 +/- 20.6 %\n",
            "Batch: 87   Training loss: 0.14720718562602997\n",
            "Precision: 63.74 +/- 37.1 %\n",
            "Recall: 33.83 +/- 20.1 %\n",
            "Batch: 88   Training loss: 0.1757691204547882\n",
            "Precision: 71.31 +/- 29.2 %\n",
            "Recall: 30.78 +/- 16.0 %\n",
            "Batch: 89   Training loss: 0.16300511360168457\n",
            "Precision: 63.52 +/- 33.1 %\n",
            "Recall: 29.73 +/- 16.1 %\n",
            "Batch: 90   Training loss: 0.1609184890985489\n",
            "Precision: 56.69 +/- 36.5 %\n",
            "Recall: 27.15 +/- 18.8 %\n",
            "Batch: 91   Training loss: 0.15961435437202454\n",
            "Precision: 50.85 +/- 39.5 %\n",
            "Recall: 23.10 +/- 19.8 %\n",
            "Batch: 92   Training loss: 0.16072742640972137\n",
            "Precision: 62.28 +/- 40.2 %\n",
            "Recall: 23.15 +/- 18.5 %\n",
            "Epoch:  8\n",
            "Batch: 0   Training loss: 0.17052918672561646\n",
            "Precision: 68.06 +/- 34.5 %\n",
            "Recall: 27.54 +/- 17.0 %\n",
            "Batch: 1   Training loss: 0.14739184081554413\n",
            "Precision: 56.08 +/- 37.1 %\n",
            "Recall: 24.67 +/- 18.2 %\n",
            "Batch: 2   Training loss: 0.1649187058210373\n",
            "Precision: 59.98 +/- 36.2 %\n",
            "Recall: 27.92 +/- 18.8 %\n",
            "Batch: 3   Training loss: 0.16245372593402863\n",
            "Precision: 56.95 +/- 37.3 %\n",
            "Recall: 25.46 +/- 17.8 %\n",
            "Batch: 4   Training loss: 0.15769271552562714\n",
            "Batch: 5   Training loss: 0.16104502975940704\n",
            "Batch: 6   Training loss: 0.17633341252803802\n",
            "Precision: 71.20 +/- 30.4 %\n",
            "Recall: 28.23 +/- 14.3 %\n",
            "Batch: 7   Training loss: 0.17170144617557526\n",
            "Precision: 58.21 +/- 36.3 %\n",
            "Recall: 29.95 +/- 19.6 %\n",
            "Batch: 8   Training loss: 0.14894318580627441\n",
            "Batch: 9   Training loss: 0.15878544747829437\n",
            "Precision: 67.27 +/- 37.7 %\n",
            "Recall: 28.97 +/- 17.5 %\n",
            "Batch: 10   Training loss: 0.1476740837097168\n",
            "Precision: 66.03 +/- 36.3 %\n",
            "Recall: 34.18 +/- 18.9 %\n",
            "Batch: 11   Training loss: 0.1834934800863266\n",
            "Precision: 56.91 +/- 35.7 %\n",
            "Recall: 29.22 +/- 19.1 %\n",
            "Batch: 12   Training loss: 0.16195181012153625\n",
            "Precision: 54.26 +/- 36.0 %\n",
            "Recall: 29.34 +/- 18.0 %\n",
            "Batch: 13   Training loss: 0.1507471352815628\n",
            "Precision: 66.08 +/- 34.7 %\n",
            "Recall: 35.39 +/- 18.5 %\n",
            "Batch: 14   Training loss: 0.13966281712055206\n",
            "Precision: 53.90 +/- 39.4 %\n",
            "Recall: 27.68 +/- 20.3 %\n",
            "Batch: 15   Training loss: 0.15491162240505219\n",
            "Precision: 54.64 +/- 40.5 %\n",
            "Recall: 24.48 +/- 18.9 %\n",
            "Batch: 16   Training loss: 0.19545505940914154\n",
            "Precision: 61.48 +/- 34.5 %\n",
            "Recall: 23.60 +/- 14.1 %\n",
            "Batch: 17   Training loss: 0.16465315222740173\n",
            "Precision: 72.33 +/- 28.2 %\n",
            "Recall: 33.31 +/- 14.9 %\n",
            "Batch: 18   Training loss: 0.17091630399227142\n",
            "Precision: 64.40 +/- 32.0 %\n",
            "Recall: 32.19 +/- 17.0 %\n",
            "Batch: 19   Training loss: 0.176019549369812\n",
            "Precision: 62.46 +/- 28.3 %\n",
            "Recall: 34.24 +/- 17.8 %\n",
            "Batch: 20   Training loss: 0.15168796479701996\n",
            "Precision: 48.39 +/- 36.5 %\n",
            "Recall: 29.47 +/- 20.4 %\n",
            "Batch: 21   Training loss: 0.16210396587848663\n",
            "Precision: 73.97 +/- 26.3 %\n",
            "Recall: 34.43 +/- 15.9 %\n",
            "Batch: 22   Training loss: 0.1599968820810318\n",
            "Precision: 62.31 +/- 36.7 %\n",
            "Recall: 29.43 +/- 18.0 %\n",
            "Batch: 23   Training loss: 0.15658453106880188\n",
            "Precision: 60.77 +/- 39.8 %\n",
            "Recall: 23.15 +/- 17.9 %\n",
            "Batch: 24   Training loss: 0.15680162608623505\n",
            "Precision: 54.34 +/- 40.8 %\n",
            "Recall: 21.57 +/- 17.7 %\n",
            "Batch: 25   Training loss: 0.1591167449951172\n",
            "Precision: 60.35 +/- 37.8 %\n",
            "Recall: 26.60 +/- 18.1 %\n",
            "Batch: 26   Training loss: 0.1635715365409851\n",
            "Precision: 58.66 +/- 37.3 %\n",
            "Recall: 25.44 +/- 18.0 %\n",
            "Batch: 27   Training loss: 0.16291750967502594\n",
            "Precision: 55.45 +/- 36.4 %\n",
            "Recall: 26.47 +/- 18.2 %\n",
            "Batch: 28   Training loss: 0.18350479006767273\n",
            "Precision: 56.52 +/- 33.6 %\n",
            "Recall: 27.51 +/- 16.8 %\n",
            "Batch: 29   Training loss: 0.1783445179462433\n",
            "Precision: 62.68 +/- 36.4 %\n",
            "Recall: 29.18 +/- 18.1 %\n",
            "Batch: 30   Training loss: 0.1657191663980484\n",
            "Precision: 57.68 +/- 30.1 %\n",
            "Recall: 35.50 +/- 20.7 %\n",
            "Batch: 31   Training loss: 0.17131982743740082\n",
            "Precision: 60.07 +/- 27.1 %\n",
            "Recall: 36.72 +/- 16.9 %\n",
            "Batch: 32   Training loss: 0.18232040107250214\n",
            "Precision: 67.42 +/- 21.6 %\n",
            "Recall: 39.88 +/- 11.9 %\n",
            "Batch: 33   Training loss: 0.16236712038516998\n",
            "Precision: 57.21 +/- 28.4 %\n",
            "Recall: 33.73 +/- 18.7 %\n",
            "Batch: 34   Training loss: 0.15384835004806519\n",
            "Precision: 66.90 +/- 37.2 %\n",
            "Recall: 29.68 +/- 18.6 %\n",
            "Batch: 35   Training loss: 0.1452396810054779\n",
            "Precision: 57.63 +/- 41.6 %\n",
            "Recall: 25.06 +/- 21.0 %\n",
            "Batch: 36   Training loss: 0.16906626522541046\n",
            "Precision: 66.79 +/- 40.6 %\n",
            "Recall: 19.52 +/- 16.8 %\n",
            "Batch: 37   Training loss: 0.15264169871807098\n",
            "Precision: 71.07 +/- 38.2 %\n",
            "Recall: 27.67 +/- 16.9 %\n",
            "Batch: 38   Training loss: 0.160932257771492\n",
            "Batch: 39   Training loss: 0.16513840854167938\n",
            "Precision: 58.81 +/- 34.8 %\n",
            "Recall: 28.27 +/- 19.2 %\n",
            "Batch: 40   Training loss: 0.15239277482032776\n",
            "Precision: 56.16 +/- 35.4 %\n",
            "Recall: 27.71 +/- 19.2 %\n",
            "Batch: 41   Training loss: 0.17580828070640564\n",
            "Precision: 58.39 +/- 34.1 %\n",
            "Recall: 32.88 +/- 17.8 %\n",
            "Batch: 42   Training loss: 0.15871234238147736\n",
            "Precision: 61.42 +/- 35.0 %\n",
            "Recall: 35.79 +/- 21.4 %\n",
            "Batch: 43   Training loss: 0.1656949520111084\n",
            "Precision: 51.08 +/- 30.9 %\n",
            "Recall: 29.01 +/- 19.5 %\n",
            "Batch: 44   Training loss: 0.18501120805740356\n",
            "Batch: 45   Training loss: 0.162896990776062\n",
            "Precision: 60.72 +/- 33.6 %\n",
            "Recall: 35.54 +/- 19.0 %\n",
            "Batch: 46   Training loss: 0.16388589143753052\n",
            "Precision: 68.47 +/- 20.3 %\n",
            "Recall: 41.30 +/- 11.1 %\n",
            "Batch: 47   Training loss: 0.1727149337530136\n",
            "Precision: 66.94 +/- 30.7 %\n",
            "Recall: 34.46 +/- 16.7 %\n",
            "Batch: 48   Training loss: 0.1669522374868393\n",
            "Precision: 57.90 +/- 35.5 %\n",
            "Recall: 33.38 +/- 22.1 %\n",
            "[9,    50] loss: 0.159\n",
            "Batch: 49   Training loss: 0.1592208594083786\n",
            "Precision: 72.48 +/- 31.6 %\n",
            "Recall: 34.99 +/- 15.6 %\n",
            "Batch: 50   Training loss: 0.1526251882314682\n",
            "Precision: 62.76 +/- 36.6 %\n",
            "Recall: 30.34 +/- 19.2 %\n",
            "Batch: 51   Training loss: 0.1569826602935791\n",
            "Precision: 69.80 +/- 28.0 %\n",
            "Recall: 31.68 +/- 13.8 %\n",
            "Batch: 52   Training loss: 0.15833395719528198\n",
            "Precision: 73.15 +/- 32.5 %\n",
            "Recall: 33.71 +/- 16.0 %\n",
            "Batch: 53   Training loss: 0.17559067904949188\n",
            "Precision: 57.74 +/- 37.6 %\n",
            "Recall: 27.08 +/- 18.4 %\n",
            "Batch: 54   Training loss: 0.1625472754240036\n",
            "Precision: 64.03 +/- 33.4 %\n",
            "Recall: 30.09 +/- 16.6 %\n",
            "Batch: 55   Training loss: 0.1451156735420227\n",
            "Precision: 49.39 +/- 40.0 %\n",
            "Recall: 25.23 +/- 21.1 %\n",
            "Batch: 56   Training loss: 0.16129523515701294\n",
            "Batch: 57   Training loss: 0.1532343327999115\n",
            "Batch: 58   Training loss: 0.16042150557041168\n",
            "Batch: 59   Training loss: 0.14357908070087433\n",
            "Precision: 79.54 +/- 34.6 %\n",
            "Recall: 32.41 +/- 19.3 %\n",
            "Batch: 60   Training loss: 0.17174439132213593\n",
            "Precision: 50.12 +/- 42.4 %\n",
            "Recall: 21.15 +/- 17.8 %\n",
            "Batch: 61   Training loss: 0.16737404465675354\n",
            "Precision: 55.14 +/- 37.8 %\n",
            "Recall: 27.65 +/- 18.9 %\n",
            "Batch: 62   Training loss: 0.15071317553520203\n",
            "Precision: 74.20 +/- 33.0 %\n",
            "Recall: 35.37 +/- 19.8 %\n",
            "Batch: 63   Training loss: 0.1716223657131195\n",
            "Precision: 54.25 +/- 40.0 %\n",
            "Recall: 26.36 +/- 19.7 %\n",
            "Batch: 64   Training loss: 0.18969757854938507\n",
            "Precision: 65.57 +/- 29.5 %\n",
            "Recall: 31.67 +/- 15.2 %\n",
            "Batch: 65   Training loss: 0.1495213806629181\n",
            "Precision: 67.55 +/- 31.0 %\n",
            "Recall: 35.20 +/- 15.7 %\n",
            "Batch: 66   Training loss: 0.16752173006534576\n",
            "Precision: 62.89 +/- 32.4 %\n",
            "Recall: 29.20 +/- 14.7 %\n",
            "Batch: 67   Training loss: 0.1546591818332672\n",
            "Precision: 65.48 +/- 28.2 %\n",
            "Recall: 36.56 +/- 16.6 %\n",
            "Batch: 68   Training loss: 0.1520712673664093\n",
            "Precision: 57.60 +/- 36.4 %\n",
            "Recall: 27.21 +/- 18.7 %\n",
            "Batch: 69   Training loss: 0.15360791981220245\n",
            "Batch: 70   Training loss: 0.16638407111167908\n",
            "Batch: 71   Training loss: 0.17833863198757172\n",
            "Precision: 65.35 +/- 37.4 %\n",
            "Recall: 26.41 +/- 16.2 %\n",
            "Batch: 72   Training loss: 0.15576796233654022\n",
            "Batch: 73   Training loss: 0.14208275079727173\n",
            "Precision: 58.56 +/- 38.8 %\n",
            "Recall: 33.99 +/- 22.8 %\n",
            "Batch: 74   Training loss: 0.16012261807918549\n",
            "Precision: 57.25 +/- 36.8 %\n",
            "Recall: 31.04 +/- 20.3 %\n",
            "Batch: 75   Training loss: 0.1499016135931015\n",
            "Precision: 63.84 +/- 36.2 %\n",
            "Recall: 29.17 +/- 17.7 %\n",
            "Batch: 76   Training loss: 0.15496845543384552\n",
            "Precision: 58.95 +/- 36.2 %\n",
            "Recall: 27.61 +/- 18.8 %\n",
            "Batch: 77   Training loss: 0.15016531944274902\n",
            "Precision: 65.62 +/- 34.2 %\n",
            "Recall: 31.18 +/- 20.2 %\n",
            "Batch: 78   Training loss: 0.18096452951431274\n",
            "Precision: 74.19 +/- 29.4 %\n",
            "Recall: 29.43 +/- 14.6 %\n",
            "Batch: 79   Training loss: 0.1517946422100067\n",
            "Precision: 68.19 +/- 37.6 %\n",
            "Recall: 31.17 +/- 21.8 %\n",
            "Batch: 80   Training loss: 0.1727253496646881\n",
            "Precision: 69.63 +/- 32.7 %\n",
            "Recall: 29.03 +/- 15.8 %\n",
            "Batch: 81   Training loss: 0.1445562094449997\n",
            "Precision: 52.51 +/- 37.1 %\n",
            "Recall: 23.70 +/- 18.8 %\n",
            "Batch: 82   Training loss: 0.16582021117210388\n",
            "Precision: 62.70 +/- 34.7 %\n",
            "Recall: 28.73 +/- 18.4 %\n",
            "Batch: 83   Training loss: 0.16224288940429688\n",
            "Precision: 64.82 +/- 32.8 %\n",
            "Recall: 30.98 +/- 17.4 %\n",
            "Batch: 84   Training loss: 0.15105664730072021\n",
            "Precision: 71.09 +/- 30.1 %\n",
            "Recall: 36.89 +/- 17.8 %\n",
            "Batch: 85   Training loss: 0.17046137154102325\n",
            "Precision: 56.19 +/- 32.9 %\n",
            "Recall: 32.24 +/- 17.1 %\n",
            "Batch: 86   Training loss: 0.15584242343902588\n",
            "Precision: 64.89 +/- 33.7 %\n",
            "Recall: 35.44 +/- 18.3 %\n",
            "Batch: 87   Training loss: 0.1507125049829483\n",
            "Precision: 56.16 +/- 35.6 %\n",
            "Recall: 30.68 +/- 20.8 %\n",
            "Batch: 88   Training loss: 0.17865385115146637\n",
            "Precision: 64.82 +/- 35.2 %\n",
            "Recall: 28.99 +/- 17.1 %\n",
            "Batch: 89   Training loss: 0.16682682931423187\n",
            "Precision: 68.04 +/- 23.8 %\n",
            "Recall: 30.97 +/- 11.5 %\n",
            "Batch: 90   Training loss: 0.16308659315109253\n",
            "Precision: 60.70 +/- 37.3 %\n",
            "Recall: 25.46 +/- 17.6 %\n",
            "Batch: 91   Training loss: 0.16195528209209442\n",
            "Precision: 75.06 +/- 27.6 %\n",
            "Recall: 29.76 +/- 13.0 %\n",
            "Batch: 92   Training loss: 0.17305101454257965\n",
            "Precision: 59.06 +/- 33.6 %\n",
            "Recall: 27.52 +/- 18.2 %\n",
            "Epoch:  9\n",
            "Batch: 0   Training loss: 0.1512230485677719\n",
            "Precision: 54.31 +/- 40.0 %\n",
            "Recall: 25.11 +/- 19.5 %\n",
            "Batch: 1   Training loss: 0.1537046581506729\n",
            "Precision: 65.01 +/- 37.6 %\n",
            "Recall: 30.80 +/- 18.5 %\n",
            "Batch: 2   Training loss: 0.17469991743564606\n",
            "Batch: 3   Training loss: 0.16093866527080536\n",
            "Batch: 4   Training loss: 0.16815441846847534\n",
            "Precision: 61.46 +/- 30.0 %\n",
            "Recall: 30.97 +/- 14.2 %\n",
            "Batch: 5   Training loss: 0.1618671417236328\n",
            "Precision: 71.26 +/- 31.4 %\n",
            "Recall: 34.40 +/- 16.4 %\n",
            "Batch: 6   Training loss: 0.14943906664848328\n",
            "Precision: 61.96 +/- 30.7 %\n",
            "Recall: 35.12 +/- 18.7 %\n",
            "Batch: 7   Training loss: 0.1671571582555771\n",
            "Precision: 56.66 +/- 31.0 %\n",
            "Recall: 30.15 +/- 14.7 %\n",
            "Batch: 8   Training loss: 0.17377522587776184\n",
            "Precision: 70.30 +/- 24.9 %\n",
            "Recall: 38.26 +/- 13.6 %\n",
            "Batch: 9   Training loss: 0.1640799343585968\n",
            "Precision: 56.01 +/- 31.8 %\n",
            "Recall: 31.18 +/- 17.9 %\n",
            "Batch: 10   Training loss: 0.1623304784297943\n",
            "Precision: 59.12 +/- 33.7 %\n",
            "Recall: 28.96 +/- 19.4 %\n",
            "Batch: 11   Training loss: 0.15937064588069916\n",
            "Precision: 56.45 +/- 41.5 %\n",
            "Recall: 23.45 +/- 19.3 %\n",
            "Batch: 12   Training loss: 0.16709229350090027\n",
            "Batch: 13   Training loss: 0.16853657364845276\n",
            "Precision: 67.32 +/- 43.6 %\n",
            "Recall: 19.01 +/- 15.5 %\n",
            "Batch: 14   Training loss: 0.17555353045463562\n",
            "Batch: 15   Training loss: 0.17513899505138397\n",
            "Precision: 65.59 +/- 35.6 %\n",
            "Recall: 26.17 +/- 16.3 %\n",
            "Batch: 16   Training loss: 0.14769817888736725\n",
            "Precision: 42.45 +/- 39.4 %\n",
            "Recall: 20.97 +/- 19.6 %\n",
            "Batch: 17   Training loss: 0.13672733306884766\n",
            "Precision: 64.29 +/- 34.9 %\n",
            "Recall: 31.96 +/- 18.7 %\n",
            "Batch: 18   Training loss: 0.15776576101779938\n",
            "Batch: 19   Training loss: 0.15975669026374817\n",
            "Batch: 20   Training loss: 0.18595312535762787\n",
            "Batch: 21   Training loss: 0.15427203476428986\n",
            "Precision: 61.27 +/- 33.2 %\n",
            "Recall: 34.81 +/- 18.4 %\n",
            "Batch: 22   Training loss: 0.16281236708164215\n",
            "Precision: 72.77 +/- 26.4 %\n",
            "Recall: 35.84 +/- 13.6 %\n",
            "Batch: 23   Training loss: 0.17569047212600708\n",
            "Precision: 66.27 +/- 30.2 %\n",
            "Recall: 31.31 +/- 15.6 %\n",
            "Batch: 24   Training loss: 0.18417750298976898\n",
            "Precision: 55.32 +/- 35.1 %\n",
            "Recall: 26.92 +/- 16.6 %\n",
            "Batch: 25   Training loss: 0.17111346125602722\n",
            "Precision: 68.79 +/- 28.8 %\n",
            "Recall: 35.55 +/- 18.7 %\n",
            "Batch: 26   Training loss: 0.17178060114383698\n",
            "Precision: 56.65 +/- 37.9 %\n",
            "Recall: 24.45 +/- 18.5 %\n",
            "Batch: 27   Training loss: 0.17069703340530396\n",
            "Precision: 58.42 +/- 41.2 %\n",
            "Recall: 23.21 +/- 19.6 %\n",
            "Batch: 28   Training loss: 0.16308216750621796\n",
            "Precision: 62.29 +/- 38.9 %\n",
            "Recall: 23.39 +/- 16.3 %\n",
            "Batch: 29   Training loss: 0.15793749690055847\n",
            "Precision: 72.62 +/- 37.4 %\n",
            "Recall: 28.81 +/- 21.0 %\n",
            "Batch: 30   Training loss: 0.15890716016292572\n",
            "Precision: 57.74 +/- 42.2 %\n",
            "Recall: 21.15 +/- 18.2 %\n",
            "Batch: 31   Training loss: 0.17684200406074524\n",
            "Precision: 71.69 +/- 34.0 %\n",
            "Recall: 26.85 +/- 15.6 %\n",
            "Batch: 32   Training loss: 0.14460338652133942\n",
            "Precision: 60.49 +/- 35.8 %\n",
            "Recall: 28.19 +/- 19.2 %\n",
            "Batch: 33   Training loss: 0.16241347789764404\n",
            "Precision: 65.15 +/- 32.3 %\n",
            "Recall: 31.06 +/- 17.1 %\n",
            "Batch: 34   Training loss: 0.1612670123577118\n",
            "Precision: 54.21 +/- 35.3 %\n",
            "Recall: 29.23 +/- 20.1 %\n",
            "Batch: 35   Training loss: 0.1532078981399536\n",
            "Precision: 64.61 +/- 35.9 %\n",
            "Recall: 35.04 +/- 19.5 %\n",
            "Batch: 36   Training loss: 0.16751523315906525\n",
            "Precision: 51.74 +/- 35.6 %\n",
            "Recall: 27.03 +/- 19.0 %\n",
            "Batch: 37   Training loss: 0.17953887581825256\n",
            "Batch: 38   Training loss: 0.1533401906490326\n",
            "Precision: 60.97 +/- 37.8 %\n",
            "Recall: 33.40 +/- 22.3 %\n",
            "Batch: 39   Training loss: 0.15548211336135864\n",
            "Precision: 71.17 +/- 30.1 %\n",
            "Recall: 37.99 +/- 16.5 %\n",
            "Batch: 40   Training loss: 0.1640540063381195\n",
            "Precision: 60.12 +/- 36.0 %\n",
            "Recall: 30.03 +/- 19.2 %\n",
            "Batch: 41   Training loss: 0.14898398518562317\n",
            "Precision: 68.44 +/- 29.4 %\n",
            "Recall: 37.87 +/- 18.6 %\n",
            "Batch: 42   Training loss: 0.14732062816619873\n",
            "Precision: 55.75 +/- 36.3 %\n",
            "Recall: 31.70 +/- 21.9 %\n",
            "Batch: 43   Training loss: 0.15206314623355865\n",
            "Precision: 63.82 +/- 34.8 %\n",
            "Recall: 30.78 +/- 17.5 %\n",
            "Batch: 44   Training loss: 0.16778992116451263\n",
            "Precision: 62.02 +/- 30.9 %\n",
            "Recall: 31.36 +/- 16.8 %\n",
            "Batch: 45   Training loss: 0.14695240557193756\n",
            "Precision: 63.12 +/- 38.1 %\n",
            "Recall: 32.13 +/- 20.7 %\n",
            "Batch: 46   Training loss: 0.1556408405303955\n",
            "Batch: 47   Training loss: 0.15959115326404572\n",
            "Batch: 48   Training loss: 0.1638559103012085\n",
            "[10,    50] loss: 0.157\n",
            "Batch: 49   Training loss: 0.15709459781646729\n",
            "Batch: 50   Training loss: 0.16831336915493011\n",
            "Precision: 69.14 +/- 33.1 %\n",
            "Recall: 29.55 +/- 15.6 %\n",
            "Batch: 51   Training loss: 0.15910011529922485\n",
            "Batch: 52   Training loss: 0.15177655220031738\n",
            "Precision: 63.84 +/- 33.3 %\n",
            "Recall: 36.43 +/- 20.8 %\n",
            "Batch: 53   Training loss: 0.15106716752052307\n",
            "Precision: 52.82 +/- 31.6 %\n",
            "Recall: 33.80 +/- 22.3 %\n",
            "Batch: 54   Training loss: 0.16819512844085693\n",
            "Precision: 55.64 +/- 36.0 %\n",
            "Recall: 25.63 +/- 18.3 %\n",
            "Batch: 55   Training loss: 0.16321326792240143\n",
            "Batch: 56   Training loss: 0.1764279007911682\n",
            "Precision: 67.72 +/- 33.4 %\n",
            "Recall: 29.95 +/- 17.6 %\n",
            "Batch: 57   Training loss: 0.1575963795185089\n",
            "Precision: 67.45 +/- 34.5 %\n",
            "Recall: 36.46 +/- 20.2 %\n",
            "Batch: 58   Training loss: 0.16407093405723572\n",
            "Precision: 75.43 +/- 24.0 %\n",
            "Recall: 37.06 +/- 15.5 %\n",
            "Batch: 59   Training loss: 0.14781560003757477\n",
            "Precision: 56.24 +/- 35.9 %\n",
            "Recall: 33.04 +/- 22.8 %\n",
            "Batch: 60   Training loss: 0.1464959681034088\n",
            "Precision: 61.47 +/- 36.3 %\n",
            "Recall: 31.55 +/- 20.8 %\n",
            "Batch: 61   Training loss: 0.15609262883663177\n",
            "Precision: 60.18 +/- 35.9 %\n",
            "Recall: 31.86 +/- 19.2 %\n",
            "Batch: 62   Training loss: 0.1718987375497818\n",
            "Precision: 58.53 +/- 31.0 %\n",
            "Recall: 30.61 +/- 17.7 %\n",
            "Batch: 63   Training loss: 0.17875118553638458\n",
            "Batch: 64   Training loss: 0.16801203787326813\n",
            "Precision: 60.35 +/- 36.0 %\n",
            "Recall: 27.45 +/- 18.6 %\n",
            "Batch: 65   Training loss: 0.1630735844373703\n",
            "Batch: 66   Training loss: 0.15197955071926117\n",
            "Batch: 67   Training loss: 0.17190976440906525\n",
            "Batch: 68   Training loss: 0.16142533719539642\n",
            "Batch: 69   Training loss: 0.16443835198879242\n",
            "Batch: 70   Training loss: 0.1517501175403595\n",
            "Batch: 71   Training loss: 0.1522301733493805\n",
            "Precision: 70.55 +/- 31.1 %\n",
            "Recall: 36.75 +/- 19.0 %\n",
            "Batch: 72   Training loss: 0.1553228795528412\n",
            "Precision: 70.78 +/- 31.6 %\n",
            "Recall: 34.60 +/- 16.5 %\n",
            "Batch: 73   Training loss: 0.15381060540676117\n",
            "Precision: 60.46 +/- 35.6 %\n",
            "Recall: 27.75 +/- 20.1 %\n",
            "Batch: 74   Training loss: 0.16229113936424255\n",
            "Precision: 61.61 +/- 41.9 %\n",
            "Recall: 24.61 +/- 17.4 %\n",
            "Batch: 75   Training loss: 0.1388406902551651\n",
            "Precision: 58.75 +/- 39.3 %\n",
            "Recall: 23.69 +/- 18.6 %\n",
            "Batch: 76   Training loss: 0.16159974038600922\n",
            "Batch: 77   Training loss: 0.15390430390834808\n",
            "Precision: 63.41 +/- 35.5 %\n",
            "Recall: 26.27 +/- 17.8 %\n",
            "Batch: 78   Training loss: 0.15883475542068481\n",
            "Batch: 79   Training loss: 0.17481037974357605\n",
            "Precision: 58.16 +/- 31.7 %\n",
            "Recall: 35.24 +/- 19.5 %\n",
            "Batch: 80   Training loss: 0.16179686784744263\n",
            "Batch: 81   Training loss: 0.1528090089559555\n",
            "Precision: 62.73 +/- 27.4 %\n",
            "Recall: 44.23 +/- 20.5 %\n",
            "Batch: 82   Training loss: 0.14194267988204956\n",
            "Batch: 83   Training loss: 0.16296346485614777\n",
            "Batch: 84   Training loss: 0.15578919649124146\n",
            "Batch: 85   Training loss: 0.1584891825914383\n",
            "Batch: 86   Training loss: 0.1813354790210724\n",
            "Batch: 87   Training loss: 0.18237565457820892\n",
            "Batch: 88   Training loss: 0.16214096546173096\n",
            "Precision: 58.83 +/- 34.6 %\n",
            "Recall: 30.66 +/- 19.7 %\n",
            "Batch: 89   Training loss: 0.16101866960525513\n",
            "Precision: 62.80 +/- 30.9 %\n",
            "Recall: 33.45 +/- 15.8 %\n",
            "Batch: 90   Training loss: 0.16845260560512543\n",
            "Batch: 91   Training loss: 0.16443029046058655\n",
            "Batch: 92   Training loss: 0.1544569730758667\n",
            "Precision: 68.04 +/- 28.5 %\n",
            "Recall: 34.18 +/- 15.3 %\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "No prediction as postive, skipping..\n",
            "Accuracy: \n",
            "0.750 +/- 0.13\n",
            "\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.3)\n",
        "cv.get_n_splits(images_filtered, tokens_filtered)\n",
        "\n",
        "ACC = []\n",
        "HIST = []\n",
        "\n",
        "BS = 28\n",
        "log_every_n_batch=1\n",
        "\n",
        "data = ImgTokenDataset(images_filtered, tokens_filtered, transform=transform, target_transform=target_transform, vocab=vocab)\n",
        "for fold, (train_index, test_index) in enumerate(cv.split(images_filtered, tokens_filtered)):\n",
        "    \n",
        "    HIST.append([])\n",
        "    \n",
        "    print('Fold: ', fold)\n",
        "    # Setup data\n",
        "\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_index)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      data, \n",
        "                      batch_size=BS, sampler=train_subsampler)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "                      data,\n",
        "                      batch_size=BS, sampler=test_subsampler)\n",
        "\n",
        "    # Model\n",
        "    net = Net(input_shape, vocab)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "    net.to(device)\n",
        "    # Train\n",
        "    for epoch in range(10):  # loop over the dataset multiple times\n",
        "        print('Epoch: ', epoch)\n",
        "        for i, d in enumerate(trainloader, 0):\n",
        "            inputs, labels = d\n",
        "        \n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            #inputs = inputs.cuda()\n",
        "            #labels = labels.cuda()\n",
        "            optimizer.zero_grad()   # zero the gradient buffers\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            if i % 50 == 49:\n",
        "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, loss.item()))\n",
        "                HIST[fold].append(loss.item())\n",
        "\n",
        "            if i % log_every_n_batch == 0:\n",
        "              print('Batch: {}   Training loss: {}'.format(i, loss.item()))\n",
        "              try:\n",
        "                p_m, p_s, r_m, r_s = report_precision_recall(net, d, device)\n",
        "              except:\n",
        "                continue\n",
        "              print(f'Precision: {100*p_m:.2f} +/- {100*p_s:.1f} %')\n",
        "              print(f'Recall: {100*r_m:.2f} +/- {100*r_s:.1f} %')\n",
        "            \n",
        "                \n",
        "    correct, total = test_cnn(net, testloader)\n",
        "    acc = correct / total\n",
        "\n",
        "    print('Accuracy: ')\n",
        "    print(f'{acc.mean():.3f} +/- {(acc.std()/2):.2f}')\n",
        "    print()\n",
        "    ACC.append(acc)\n",
        "    \n",
        "                      \n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_accs = []\n",
        "for vals in ACC:\n",
        "    mean_accs.append(np.mean(vals))\n",
        "\n",
        "print(np.mean(mean_accs))\n",
        "print(np.std(mean_accs))"
      ],
      "metadata": {
        "id": "KFm3fJ1-IJRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c83d2a-f49d-4a6e-a820-428afebac3b6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7352448281203714\n",
            "0.018031604854437833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "for fold in range(5):\n",
        "    plt.plot(list(range(len(HIST[fold]))), HIST[fold], label=f'fold {fold}', alpha=0.5)\n",
        "    \n",
        "# plt.xticks(labels=str(50*np.arange(len(HIST[fold]))))\n",
        "plt.xlabel('iterations * 50')\n",
        "plt.ylabel('training loss')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "dXWNQJs1IMZ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "62681190-efdf-45b5-eb61-1d8652f6b962"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment with different activation functions and pooling types"
      ],
      "metadata": {
        "id": "aTm87slwhng6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(train_data, batch_size=28, shuffle=True)\n",
        "testLoader = DataLoader(test_data, batch_size=28, shuffle=True)\n",
        "threshold = 0.5\n",
        "max_acc = float('-inf')"
      ],
      "metadata": {
        "id": "EE1TnqVjkXlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for activation_function in ['sigmoid', 'relu', 'tanh']:\n",
        "      for pooling_type in ['max', 'avg']:\n",
        "        net = Net(input_shape, vocab, activation_function=activation_function, pooling_type=pooling_type)\n",
        "        net.to(device)\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "        for epoch in range(10):  # loop over the dataset multiple times\n",
        "          print('start')\n",
        "          for i, data in enumerate(trainloader, 0):\n",
        "              inputs, labels = data\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "              optimizer.zero_grad()   # zero the gradient buffers\n",
        "              outputs = net(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              loss.requires_grad = True\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "        print('Finished Training')\n",
        "        correct = []\n",
        "        total = []\n",
        "        for i, data in enumerate(testloader, 0):\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "          outputs = net(inputs)\n",
        "        \n",
        "          for sample in range(len(outputs.cpu())):\n",
        "              preds = np.where(outputs[sample].cpu()>threshold)[0]\n",
        "              labels_ = np.where(labels[sample].cpu()==1)[0]\n",
        "              if len(labels_) < 1:\n",
        "                  continue\n",
        "              intersection = set(preds) & set(labels_)\n",
        "              total.append(len(labels_))\n",
        "              correct.append(len(intersection))\n",
        "        correct, total = np.array(correct), np.array(total)\n",
        "        acc = correct / total\n",
        "        print(\"Activation Function: \", activation_function)\n",
        "        print(\"Pooling type: \", pooling_type)\n",
        "        max_acc = max(max_acc, acc.mean())\n",
        "        print(acc.std())\n",
        "        print(acc.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu9QiDxHhrki",
        "outputId": "88b24d9a-09ba-4d3f-b642-f67b1a926ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "Finished Training\n",
            "Activation Function:  sigmoid\n",
            "Pooling type:  max\n",
            "0.181659119925223\n",
            "0.47665325426424526\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "Finished Training\n",
            "Activation Function:  sigmoid\n",
            "Pooling type:  avg\n",
            "0.1742652038501674\n",
            "0.4519107982005662\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "Finished Training\n",
            "Activation Function:  relu\n",
            "Pooling type:  max\n",
            "0.19316164602249752\n",
            "0.39556651818846106\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "Finished Training\n",
            "Activation Function:  relu\n",
            "Pooling type:  avg\n",
            "0.2004991317364644\n",
            "0.5347273087022126\n",
            "start\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "Finished Training\n",
            "Activation Function:  tanh\n",
            "Pooling type:  max\n",
            "0.18924631471384631\n",
            "0.49393252037589863\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "start\n",
            "Finished Training\n",
            "Activation Function:  tanh\n",
            "Pooling type:  avg\n",
            "0.1978202026539898\n",
            "0.524757607075287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fc5kTtmXkkhu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "data_augmentation_experiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}